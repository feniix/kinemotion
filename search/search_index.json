{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quick Start","text":"<p>Kinemotion provides video-based kinematic analysis for athletic performance using MediaPipe pose tracking.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install kinemotion\n</code></pre>"},{"location":"#supported-jump-types","title":"Supported Jump Types","text":"<ul> <li>Drop Jump: Ground contact time, flight time, reactive strength index</li> <li>Counter Movement Jump (CMJ): Jump height, flight time, countermovement depth, triple extension</li> </ul>"},{"location":"#quick-examples","title":"Quick Examples","text":""},{"location":"#drop-jump-analysis","title":"Drop Jump Analysis","text":"<pre><code>kinemotion dropjump-analyze video.mp4\n</code></pre> <p>Or via Python API:</p> <pre><code>from kinemotion import process_dropjump_video\n\nmetrics = process_dropjump_video(\"video.mp4\", quality=\"balanced\")\nprint(f\"Ground contact time: {metrics.ground_contact_time:.3f}s\")\nprint(f\"RSI: {metrics.reactive_strength_index:.2f}\")\n</code></pre>"},{"location":"#cmj-analysis","title":"CMJ Analysis","text":"<pre><code>kinemotion cmj-analyze video.mp4\n</code></pre> <p>Or via Python API:</p> <pre><code>from kinemotion import process_cmj_video\n\nmetrics = process_cmj_video(\"video.mp4\")\nprint(f\"Jump height: {metrics.jump_height:.2f}m\")\nprint(f\"Flight time: {metrics.flight_time:.3f}s\")\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>CMJ Analysis Guide - Detailed guide for CMJ analysis</li> <li>Camera Setup - How to set up your camera for best results</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"api/cmj/","title":"CMJ API","text":"<p>The CMJ API provides functions for analyzing counter movement jump videos and extracting kinematic metrics including triple extension analysis.</p>"},{"location":"api/cmj/#quick-example","title":"Quick Example","text":"<pre><code>from kinemotion import process_cmj_video\n\nmetrics = process_cmj_video(\n    video_path=\"cmj.mp4\",\n    quality=\"balanced\",  # fast, balanced, or accurate\n    output_video=\"debug.mp4\",  # optional\n    verbose=True\n)\n\nprint(f\"Jump height: {metrics.jump_height:.2f}m\")\nprint(f\"Flight time: {metrics.flight_time:.3f}s\")\nprint(f\"Countermovement depth: {metrics.countermovement_depth:.3f}m\")\nprint(f\"Triple extension: {metrics.triple_extension_percentage:.1f}%\")\n</code></pre>"},{"location":"api/cmj/#main-functions","title":"Main Functions","text":"<p>options: show_root_heading: true show_source: false</p> <p>options: show_root_heading: true show_source: false</p>"},{"location":"api/cmj/#kinemotion.api.process_cmj_video","title":"process_cmj_video","text":"<pre><code>process_cmj_video(\n    video_path,\n    quality=\"balanced\",\n    output_video=None,\n    json_output=None,\n    smoothing_window=None,\n    velocity_threshold=None,\n    min_contact_frames=None,\n    visibility_threshold=None,\n    detection_confidence=None,\n    tracking_confidence=None,\n    verbose=False,\n)\n</code></pre> <p>Process a single CMJ video and return metrics.</p> <p>CMJ (Counter Movement Jump) is performed at floor level without a drop box. Athletes start standing, perform a countermovement (eccentric phase), then jump upward (concentric phase).</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the input video file</p> required <code>quality</code> <code>str</code> <p>Analysis quality preset (\"fast\", \"balanced\", or \"accurate\")</p> <code>'balanced'</code> <code>output_video</code> <code>str | None</code> <p>Optional path for debug video output</p> <code>None</code> <code>json_output</code> <code>str | None</code> <p>Optional path for JSON metrics output</p> <code>None</code> <code>smoothing_window</code> <code>int | None</code> <p>Optional override for smoothing window</p> <code>None</code> <code>velocity_threshold</code> <code>float | None</code> <p>Optional override for velocity threshold</p> <code>None</code> <code>min_contact_frames</code> <code>int | None</code> <p>Optional override for minimum contact frames</p> <code>None</code> <code>visibility_threshold</code> <code>float | None</code> <p>Optional override for visibility threshold</p> <code>None</code> <code>detection_confidence</code> <code>float | None</code> <p>Optional override for pose detection confidence</p> <code>None</code> <code>tracking_confidence</code> <code>float | None</code> <p>Optional override for pose tracking confidence</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print processing details</p> <code>False</code> <p>Returns:</p> Type Description <code>CMJMetrics</code> <p>CMJMetrics object containing analysis results</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If video cannot be processed or parameters are invalid</p> <code>FileNotFoundError</code> <p>If video file does not exist</p> Example <p>metrics = process_cmj_video( ...     \"athlete_cmj.mp4\", ...     quality=\"balanced\", ...     verbose=True ... ) print(f\"Jump height: {metrics.jump_height:.3f}m\") print(f\"Countermovement depth: {metrics.countermovement_depth:.3f}m\")</p> Source code in <code>src/kinemotion/api.py</code> <pre><code>def process_cmj_video(\n    video_path: str,\n    quality: str = \"balanced\",\n    output_video: str | None = None,\n    json_output: str | None = None,\n    smoothing_window: int | None = None,\n    velocity_threshold: float | None = None,\n    min_contact_frames: int | None = None,\n    visibility_threshold: float | None = None,\n    detection_confidence: float | None = None,\n    tracking_confidence: float | None = None,\n    verbose: bool = False,\n) -&gt; CMJMetrics:\n    \"\"\"\n    Process a single CMJ video and return metrics.\n\n    CMJ (Counter Movement Jump) is performed at floor level without a drop box.\n    Athletes start standing, perform a countermovement (eccentric phase), then\n    jump upward (concentric phase).\n\n    Args:\n        video_path: Path to the input video file\n        quality: Analysis quality preset (\"fast\", \"balanced\", or \"accurate\")\n        output_video: Optional path for debug video output\n        json_output: Optional path for JSON metrics output\n        smoothing_window: Optional override for smoothing window\n        velocity_threshold: Optional override for velocity threshold\n        min_contact_frames: Optional override for minimum contact frames\n        visibility_threshold: Optional override for visibility threshold\n        detection_confidence: Optional override for pose detection confidence\n        tracking_confidence: Optional override for pose tracking confidence\n        verbose: Print processing details\n\n    Returns:\n        CMJMetrics object containing analysis results\n\n    Raises:\n        ValueError: If video cannot be processed or parameters are invalid\n        FileNotFoundError: If video file does not exist\n\n    Example:\n        &gt;&gt;&gt; metrics = process_cmj_video(\n        ...     \"athlete_cmj.mp4\",\n        ...     quality=\"balanced\",\n        ...     verbose=True\n        ... )\n        &gt;&gt;&gt; print(f\"Jump height: {metrics.jump_height:.3f}m\")\n        &gt;&gt;&gt; print(f\"Countermovement depth: {metrics.countermovement_depth:.3f}m\")\n    \"\"\"\n    if not Path(video_path).exists():\n        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n\n    # Convert quality string to enum\n    quality_preset = _parse_quality_preset(quality)\n\n    # Initialize video processor\n    with VideoProcessor(video_path) as video:\n        if verbose:\n            print(\n                f\"Video: {video.width}x{video.height} @ {video.fps:.2f} fps, \"\n                f\"{video.frame_count} frames\"\n            )\n\n        # Determine confidence levels\n        det_conf, track_conf = _determine_confidence_levels(\n            quality_preset, detection_confidence, tracking_confidence\n        )\n\n        # Track all frames\n        tracker = PoseTracker(\n            min_detection_confidence=det_conf, min_tracking_confidence=track_conf\n        )\n        frames, landmarks_sequence = _process_all_frames(video, tracker, verbose)\n\n        # Auto-tune parameters\n        characteristics = analyze_video_sample(\n            landmarks_sequence, video.fps, video.frame_count\n        )\n        params = auto_tune_parameters(characteristics, quality_preset)\n\n        # Apply expert overrides\n        params = _apply_expert_overrides(\n            params,\n            smoothing_window,\n            velocity_threshold,\n            min_contact_frames,\n            visibility_threshold,\n        )\n\n        if verbose:\n            _print_verbose_parameters(video, characteristics, quality_preset, params)\n\n        # Apply smoothing\n        smoothed_landmarks = _apply_smoothing(landmarks_sequence, params, verbose)\n\n        # Extract foot positions\n        if verbose:\n            print(\"Extracting foot positions...\")\n        vertical_positions, _ = _extract_vertical_positions(smoothed_landmarks)\n        tracking_method = \"foot\"\n\n        # Detect CMJ phases\n        if verbose:\n            print(\"Detecting CMJ phases...\")\n\n        phases = detect_cmj_phases(\n            vertical_positions,\n            video.fps,\n            window_length=params.smoothing_window,\n            polyorder=params.polyorder,\n        )\n\n        if phases is None:\n            raise ValueError(\"Could not detect CMJ phases in video\")\n\n        standing_end, lowest_point, takeoff_frame, landing_frame = phases\n\n        # Calculate metrics\n        if verbose:\n            print(\"Calculating metrics...\")\n\n        # Use signed velocity for CMJ (need direction information)\n        from .cmj.analysis import compute_signed_velocity\n\n        velocities = compute_signed_velocity(\n            vertical_positions,\n            window_length=params.smoothing_window,\n            polyorder=params.polyorder,\n        )\n\n        metrics = calculate_cmj_metrics(\n            vertical_positions,\n            velocities,\n            standing_end,\n            lowest_point,\n            takeoff_frame,\n            landing_frame,\n            video.fps,\n            tracking_method=tracking_method,\n        )\n\n        # Generate outputs if requested\n        _generate_cmj_outputs(\n            output_video,\n            json_output,\n            metrics,\n            frames,\n            smoothed_landmarks,\n            video.width,\n            video.height,\n            video.display_width,\n            video.display_height,\n            video.fps,\n            verbose,\n        )\n\n        if verbose:\n            print(f\"\\nJump height: {metrics.jump_height:.3f}m\")\n            print(f\"Flight time: {metrics.flight_time*1000:.1f}ms\")\n            print(f\"Countermovement depth: {metrics.countermovement_depth:.3f}m\")\n\n        return metrics\n</code></pre>"},{"location":"api/cmj/#kinemotion.api.process_cmj_videos_bulk","title":"process_cmj_videos_bulk","text":"<pre><code>process_cmj_videos_bulk(\n    configs, max_workers=4, progress_callback=None\n)\n</code></pre> <p>Process multiple CMJ videos in parallel using ProcessPoolExecutor.</p> <p>Parameters:</p> Name Type Description Default <code>configs</code> <code>list[CMJVideoConfig]</code> <p>List of CMJVideoConfig objects specifying video paths and parameters</p> required <code>max_workers</code> <code>int</code> <p>Maximum number of parallel workers (default: 4)</p> <code>4</code> <code>progress_callback</code> <code>Callable[[CMJVideoResult], None] | None</code> <p>Optional callback function called after each video completes.              Receives CMJVideoResult object.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[CMJVideoResult]</code> <p>List of CMJVideoResult objects, one per input video, in completion order</p> Example <p>configs = [ ...     CMJVideoConfig(\"video1.mp4\"), ...     CMJVideoConfig(\"video2.mp4\", quality=\"accurate\"), ...     CMJVideoConfig(\"video3.mp4\", output_video=\"debug3.mp4\"), ... ] results = process_cmj_videos_bulk(configs, max_workers=4) for result in results: ...     if result.success: ...         print(f\"{result.video_path}: {result.metrics.jump_height:.3f}m\") ...     else: ...         print(f\"{result.video_path}: FAILED - {result.error}\")</p> Source code in <code>src/kinemotion/api.py</code> <pre><code>def process_cmj_videos_bulk(\n    configs: list[CMJVideoConfig],\n    max_workers: int = 4,\n    progress_callback: Callable[[CMJVideoResult], None] | None = None,\n) -&gt; list[CMJVideoResult]:\n    \"\"\"\n    Process multiple CMJ videos in parallel using ProcessPoolExecutor.\n\n    Args:\n        configs: List of CMJVideoConfig objects specifying video paths and parameters\n        max_workers: Maximum number of parallel workers (default: 4)\n        progress_callback: Optional callback function called after each video completes.\n                         Receives CMJVideoResult object.\n\n    Returns:\n        List of CMJVideoResult objects, one per input video, in completion order\n\n    Example:\n        &gt;&gt;&gt; configs = [\n        ...     CMJVideoConfig(\"video1.mp4\"),\n        ...     CMJVideoConfig(\"video2.mp4\", quality=\"accurate\"),\n        ...     CMJVideoConfig(\"video3.mp4\", output_video=\"debug3.mp4\"),\n        ... ]\n        &gt;&gt;&gt; results = process_cmj_videos_bulk(configs, max_workers=4)\n        &gt;&gt;&gt; for result in results:\n        ...     if result.success:\n        ...         print(f\"{result.video_path}: {result.metrics.jump_height:.3f}m\")\n        ...     else:\n        ...         print(f\"{result.video_path}: FAILED - {result.error}\")\n    \"\"\"\n    results: list[CMJVideoResult] = []\n\n    # Use ProcessPoolExecutor for CPU-bound video processing\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all jobs\n        future_to_config = {\n            executor.submit(_process_cmj_video_wrapper, config): config\n            for config in configs\n        }\n\n        # Process results as they complete\n        for future in as_completed(future_to_config):\n            config = future_to_config[future]\n            result: CMJVideoResult\n\n            try:\n                result = future.result()\n                results.append(result)\n            except Exception as e:\n                result = CMJVideoResult(\n                    video_path=config.video_path, success=False, error=str(e)\n                )\n                results.append(result)\n\n            # Call progress callback if provided\n            if progress_callback:\n                progress_callback(result)\n\n    return results\n</code></pre>"},{"location":"api/cmj/#configuration","title":"Configuration","text":"<p>options: show_root_heading: true show_source: false</p>"},{"location":"api/cmj/#kinemotion.api.CMJVideoConfig","title":"CMJVideoConfig  <code>dataclass</code>","text":"<pre><code>CMJVideoConfig(\n    video_path,\n    quality=\"balanced\",\n    output_video=None,\n    json_output=None,\n    smoothing_window=None,\n    velocity_threshold=None,\n    min_contact_frames=None,\n    visibility_threshold=None,\n    detection_confidence=None,\n    tracking_confidence=None,\n)\n</code></pre> <p>Configuration for processing a single CMJ video.</p>"},{"location":"api/cmj/#results","title":"Results","text":"<p>options: show_root_heading: true show_source: false</p>"},{"location":"api/cmj/#kinemotion.api.CMJVideoResult","title":"CMJVideoResult  <code>dataclass</code>","text":"<pre><code>CMJVideoResult(\n    video_path,\n    success,\n    metrics=None,\n    error=None,\n    processing_time=0.0,\n)\n</code></pre> <p>Result of processing a single CMJ video.</p>"},{"location":"api/cmj/#metrics","title":"Metrics","text":"<p>options: show_root_heading: true show_source: false</p>"},{"location":"api/cmj/#kinemotion.cmj.kinematics.CMJMetrics","title":"CMJMetrics  <code>dataclass</code>","text":"<pre><code>CMJMetrics(\n    jump_height,\n    flight_time,\n    countermovement_depth,\n    eccentric_duration,\n    concentric_duration,\n    total_movement_time,\n    peak_eccentric_velocity,\n    peak_concentric_velocity,\n    transition_time,\n    standing_start_frame,\n    lowest_point_frame,\n    takeoff_frame,\n    landing_frame,\n    video_fps,\n    tracking_method,\n)\n</code></pre> <p>Metrics for a counter movement jump analysis.</p> <p>Attributes:</p> Name Type Description <code>jump_height</code> <code>float</code> <p>Maximum jump height in meters</p> <code>flight_time</code> <code>float</code> <p>Time spent in the air in seconds</p> <code>countermovement_depth</code> <code>float</code> <p>Vertical distance traveled during eccentric phase in meters</p> <code>eccentric_duration</code> <code>float</code> <p>Time from countermovement start to lowest point in seconds</p> <code>concentric_duration</code> <code>float</code> <p>Time from lowest point to takeoff in seconds</p> <code>total_movement_time</code> <code>float</code> <p>Total time from countermovement start to takeoff in seconds</p> <code>peak_eccentric_velocity</code> <code>float</code> <p>Maximum downward velocity during countermovement in m/s</p> <code>peak_concentric_velocity</code> <code>float</code> <p>Maximum upward velocity during propulsion in m/s</p> <code>transition_time</code> <code>float | None</code> <p>Duration at lowest point (amortization phase) in seconds</p> <code>standing_start_frame</code> <code>float | None</code> <p>Frame where standing phase ends (countermovement begins)</p> <code>lowest_point_frame</code> <code>float</code> <p>Frame at lowest point of countermovement</p> <code>takeoff_frame</code> <code>float</code> <p>Frame where athlete leaves ground</p> <code>landing_frame</code> <code>float</code> <p>Frame where athlete lands</p> <code>video_fps</code> <code>float</code> <p>Frames per second of the analyzed video</p> <code>tracking_method</code> <code>str</code> <p>Method used for tracking (\"foot\" or \"com\")</p>"},{"location":"api/cmj/#kinemotion.cmj.kinematics.CMJMetrics.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Convert metrics to JSON-serializable dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with all metrics, converting NumPy types to Python types.</p> Source code in <code>src/kinemotion/cmj/kinematics.py</code> <pre><code>def to_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert metrics to JSON-serializable dictionary.\n\n    Returns:\n        Dictionary with all metrics, converting NumPy types to Python types.\n    \"\"\"\n    return {\n        \"jump_height_m\": float(self.jump_height),\n        \"flight_time_s\": float(self.flight_time),\n        \"countermovement_depth_m\": float(self.countermovement_depth),\n        \"eccentric_duration_s\": float(self.eccentric_duration),\n        \"concentric_duration_s\": float(self.concentric_duration),\n        \"total_movement_time_s\": float(self.total_movement_time),\n        \"peak_eccentric_velocity_m_s\": float(self.peak_eccentric_velocity),\n        \"peak_concentric_velocity_m_s\": float(self.peak_concentric_velocity),\n        \"transition_time_s\": (\n            float(self.transition_time)\n            if self.transition_time is not None\n            else None\n        ),\n        \"standing_start_frame\": (\n            float(self.standing_start_frame)\n            if self.standing_start_frame is not None\n            else None\n        ),\n        \"lowest_point_frame\": float(self.lowest_point_frame),\n        \"takeoff_frame\": float(self.takeoff_frame),\n        \"landing_frame\": float(self.landing_frame),\n        \"video_fps\": float(self.video_fps),\n        \"tracking_method\": self.tracking_method,\n    }\n</code></pre>"},{"location":"api/cmj/#key-differences-from-drop-jump","title":"Key Differences from Drop Jump","text":""},{"location":"api/cmj/#no-calibration-required","title":"No Calibration Required","text":"<p>Unlike drop jumps, CMJ analysis doesn't require a <code>drop_height</code> parameter. All measurements are relative to the starting position.</p>"},{"location":"api/cmj/#backward-search-algorithm","title":"Backward Search Algorithm","text":"<p>CMJ detection uses a backward search algorithm starting from the peak height, making it more robust than forward search.</p>"},{"location":"api/cmj/#signed-velocity","title":"Signed Velocity","text":"<p>CMJ analysis uses signed velocity (direction matters) to distinguish upward vs downward motion phases.</p>"},{"location":"api/cmj/#lateral-view-required","title":"Lateral View Required","text":"<p>CMJ analysis requires a lateral (side) view for accurate depth and triple extension measurements. Front view will not work due to parallax errors.</p>"},{"location":"api/cmj/#triple-extension-analysis","title":"Triple Extension Analysis","text":"<p>The CMJ API includes detailed triple extension analysis:</p> <pre><code>metrics = process_cmj_video(\"cmj.mp4\")\n\n# Triple extension metrics\nprint(f\"Hip extension: {metrics.hip_extension_angle:.1f}\u00b0\")\nprint(f\"Knee extension: {metrics.knee_extension_angle:.1f}\u00b0\")\nprint(f\"Ankle plantar flexion: {metrics.ankle_plantar_flexion_angle:.1f}\u00b0\")\nprint(f\"Overall triple extension: {metrics.triple_extension_percentage:.1f}%\")\n</code></pre> <p>See Triple Extension Technical Documentation for biomechanics details.</p>"},{"location":"api/core/","title":"Core Utilities","text":"<p>Lower-level utilities for advanced usage and custom analysis pipelines.</p>"},{"location":"api/core/#pose-detection","title":"Pose Detection","text":"<p>options: show_root_heading: true show_source: false</p> <p>options: show_root_heading: true show_source: false</p>"},{"location":"api/core/#kinemotion.core.pose.PoseTracker","title":"PoseTracker","text":"<pre><code>PoseTracker(\n    min_detection_confidence=0.5,\n    min_tracking_confidence=0.5,\n)\n</code></pre> <p>Tracks human pose landmarks in video frames using MediaPipe.</p> <p>Initialize the pose tracker.</p> <p>Parameters:</p> Name Type Description Default <code>min_detection_confidence</code> <code>float</code> <p>Minimum confidence for pose detection</p> <code>0.5</code> <code>min_tracking_confidence</code> <code>float</code> <p>Minimum confidence for pose tracking</p> <code>0.5</code> Source code in <code>src/kinemotion/core/pose.py</code> <pre><code>def __init__(\n    self,\n    min_detection_confidence: float = 0.5,\n    min_tracking_confidence: float = 0.5,\n):\n    \"\"\"\n    Initialize the pose tracker.\n\n    Args:\n        min_detection_confidence: Minimum confidence for pose detection\n        min_tracking_confidence: Minimum confidence for pose tracking\n    \"\"\"\n    self.mp_pose = mp.solutions.pose\n    self.pose = self.mp_pose.Pose(\n        min_detection_confidence=min_detection_confidence,\n        min_tracking_confidence=min_tracking_confidence,\n        model_complexity=1,\n    )\n</code></pre>"},{"location":"api/core/#kinemotion.core.pose.PoseTracker.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Release resources.</p> Source code in <code>src/kinemotion/core/pose.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Release resources.\"\"\"\n    self.pose.close()\n</code></pre>"},{"location":"api/core/#kinemotion.core.pose.PoseTracker.process_frame","title":"process_frame","text":"<pre><code>process_frame(frame)\n</code></pre> <p>Process a single frame and extract pose landmarks.</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray</code> <p>BGR image frame</p> required <p>Returns:</p> Type Description <code>dict[str, tuple[float, float, float]] | None</code> <p>Dictionary mapping landmark names to (x, y, visibility) tuples,</p> <code>dict[str, tuple[float, float, float]] | None</code> <p>or None if no pose detected. Coordinates are normalized (0-1).</p> Source code in <code>src/kinemotion/core/pose.py</code> <pre><code>def process_frame(\n    self, frame: np.ndarray\n) -&gt; dict[str, tuple[float, float, float]] | None:\n    \"\"\"\n    Process a single frame and extract pose landmarks.\n\n    Args:\n        frame: BGR image frame\n\n    Returns:\n        Dictionary mapping landmark names to (x, y, visibility) tuples,\n        or None if no pose detected. Coordinates are normalized (0-1).\n    \"\"\"\n    # Convert BGR to RGB\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    # Process the frame\n    results = self.pose.process(rgb_frame)\n\n    if not results.pose_landmarks:\n        return None\n\n    # Extract key landmarks for feet tracking and CoM estimation\n    landmarks = {}\n    landmark_names = {\n        # Feet landmarks\n        self.mp_pose.PoseLandmark.LEFT_ANKLE: \"left_ankle\",\n        self.mp_pose.PoseLandmark.RIGHT_ANKLE: \"right_ankle\",\n        self.mp_pose.PoseLandmark.LEFT_HEEL: \"left_heel\",\n        self.mp_pose.PoseLandmark.RIGHT_HEEL: \"right_heel\",\n        self.mp_pose.PoseLandmark.LEFT_FOOT_INDEX: \"left_foot_index\",\n        self.mp_pose.PoseLandmark.RIGHT_FOOT_INDEX: \"right_foot_index\",\n        # Torso landmarks for CoM estimation\n        self.mp_pose.PoseLandmark.LEFT_HIP: \"left_hip\",\n        self.mp_pose.PoseLandmark.RIGHT_HIP: \"right_hip\",\n        self.mp_pose.PoseLandmark.LEFT_SHOULDER: \"left_shoulder\",\n        self.mp_pose.PoseLandmark.RIGHT_SHOULDER: \"right_shoulder\",\n        # Additional landmarks for better CoM estimation\n        self.mp_pose.PoseLandmark.NOSE: \"nose\",\n        self.mp_pose.PoseLandmark.LEFT_KNEE: \"left_knee\",\n        self.mp_pose.PoseLandmark.RIGHT_KNEE: \"right_knee\",\n    }\n\n    for landmark_id, name in landmark_names.items():\n        lm = results.pose_landmarks.landmark[landmark_id]\n        landmarks[name] = (lm.x, lm.y, lm.visibility)\n\n    return landmarks\n</code></pre>"},{"location":"api/core/#kinemotion.core.pose.compute_center_of_mass","title":"compute_center_of_mass","text":"<pre><code>compute_center_of_mass(landmarks, visibility_threshold=0.5)\n</code></pre> <p>Compute approximate center of mass (CoM) from body landmarks.</p> <p>Uses biomechanical segment weights based on Dempster's body segment parameters: - Head: 8% of body mass (represented by nose) - Trunk (shoulders to hips): 50% of body mass - Thighs: 2 \u00d7 10% = 20% of body mass - Legs (knees to ankles): 2 \u00d7 5% = 10% of body mass - Feet: 2 \u00d7 1.5% = 3% of body mass</p> <p>The CoM is estimated as a weighted average of these segments, with weights corresponding to their proportion of total body mass.</p> <p>Parameters:</p> Name Type Description Default <code>landmarks</code> <code>dict[str, tuple[float, float, float]]</code> <p>Dictionary of landmark positions (x, y, visibility)</p> required <code>visibility_threshold</code> <code>float</code> <p>Minimum visibility to include landmark in calculation</p> <code>0.5</code> <p>Returns:</p> Type Description <code>float</code> <p>(x, y, visibility) tuple for estimated CoM position</p> <code>float</code> <p>visibility = average visibility of all segments used</p> Source code in <code>src/kinemotion/core/pose.py</code> <pre><code>def compute_center_of_mass(\n    landmarks: dict[str, tuple[float, float, float]],\n    visibility_threshold: float = 0.5,\n) -&gt; tuple[float, float, float]:\n    \"\"\"\n    Compute approximate center of mass (CoM) from body landmarks.\n\n    Uses biomechanical segment weights based on Dempster's body segment parameters:\n    - Head: 8% of body mass (represented by nose)\n    - Trunk (shoulders to hips): 50% of body mass\n    - Thighs: 2 \u00d7 10% = 20% of body mass\n    - Legs (knees to ankles): 2 \u00d7 5% = 10% of body mass\n    - Feet: 2 \u00d7 1.5% = 3% of body mass\n\n    The CoM is estimated as a weighted average of these segments, with\n    weights corresponding to their proportion of total body mass.\n\n    Args:\n        landmarks: Dictionary of landmark positions (x, y, visibility)\n        visibility_threshold: Minimum visibility to include landmark in calculation\n\n    Returns:\n        (x, y, visibility) tuple for estimated CoM position\n        visibility = average visibility of all segments used\n    \"\"\"\n    segments: list = []\n    weights: list = []\n    visibilities: list = []\n\n    # Add body segments\n    _add_head_segment(segments, weights, visibilities, landmarks, visibility_threshold)\n    _add_trunk_segment(segments, weights, visibilities, landmarks, visibility_threshold)\n\n    # Add bilateral limb segments\n    for side in [\"left\", \"right\"]:\n        _add_limb_segment(\n            segments,\n            weights,\n            visibilities,\n            landmarks,\n            side,\n            \"hip\",\n            \"knee\",\n            0.10,\n            visibility_threshold,\n        )\n        _add_limb_segment(\n            segments,\n            weights,\n            visibilities,\n            landmarks,\n            side,\n            \"knee\",\n            \"ankle\",\n            0.05,\n            visibility_threshold,\n        )\n        _add_foot_segment(\n            segments, weights, visibilities, landmarks, side, visibility_threshold\n        )\n\n    # Fallback if no segments found\n    if not segments:\n        if \"left_hip\" in landmarks and \"right_hip\" in landmarks:\n            lh_x, lh_y, lh_vis = landmarks[\"left_hip\"]\n            rh_x, rh_y, rh_vis = landmarks[\"right_hip\"]\n            return ((lh_x + rh_x) / 2, (lh_y + rh_y) / 2, (lh_vis + rh_vis) / 2)\n        return (0.5, 0.5, 0.0)\n\n    # Normalize weights and compute weighted average\n    total_weight = sum(weights)\n    normalized_weights = [w / total_weight for w in weights]\n\n    com_x = float(\n        sum(p[0] * w for p, w in zip(segments, normalized_weights, strict=True))\n    )\n    com_y = float(\n        sum(p[1] * w for p, w in zip(segments, normalized_weights, strict=True))\n    )\n    com_visibility = float(np.mean(visibilities)) if visibilities else 0.0\n\n    return (com_x, com_y, com_visibility)\n</code></pre>"},{"location":"api/core/#smoothing-filtering","title":"Smoothing &amp; Filtering","text":"<p>options: show_root_heading: true show_source: false</p> <p>options: show_root_heading: true show_source: false</p>"},{"location":"api/core/#kinemotion.core.smoothing.smooth_landmarks","title":"smooth_landmarks","text":"<pre><code>smooth_landmarks(\n    landmark_sequence, window_length=5, polyorder=2\n)\n</code></pre> <p>Smooth landmark trajectories using Savitzky-Golay filter.</p> <p>Parameters:</p> Name Type Description Default <code>landmark_sequence</code> <code>list[dict[str, tuple[float, float, float]] | None]</code> <p>List of landmark dictionaries from each frame</p> required <code>window_length</code> <code>int</code> <p>Length of filter window (must be odd, &gt;= polyorder + 2)</p> <code>5</code> <code>polyorder</code> <code>int</code> <p>Order of polynomial used to fit samples</p> <code>2</code> <p>Returns:</p> Type Description <code>list[dict[str, tuple[float, float, float]] | None]</code> <p>Smoothed landmark sequence with same structure as input</p> Source code in <code>src/kinemotion/core/smoothing.py</code> <pre><code>def smooth_landmarks(\n    landmark_sequence: list[dict[str, tuple[float, float, float]] | None],\n    window_length: int = 5,\n    polyorder: int = 2,\n) -&gt; list[dict[str, tuple[float, float, float]] | None]:\n    \"\"\"\n    Smooth landmark trajectories using Savitzky-Golay filter.\n\n    Args:\n        landmark_sequence: List of landmark dictionaries from each frame\n        window_length: Length of filter window (must be odd, &gt;= polyorder + 2)\n        polyorder: Order of polynomial used to fit samples\n\n    Returns:\n        Smoothed landmark sequence with same structure as input\n    \"\"\"\n    if len(landmark_sequence) &lt; window_length:\n        return landmark_sequence\n\n    # Ensure window_length is odd\n    if window_length % 2 == 0:\n        window_length += 1\n\n    def savgol_smoother(x_coords, y_coords, _valid_frames):  # type: ignore[no-untyped-def]\n        x_smooth = savgol_filter(x_coords, window_length, polyorder)\n        y_smooth = savgol_filter(y_coords, window_length, polyorder)\n        return x_smooth, y_smooth\n\n    return _smooth_landmarks_core(\n        landmark_sequence, window_length, polyorder, savgol_smoother\n    )\n</code></pre>"},{"location":"api/core/#kinemotion.core.smoothing.smooth_landmarks_advanced","title":"smooth_landmarks_advanced","text":"<pre><code>smooth_landmarks_advanced(\n    landmark_sequence,\n    window_length=5,\n    polyorder=2,\n    use_outlier_rejection=True,\n    use_bilateral=False,\n    ransac_threshold=0.02,\n    bilateral_sigma_spatial=3.0,\n    bilateral_sigma_intensity=0.02,\n)\n</code></pre> <p>Advanced landmark smoothing with outlier rejection and bilateral filtering.</p> <p>Combines multiple techniques for robust smoothing: 1. Outlier rejection (RANSAC + median filtering) 2. Optional bilateral filtering (edge-preserving) 3. Savitzky-Golay smoothing</p> <p>Parameters:</p> Name Type Description Default <code>landmark_sequence</code> <code>list[dict[str, tuple[float, float, float]] | None]</code> <p>List of landmark dictionaries from each frame</p> required <code>window_length</code> <code>int</code> <p>Length of filter window (must be odd, &gt;= polyorder + 2)</p> <code>5</code> <code>polyorder</code> <code>int</code> <p>Order of polynomial used to fit samples</p> <code>2</code> <code>use_outlier_rejection</code> <code>bool</code> <p>Apply outlier detection and removal</p> <code>True</code> <code>use_bilateral</code> <code>bool</code> <p>Use bilateral filter instead of Savitzky-Golay</p> <code>False</code> <code>ransac_threshold</code> <code>float</code> <p>Threshold for RANSAC outlier detection</p> <code>0.02</code> <code>bilateral_sigma_spatial</code> <code>float</code> <p>Spatial sigma for bilateral filter</p> <code>3.0</code> <code>bilateral_sigma_intensity</code> <code>float</code> <p>Intensity sigma for bilateral filter</p> <code>0.02</code> <p>Returns:</p> Type Description <code>list[dict[str, tuple[float, float, float]] | None]</code> <p>Smoothed landmark sequence with same structure as input</p> Source code in <code>src/kinemotion/core/smoothing.py</code> <pre><code>def smooth_landmarks_advanced(\n    landmark_sequence: list[dict[str, tuple[float, float, float]] | None],\n    window_length: int = 5,\n    polyorder: int = 2,\n    use_outlier_rejection: bool = True,\n    use_bilateral: bool = False,\n    ransac_threshold: float = 0.02,\n    bilateral_sigma_spatial: float = 3.0,\n    bilateral_sigma_intensity: float = 0.02,\n) -&gt; list[dict[str, tuple[float, float, float]] | None]:\n    \"\"\"\n    Advanced landmark smoothing with outlier rejection and bilateral filtering.\n\n    Combines multiple techniques for robust smoothing:\n    1. Outlier rejection (RANSAC + median filtering)\n    2. Optional bilateral filtering (edge-preserving)\n    3. Savitzky-Golay smoothing\n\n    Args:\n        landmark_sequence: List of landmark dictionaries from each frame\n        window_length: Length of filter window (must be odd, &gt;= polyorder + 2)\n        polyorder: Order of polynomial used to fit samples\n        use_outlier_rejection: Apply outlier detection and removal\n        use_bilateral: Use bilateral filter instead of Savitzky-Golay\n        ransac_threshold: Threshold for RANSAC outlier detection\n        bilateral_sigma_spatial: Spatial sigma for bilateral filter\n        bilateral_sigma_intensity: Intensity sigma for bilateral filter\n\n    Returns:\n        Smoothed landmark sequence with same structure as input\n    \"\"\"\n    if len(landmark_sequence) &lt; window_length:\n        return landmark_sequence\n\n    # Ensure window_length is odd\n    if window_length % 2 == 0:\n        window_length += 1\n\n    def advanced_smoother(x_coords, y_coords, _valid_frames):  # type: ignore[no-untyped-def]\n        x_array = np.array(x_coords)\n        y_array = np.array(y_coords)\n\n        # Step 1: Outlier rejection\n        if use_outlier_rejection:\n            x_array, _ = reject_outliers(\n                x_array,\n                use_ransac=True,\n                use_median=True,\n                ransac_threshold=ransac_threshold,\n            )\n            y_array, _ = reject_outliers(\n                y_array,\n                use_ransac=True,\n                use_median=True,\n                ransac_threshold=ransac_threshold,\n            )\n\n        # Step 2: Smoothing (bilateral or Savitzky-Golay)\n        if use_bilateral:\n            x_smooth = bilateral_temporal_filter(\n                x_array,\n                window_size=window_length,\n                sigma_spatial=bilateral_sigma_spatial,\n                sigma_intensity=bilateral_sigma_intensity,\n            )\n            y_smooth = bilateral_temporal_filter(\n                y_array,\n                window_size=window_length,\n                sigma_spatial=bilateral_sigma_spatial,\n                sigma_intensity=bilateral_sigma_intensity,\n            )\n        else:\n            # Standard Savitzky-Golay\n            x_smooth = savgol_filter(x_array, window_length, polyorder)\n            y_smooth = savgol_filter(y_array, window_length, polyorder)\n\n        return x_smooth, y_smooth\n\n    return _smooth_landmarks_core(\n        landmark_sequence, window_length, polyorder, advanced_smoother\n    )\n</code></pre>"},{"location":"api/core/#video-processing","title":"Video Processing","text":"<p>options: show_root_heading: true show_source: false</p>"},{"location":"api/core/#kinemotion.core.video_io.VideoProcessor","title":"VideoProcessor","text":"<pre><code>VideoProcessor(video_path)\n</code></pre> <p>Handles video reading and processing.</p> <p>IMPORTANT: This class preserves the exact aspect ratio of the source video. No dimensions are hardcoded - all dimensions are extracted from actual frame data.</p> <p>Initialize video processor.</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to input video file</p> required Source code in <code>src/kinemotion/core/video_io.py</code> <pre><code>def __init__(self, video_path: str):\n    \"\"\"\n    Initialize video processor.\n\n    Args:\n        video_path: Path to input video file\n    \"\"\"\n    self.video_path = video_path\n    self.cap = cv2.VideoCapture(video_path)\n\n    if not self.cap.isOpened():\n        raise ValueError(f\"Could not open video: {video_path}\")\n\n    self.fps = self.cap.get(cv2.CAP_PROP_FPS)\n    self.frame_count = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Read first frame to get actual dimensions\n    # This is critical for preserving aspect ratio, especially with mobile videos\n    # that have rotation metadata. OpenCV properties (CAP_PROP_FRAME_WIDTH/HEIGHT)\n    # may return incorrect dimensions, so we read the actual frame data.\n    ret, first_frame = self.cap.read()\n    if ret:\n        # frame.shape is (height, width, channels) - extract actual dimensions\n        self.height, self.width = first_frame.shape[:2]\n        self.cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reset to beginning\n    else:\n        # Fallback to video properties if can't read frame\n        self.width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        self.height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Extract rotation metadata from video (iPhones store rotation in side_data_list)\n    # OpenCV ignores rotation metadata, so we need to extract and apply it manually\n    self.rotation = 0  # Will be set by _extract_video_metadata()\n\n    # Calculate display dimensions considering SAR (Sample Aspect Ratio)\n    # Mobile videos often have non-square pixels encoded in SAR metadata\n    # OpenCV doesn't directly expose SAR, but we need to handle display correctly\n    self.display_width = self.width\n    self.display_height = self.height\n    self._extract_video_metadata()\n\n    # Apply rotation to dimensions if needed\n    if self.rotation in [90, -90, 270]:\n        # Swap dimensions for 90/-90 degree rotations\n        self.width, self.height = self.height, self.width\n        self.display_width, self.display_height = (\n            self.display_height,\n            self.display_width,\n        )\n</code></pre>"},{"location":"api/core/#kinemotion.core.video_io.VideoProcessor.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Release video capture.</p> Source code in <code>src/kinemotion/core/video_io.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Release video capture.\"\"\"\n    self.cap.release()\n</code></pre>"},{"location":"api/core/#kinemotion.core.video_io.VideoProcessor.read_frame","title":"read_frame","text":"<pre><code>read_frame()\n</code></pre> <p>Read next frame from video and apply rotation if needed.</p> <p>OpenCV ignores rotation metadata, so we manually apply rotation based on the display matrix metadata extracted from the video.</p> Source code in <code>src/kinemotion/core/video_io.py</code> <pre><code>def read_frame(self) -&gt; np.ndarray | None:\n    \"\"\"\n    Read next frame from video and apply rotation if needed.\n\n    OpenCV ignores rotation metadata, so we manually apply rotation\n    based on the display matrix metadata extracted from the video.\n    \"\"\"\n    ret, frame = self.cap.read()\n    if not ret:\n        return None\n\n    # Apply rotation if video has rotation metadata\n    if self.rotation == -90 or self.rotation == 270:\n        # -90 degrees = rotate 90 degrees clockwise\n        frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n    elif self.rotation == 90 or self.rotation == -270:\n        # 90 degrees = rotate 90 degrees counter-clockwise\n        frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n    elif self.rotation == 180 or self.rotation == -180:\n        # 180 degrees rotation\n        frame = cv2.rotate(frame, cv2.ROTATE_180)\n\n    return frame\n</code></pre>"},{"location":"api/core/#auto-tuning","title":"Auto-Tuning","text":"<p>options: show_root_heading: true show_source: false</p> <p>options: show_root_heading: true show_source: false</p> <p>options: show_root_heading: true show_source: false</p>"},{"location":"api/core/#kinemotion.core.auto_tuning.auto_tune_parameters","title":"auto_tune_parameters","text":"<pre><code>auto_tune_parameters(\n    characteristics, quality_preset=BALANCED\n)\n</code></pre> <p>Automatically tune analysis parameters based on video characteristics.</p> <p>This function implements heuristics to select optimal parameters without requiring user expertise in video analysis or kinematic tracking.</p> <p>Key principles: 1. FPS-based scaling: Higher fps needs lower velocity thresholds 2. Quality-based smoothing: Noisy video needs more smoothing 3. Always enable proven features: outlier rejection, curvature analysis 4. Preset modifiers: fast/balanced/accurate adjust base parameters</p> <p>Parameters:</p> Name Type Description Default <code>characteristics</code> <code>VideoCharacteristics</code> <p>Analyzed video characteristics</p> required <code>quality_preset</code> <code>QualityPreset</code> <p>Quality vs speed tradeoff</p> <code>BALANCED</code> <p>Returns:</p> Type Description <code>AnalysisParameters</code> <p>AnalysisParameters with auto-tuned values</p> Source code in <code>src/kinemotion/core/auto_tuning.py</code> <pre><code>def auto_tune_parameters(\n    characteristics: VideoCharacteristics,\n    quality_preset: QualityPreset = QualityPreset.BALANCED,\n) -&gt; AnalysisParameters:\n    \"\"\"\n    Automatically tune analysis parameters based on video characteristics.\n\n    This function implements heuristics to select optimal parameters without\n    requiring user expertise in video analysis or kinematic tracking.\n\n    Key principles:\n    1. FPS-based scaling: Higher fps needs lower velocity thresholds\n    2. Quality-based smoothing: Noisy video needs more smoothing\n    3. Always enable proven features: outlier rejection, curvature analysis\n    4. Preset modifiers: fast/balanced/accurate adjust base parameters\n\n    Args:\n        characteristics: Analyzed video characteristics\n        quality_preset: Quality vs speed tradeoff\n\n    Returns:\n        AnalysisParameters with auto-tuned values\n    \"\"\"\n    fps = characteristics.fps\n    quality = characteristics.tracking_quality\n\n    # =================================================================\n    # STEP 1: FPS-based baseline parameters\n    # These scale automatically with frame rate to maintain consistent\n    # temporal resolution and sensitivity\n    # =================================================================\n\n    # Velocity threshold: Scale inversely with fps\n    # At 30fps, feet move ~2% of frame per frame when \"stationary\"\n    # At 60fps, feet move ~1% of frame per frame when \"stationary\"\n    # Formula: threshold = 0.02 * (30 / fps)\n    base_velocity_threshold = 0.02 * (30.0 / fps)\n\n    # Min contact frames: Scale with fps to maintain same time duration\n    # Goal: ~100ms minimum contact (3 frames @ 30fps, 6 frames @ 60fps)\n    # Formula: frames = round(3 * (fps / 30))\n    base_min_contact_frames = max(2, round(3.0 * (fps / 30.0)))\n\n    # Smoothing window: Decrease with higher fps for better temporal resolution\n    # Lower fps (30fps): 5-frame window = 167ms\n    # Higher fps (60fps): 3-frame window = 50ms (same temporal resolution)\n    if fps &lt;= 30:\n        base_smoothing_window = 5\n    elif fps &lt;= 60:\n        base_smoothing_window = 3\n    else:\n        base_smoothing_window = 3  # Even at 120fps, 3 is minimum for Savitzky-Golay\n\n    # =================================================================\n    # STEP 2: Quality-based adjustments\n    # Adapt smoothing and filtering based on tracking quality\n    # =================================================================\n\n    smoothing_adjustment = 0\n    enable_bilateral = False\n\n    if quality == \"low\":\n        # Poor tracking quality: aggressive smoothing and filtering\n        smoothing_adjustment = +2\n        enable_bilateral = True\n    elif quality == \"medium\":\n        # Moderate quality: slight smoothing increase\n        smoothing_adjustment = +1\n        enable_bilateral = True\n    else:  # high quality\n        # Good tracking: preserve detail, minimal smoothing\n        smoothing_adjustment = 0\n        enable_bilateral = False\n\n    # =================================================================\n    # STEP 3: Apply quality preset modifiers\n    # User can choose speed vs accuracy tradeoff\n    # =================================================================\n\n    if quality_preset == QualityPreset.FAST:\n        # Fast: Trade accuracy for speed\n        velocity_threshold = base_velocity_threshold * 1.5  # Less sensitive\n        min_contact_frames = max(2, int(base_min_contact_frames * 0.67))\n        smoothing_window = max(3, base_smoothing_window - 2 + smoothing_adjustment)\n        bilateral_filter = False  # Skip expensive filtering\n        detection_confidence = 0.3\n        tracking_confidence = 0.3\n\n    elif quality_preset == QualityPreset.ACCURATE:\n        # Accurate: Maximize accuracy, accept slower processing\n        velocity_threshold = base_velocity_threshold * 0.5  # More sensitive\n        min_contact_frames = (\n            base_min_contact_frames  # Don't increase (would miss brief)\n        )\n        smoothing_window = min(11, base_smoothing_window + 2 + smoothing_adjustment)\n        bilateral_filter = True  # Always use for best accuracy\n        detection_confidence = 0.6\n        tracking_confidence = 0.6\n\n    else:  # QualityPreset.BALANCED (default)\n        # Balanced: Good accuracy, reasonable speed\n        velocity_threshold = base_velocity_threshold\n        min_contact_frames = base_min_contact_frames\n        smoothing_window = max(3, base_smoothing_window + smoothing_adjustment)\n        bilateral_filter = enable_bilateral\n        detection_confidence = 0.5\n        tracking_confidence = 0.5\n\n    # Ensure smoothing window is odd (required for Savitzky-Golay)\n    if smoothing_window % 2 == 0:\n        smoothing_window += 1\n\n    # =================================================================\n    # STEP 4: Set fixed optimal values\n    # These are always the same regardless of video characteristics\n    # =================================================================\n\n    # Polyorder: Always 2 (quadratic) - optimal for jump physics (parabolic motion)\n    polyorder = 2\n\n    # Visibility threshold: Standard MediaPipe threshold\n    visibility_threshold = 0.5\n\n    # Always enable proven accuracy features\n    outlier_rejection = True  # Removes tracking glitches (minimal cost)\n    use_curvature = True  # Trajectory curvature analysis (minimal cost)\n\n    return AnalysisParameters(\n        smoothing_window=smoothing_window,\n        polyorder=polyorder,\n        velocity_threshold=velocity_threshold,\n        min_contact_frames=min_contact_frames,\n        visibility_threshold=visibility_threshold,\n        detection_confidence=detection_confidence,\n        tracking_confidence=tracking_confidence,\n        outlier_rejection=outlier_rejection,\n        bilateral_filter=bilateral_filter,\n        use_curvature=use_curvature,\n    )\n</code></pre>"},{"location":"api/core/#kinemotion.core.auto_tuning.analyze_video_sample","title":"analyze_video_sample","text":"<pre><code>analyze_video_sample(landmarks_sequence, fps, frame_count)\n</code></pre> <p>Analyze video characteristics from a sample of frames.</p> <p>This function should be called after tracking the first 30-60 frames to understand video quality and characteristics.</p> <p>Parameters:</p> Name Type Description Default <code>landmarks_sequence</code> <code>list[dict[str, tuple[float, float, float]] | None]</code> <p>Tracked landmarks from sample frames</p> required <code>fps</code> <code>float</code> <p>Video frame rate</p> required <code>frame_count</code> <code>int</code> <p>Total number of frames in video</p> required <p>Returns:</p> Type Description <code>VideoCharacteristics</code> <p>VideoCharacteristics with analyzed properties</p> Source code in <code>src/kinemotion/core/auto_tuning.py</code> <pre><code>def analyze_video_sample(\n    landmarks_sequence: list[dict[str, tuple[float, float, float]] | None],\n    fps: float,\n    frame_count: int,\n) -&gt; VideoCharacteristics:\n    \"\"\"\n    Analyze video characteristics from a sample of frames.\n\n    This function should be called after tracking the first 30-60 frames\n    to understand video quality and characteristics.\n\n    Args:\n        landmarks_sequence: Tracked landmarks from sample frames\n        fps: Video frame rate\n        frame_count: Total number of frames in video\n\n    Returns:\n        VideoCharacteristics with analyzed properties\n    \"\"\"\n    visibilities = []\n    positions = []\n\n    # Collect visibility and position data from all frames\n    for frame_landmarks in landmarks_sequence:\n        if not frame_landmarks:\n            continue\n\n        frame_vis, frame_y_positions = _collect_foot_visibility_and_positions(\n            frame_landmarks\n        )\n\n        if frame_vis:\n            visibilities.append(float(np.mean(frame_vis)))\n        if frame_y_positions:\n            positions.append(float(np.mean(frame_y_positions)))\n\n    # Compute metrics\n    avg_visibility = float(np.mean(visibilities)) if visibilities else 0.5\n    position_variance = float(np.var(positions)) if len(positions) &gt; 1 else 0.0\n\n    # Determine tracking quality\n    tracking_quality = analyze_tracking_quality(avg_visibility)\n\n    # Check for stable period (indicates drop jump from elevated platform)\n    has_stable_period = _check_stable_period(positions)\n\n    return VideoCharacteristics(\n        fps=fps,\n        frame_count=frame_count,\n        avg_visibility=avg_visibility,\n        position_variance=position_variance,\n        has_stable_period=has_stable_period,\n        tracking_quality=tracking_quality,\n    )\n</code></pre>"},{"location":"api/core/#kinemotion.core.auto_tuning.QualityPreset","title":"QualityPreset","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Quality presets for analysis.</p>"},{"location":"api/core/#usage-example","title":"Usage Example","text":"<pre><code>from kinemotion.core.pose import PoseTracker\nfrom kinemotion.core.smoothing import smooth_landmarks\nfrom kinemotion.core.video_io import VideoProcessor\n\n# Initialize pose tracker\ntracker = PoseTracker(\n    min_detection_confidence=0.5,\n    min_tracking_confidence=0.5\n)\n\n# Process video\nvideo = VideoProcessor(\"video.mp4\")\nlandmarks = []\n\nfor frame in video:\n    result = tracker.process_frame(frame)\n    if result:\n        landmarks.append(result)\n\n# Apply smoothing\nsmoothed = smooth_landmarks(\n    landmarks,\n    window_length=13,\n    polyorder=3\n)\n</code></pre>"},{"location":"api/dropjump/","title":"Drop Jump API","text":"<p>The drop jump API provides functions for analyzing drop jump videos and extracting kinematic metrics.</p>"},{"location":"api/dropjump/#quick-example","title":"Quick Example","text":"<pre><code>from kinemotion import process_dropjump_video\n\nmetrics = process_dropjump_video(\n    video_path=\"dropjump.mp4\",\n    quality=\"balanced\",  # fast, balanced, or accurate\n    output_video=\"debug.mp4\",  # optional\n    verbose=True\n)\n\nprint(f\"Ground contact time: {metrics.ground_contact_time:.3f}s\")\nprint(f\"Flight time: {metrics.flight_time:.3f}s\")\nprint(f\"RSI: {metrics.reactive_strength_index:.2f}\")\n</code></pre>"},{"location":"api/dropjump/#main-functions","title":"Main Functions","text":"<p>options: show_root_heading: true show_source: false</p> <p>options: show_root_heading: true show_source: false</p>"},{"location":"api/dropjump/#kinemotion.api.process_dropjump_video","title":"process_dropjump_video","text":"<pre><code>process_dropjump_video(\n    video_path,\n    quality=\"balanced\",\n    output_video=None,\n    json_output=None,\n    drop_start_frame=None,\n    smoothing_window=None,\n    velocity_threshold=None,\n    min_contact_frames=None,\n    visibility_threshold=None,\n    detection_confidence=None,\n    tracking_confidence=None,\n    verbose=False,\n)\n</code></pre> <p>Process a single drop jump video and return metrics.</p> <p>Jump height is calculated from flight time using kinematic formula (h = g*t\u00b2/8).</p> <p>Parameters:</p> Name Type Description Default <code>video_path</code> <code>str</code> <p>Path to the input video file</p> required <code>quality</code> <code>str</code> <p>Analysis quality preset (\"fast\", \"balanced\", or \"accurate\")</p> <code>'balanced'</code> <code>output_video</code> <code>str | None</code> <p>Optional path for debug video output</p> <code>None</code> <code>json_output</code> <code>str | None</code> <p>Optional path for JSON metrics output</p> <code>None</code> <code>drop_start_frame</code> <code>int | None</code> <p>Optional manual drop start frame</p> <code>None</code> <code>smoothing_window</code> <code>int | None</code> <p>Optional override for smoothing window</p> <code>None</code> <code>velocity_threshold</code> <code>float | None</code> <p>Optional override for velocity threshold</p> <code>None</code> <code>min_contact_frames</code> <code>int | None</code> <p>Optional override for minimum contact frames</p> <code>None</code> <code>visibility_threshold</code> <code>float | None</code> <p>Optional override for visibility threshold</p> <code>None</code> <code>detection_confidence</code> <code>float | None</code> <p>Optional override for pose detection confidence</p> <code>None</code> <code>tracking_confidence</code> <code>float | None</code> <p>Optional override for pose tracking confidence</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Print processing details</p> <code>False</code> <p>Returns:</p> Type Description <code>DropJumpMetrics</code> <p>DropJumpMetrics object containing analysis results</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If video cannot be processed or parameters are invalid</p> <code>FileNotFoundError</code> <p>If video file does not exist</p> Source code in <code>src/kinemotion/api.py</code> <pre><code>def process_dropjump_video(\n    video_path: str,\n    quality: str = \"balanced\",\n    output_video: str | None = None,\n    json_output: str | None = None,\n    drop_start_frame: int | None = None,\n    smoothing_window: int | None = None,\n    velocity_threshold: float | None = None,\n    min_contact_frames: int | None = None,\n    visibility_threshold: float | None = None,\n    detection_confidence: float | None = None,\n    tracking_confidence: float | None = None,\n    verbose: bool = False,\n) -&gt; DropJumpMetrics:\n    \"\"\"\n    Process a single drop jump video and return metrics.\n\n    Jump height is calculated from flight time using kinematic formula (h = g*t\u00b2/8).\n\n    Args:\n        video_path: Path to the input video file\n        quality: Analysis quality preset (\"fast\", \"balanced\", or \"accurate\")\n        output_video: Optional path for debug video output\n        json_output: Optional path for JSON metrics output\n        drop_start_frame: Optional manual drop start frame\n        smoothing_window: Optional override for smoothing window\n        velocity_threshold: Optional override for velocity threshold\n        min_contact_frames: Optional override for minimum contact frames\n        visibility_threshold: Optional override for visibility threshold\n        detection_confidence: Optional override for pose detection confidence\n        tracking_confidence: Optional override for pose tracking confidence\n        verbose: Print processing details\n\n    Returns:\n        DropJumpMetrics object containing analysis results\n\n    Raises:\n        ValueError: If video cannot be processed or parameters are invalid\n        FileNotFoundError: If video file does not exist\n    \"\"\"\n    if not Path(video_path).exists():\n        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n\n    # Convert quality string to enum\n    quality_preset = _parse_quality_preset(quality)\n\n    # Initialize video processor\n    with VideoProcessor(video_path) as video:\n        if verbose:\n            print(\n                f\"Video: {video.width}x{video.height} @ {video.fps:.2f} fps, \"\n                f\"{video.frame_count} frames\"\n            )\n\n        # Determine detection/tracking confidence levels\n        detection_conf, tracking_conf = _determine_confidence_levels(\n            quality_preset, detection_confidence, tracking_confidence\n        )\n\n        # Process all frames with pose tracking\n        tracker = PoseTracker(\n            min_detection_confidence=detection_conf,\n            min_tracking_confidence=tracking_conf,\n        )\n        frames, landmarks_sequence = _process_all_frames(video, tracker, verbose)\n\n        # Analyze video characteristics and auto-tune parameters\n        characteristics = analyze_video_sample(\n            landmarks_sequence, video.fps, video.frame_count\n        )\n        params = auto_tune_parameters(characteristics, quality_preset)\n\n        # Apply expert overrides if provided\n        params = _apply_expert_overrides(\n            params,\n            smoothing_window,\n            velocity_threshold,\n            min_contact_frames,\n            visibility_threshold,\n        )\n\n        # Show selected parameters if verbose\n        if verbose:\n            _print_verbose_parameters(video, characteristics, quality_preset, params)\n\n        # Apply smoothing with auto-tuned parameters\n        smoothed_landmarks = _apply_smoothing(landmarks_sequence, params, verbose)\n\n        # Extract vertical positions from feet\n        if verbose:\n            print(\"Extracting foot positions...\")\n        vertical_positions, visibilities = _extract_vertical_positions(\n            smoothed_landmarks\n        )\n\n        # Detect ground contact\n        contact_states = detect_ground_contact(\n            vertical_positions,\n            velocity_threshold=params.velocity_threshold,\n            min_contact_frames=params.min_contact_frames,\n            visibility_threshold=params.visibility_threshold,\n            visibilities=visibilities,\n            window_length=params.smoothing_window,\n            polyorder=params.polyorder,\n        )\n\n        # Calculate metrics\n        if verbose:\n            print(\"Calculating metrics...\")\n\n        metrics = calculate_drop_jump_metrics(\n            contact_states,\n            vertical_positions,\n            video.fps,\n            drop_start_frame=drop_start_frame,\n            velocity_threshold=params.velocity_threshold,\n            smoothing_window=params.smoothing_window,\n            polyorder=params.polyorder,\n            use_curvature=params.use_curvature,\n        )\n\n        # Generate outputs (JSON and debug video)\n        _generate_outputs(\n            metrics,\n            json_output,\n            output_video,\n            frames,\n            smoothed_landmarks,\n            contact_states,\n            video,\n            verbose,\n        )\n\n        if verbose:\n            print(\"Analysis complete!\")\n\n        return metrics\n</code></pre>"},{"location":"api/dropjump/#kinemotion.api.process_dropjump_videos_bulk","title":"process_dropjump_videos_bulk","text":"<pre><code>process_dropjump_videos_bulk(\n    configs, max_workers=4, progress_callback=None\n)\n</code></pre> <p>Process multiple drop jump videos in parallel using ProcessPoolExecutor.</p> <p>Parameters:</p> Name Type Description Default <code>configs</code> <code>list[DropJumpVideoConfig]</code> <p>List of DropJumpVideoConfig objects specifying video paths and parameters</p> required <code>max_workers</code> <code>int</code> <p>Maximum number of parallel workers (default: 4)</p> <code>4</code> <code>progress_callback</code> <code>Callable[[DropJumpVideoResult], None] | None</code> <p>Optional callback function called after each video completes.              Receives DropJumpVideoResult object.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[DropJumpVideoResult]</code> <p>List of DropJumpVideoResult objects, one per input video, in completion order</p> Example <p>configs = [ ...     DropJumpVideoConfig(\"video1.mp4\"), ...     DropJumpVideoConfig(\"video2.mp4\", quality=\"accurate\"), ...     DropJumpVideoConfig(\"video3.mp4\", output_video=\"debug3.mp4\"), ... ] results = process_dropjump_videos_bulk(configs, max_workers=4) for result in results: ...     if result.success: ...         print(f\"{result.video_path}: {result.metrics.jump_height_m:.3f}m\") ...     else: ...         print(f\"{result.video_path}: FAILED - {result.error}\")</p> Source code in <code>src/kinemotion/api.py</code> <pre><code>def process_dropjump_videos_bulk(\n    configs: list[DropJumpVideoConfig],\n    max_workers: int = 4,\n    progress_callback: Callable[[DropJumpVideoResult], None] | None = None,\n) -&gt; list[DropJumpVideoResult]:\n    \"\"\"\n    Process multiple drop jump videos in parallel using ProcessPoolExecutor.\n\n    Args:\n        configs: List of DropJumpVideoConfig objects specifying video paths and parameters\n        max_workers: Maximum number of parallel workers (default: 4)\n        progress_callback: Optional callback function called after each video completes.\n                         Receives DropJumpVideoResult object.\n\n    Returns:\n        List of DropJumpVideoResult objects, one per input video, in completion order\n\n    Example:\n        &gt;&gt;&gt; configs = [\n        ...     DropJumpVideoConfig(\"video1.mp4\"),\n        ...     DropJumpVideoConfig(\"video2.mp4\", quality=\"accurate\"),\n        ...     DropJumpVideoConfig(\"video3.mp4\", output_video=\"debug3.mp4\"),\n        ... ]\n        &gt;&gt;&gt; results = process_dropjump_videos_bulk(configs, max_workers=4)\n        &gt;&gt;&gt; for result in results:\n        ...     if result.success:\n        ...         print(f\"{result.video_path}: {result.metrics.jump_height_m:.3f}m\")\n        ...     else:\n        ...         print(f\"{result.video_path}: FAILED - {result.error}\")\n    \"\"\"\n    results: list[DropJumpVideoResult] = []\n\n    # Use ProcessPoolExecutor for CPU-bound video processing\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all jobs\n        future_to_config = {\n            executor.submit(_process_dropjump_video_wrapper, config): config\n            for config in configs\n        }\n\n        # Process results as they complete\n        for future in as_completed(future_to_config):\n            config = future_to_config[future]\n            result: DropJumpVideoResult\n\n            try:\n                result = future.result()\n            except Exception as exc:\n                # Handle unexpected errors\n                result = DropJumpVideoResult(\n                    video_path=config.video_path,\n                    success=False,\n                    error=f\"Unexpected error: {str(exc)}\",\n                )\n\n            results.append(result)\n\n            # Call progress callback if provided\n            if progress_callback:\n                progress_callback(result)\n\n    return results\n</code></pre>"},{"location":"api/dropjump/#configuration","title":"Configuration","text":"<p>options: show_root_heading: true show_source: false</p>"},{"location":"api/dropjump/#kinemotion.api.DropJumpVideoConfig","title":"DropJumpVideoConfig  <code>dataclass</code>","text":"<pre><code>DropJumpVideoConfig(\n    video_path,\n    quality=\"balanced\",\n    output_video=None,\n    json_output=None,\n    drop_start_frame=None,\n    smoothing_window=None,\n    velocity_threshold=None,\n    min_contact_frames=None,\n    visibility_threshold=None,\n    detection_confidence=None,\n    tracking_confidence=None,\n)\n</code></pre> <p>Configuration for processing a single drop jump video.</p>"},{"location":"api/dropjump/#results","title":"Results","text":"<p>options: show_root_heading: true show_source: false</p>"},{"location":"api/dropjump/#kinemotion.api.DropJumpVideoResult","title":"DropJumpVideoResult  <code>dataclass</code>","text":"<pre><code>DropJumpVideoResult(\n    video_path,\n    success,\n    metrics=None,\n    error=None,\n    processing_time=0.0,\n)\n</code></pre> <p>Result of processing a single drop jump video.</p>"},{"location":"api/dropjump/#metrics","title":"Metrics","text":"<p>options: show_root_heading: true show_source: false</p>"},{"location":"api/dropjump/#kinemotion.dropjump.kinematics.DropJumpMetrics","title":"DropJumpMetrics","text":"<pre><code>DropJumpMetrics()\n</code></pre> <p>Container for drop-jump analysis metrics.</p> Source code in <code>src/kinemotion/dropjump/kinematics.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.ground_contact_time: float | None = None\n    self.flight_time: float | None = None\n    self.jump_height: float | None = None\n    self.jump_height_kinematic: float | None = None  # From flight time\n    self.jump_height_trajectory: float | None = None  # From position tracking\n    self.contact_start_frame: int | None = None\n    self.contact_end_frame: int | None = None\n    self.flight_start_frame: int | None = None\n    self.flight_end_frame: int | None = None\n    self.peak_height_frame: int | None = None\n    # Fractional frame indices for sub-frame precision timing\n    self.contact_start_frame_precise: float | None = None\n    self.contact_end_frame_precise: float | None = None\n    self.flight_start_frame_precise: float | None = None\n    self.flight_end_frame_precise: float | None = None\n</code></pre>"},{"location":"api/dropjump/#kinemotion.dropjump.kinematics.DropJumpMetrics.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Convert metrics to dictionary for JSON output.</p> Source code in <code>src/kinemotion/dropjump/kinematics.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Convert metrics to dictionary for JSON output.\"\"\"\n    return {\n        \"ground_contact_time_ms\": (\n            round(self.ground_contact_time * 1000, 2)\n            if self.ground_contact_time is not None\n            else None\n        ),\n        \"flight_time_ms\": (\n            round(self.flight_time * 1000, 2)\n            if self.flight_time is not None\n            else None\n        ),\n        \"jump_height_m\": (\n            round(self.jump_height, 3) if self.jump_height is not None else None\n        ),\n        \"jump_height_kinematic_m\": (\n            round(self.jump_height_kinematic, 3)\n            if self.jump_height_kinematic is not None\n            else None\n        ),\n        \"jump_height_trajectory_normalized\": (\n            round(self.jump_height_trajectory, 4)\n            if self.jump_height_trajectory is not None\n            else None\n        ),\n        \"contact_start_frame\": (\n            int(self.contact_start_frame)\n            if self.contact_start_frame is not None\n            else None\n        ),\n        \"contact_end_frame\": (\n            int(self.contact_end_frame)\n            if self.contact_end_frame is not None\n            else None\n        ),\n        \"flight_start_frame\": (\n            int(self.flight_start_frame)\n            if self.flight_start_frame is not None\n            else None\n        ),\n        \"flight_end_frame\": (\n            int(self.flight_end_frame)\n            if self.flight_end_frame is not None\n            else None\n        ),\n        \"peak_height_frame\": (\n            int(self.peak_height_frame)\n            if self.peak_height_frame is not None\n            else None\n        ),\n        \"contact_start_frame_precise\": (\n            round(self.contact_start_frame_precise, 3)\n            if self.contact_start_frame_precise is not None\n            else None\n        ),\n        \"contact_end_frame_precise\": (\n            round(self.contact_end_frame_precise, 3)\n            if self.contact_end_frame_precise is not None\n            else None\n        ),\n        \"flight_start_frame_precise\": (\n            round(self.flight_start_frame_precise, 3)\n            if self.flight_start_frame_precise is not None\n            else None\n        ),\n        \"flight_end_frame_precise\": (\n            round(self.flight_end_frame_precise, 3)\n            if self.flight_end_frame_precise is not None\n            else None\n        ),\n    }\n</code></pre>"},{"location":"api/dropjump/#key-parameters","title":"Key Parameters","text":""},{"location":"api/dropjump/#quality","title":"quality","text":"<p>Analysis quality preset that determines processing speed and accuracy. The system automatically tunes parameters based on video characteristics and the selected preset.</p> <p>Options:</p> <ul> <li><code>\"fast\"</code> - Quick processing, lower precision</li> <li><code>\"balanced\"</code> - Default, good for most cases</li> <li><code>\"accurate\"</code> - Research-grade, slower processing</li> </ul> <p>Default: <code>\"balanced\"</code></p> <pre><code>metrics = process_dropjump_video(\"video.mp4\", quality=\"accurate\")\n</code></pre>"},{"location":"api/dropjump/#output_video","title":"output_video","text":"<p>Path to write debug video with overlay visualization. If not provided, no debug video is created.</p> <pre><code>metrics = process_dropjump_video(\n    \"video.mp4\",\n    quality=\"balanced\",\n    output_video=\"debug.mp4\"\n)\n</code></pre>"},{"location":"api/dropjump/#json_output","title":"json_output","text":"<p>Path to write JSON metrics output. If not provided, metrics are only returned as a Python object.</p> <pre><code>metrics = process_dropjump_video(\n    \"video.mp4\",\n    json_output=\"metrics.json\"\n)\n</code></pre>"},{"location":"api/dropjump/#expert-parameters","title":"Expert Parameters","text":"<p>For advanced users, you can override auto-tuned parameters:</p> <ul> <li><code>smoothing_window</code> - Override auto-tuned smoothing window size</li> <li><code>velocity_threshold</code> - Override velocity threshold for ground contact detection</li> <li><code>min_contact_frames</code> - Override minimum contact frames</li> <li><code>visibility_threshold</code> - Override visibility threshold</li> <li><code>detection_confidence</code> - Override pose detection confidence</li> <li><code>tracking_confidence</code> - Override pose tracking confidence</li> <li><code>drop_start_frame</code> - Manually specify frame where drop begins</li> </ul>"},{"location":"api/overview/","title":"API Overview","text":"<p>Kinemotion provides a Python API for video-based kinematic analysis. The API is organized around two main jump types:</p>"},{"location":"api/overview/#main-functions","title":"Main Functions","text":""},{"location":"api/overview/#drop-jump-analysis","title":"Drop Jump Analysis","text":"<p>Process drop jump videos and extract kinematic metrics:</p> <ul> <li><code>process_dropjump_video()</code> - Analyze a single drop jump video</li> <li><code>process_dropjump_videos_bulk()</code> - Batch process multiple drop jump videos</li> <li><code>DropJumpVideoConfig</code> - Configuration for drop jump analysis</li> <li><code>DropJumpVideoResult</code> - Results from drop jump analysis</li> <li><code>DropJumpMetrics</code> - Kinematic metrics for drop jumps</li> </ul> <p>See Drop Jump API for detailed documentation.</p>"},{"location":"api/overview/#cmj-analysis","title":"CMJ Analysis","text":"<p>Process counter movement jump videos and extract kinematic metrics:</p> <ul> <li><code>process_cmj_video()</code> - Analyze a single CMJ video</li> <li><code>process_cmj_videos_bulk()</code> - Batch process multiple CMJ videos</li> <li><code>CMJVideoConfig</code> - Configuration for CMJ analysis</li> <li><code>CMJVideoResult</code> - Results from CMJ analysis</li> <li><code>CMJMetrics</code> - Kinematic metrics for CMJs</li> </ul> <p>See CMJ API for detailed documentation.</p>"},{"location":"api/overview/#basic-usage","title":"Basic Usage","text":"<pre><code>from kinemotion import process_dropjump_video, process_cmj_video\n\n# Drop jump analysis\ndrop_metrics = process_dropjump_video(\"dropjump.mp4\", quality=\"balanced\")\n\n# CMJ analysis\ncmj_metrics = process_cmj_video(\"cmj.mp4\", quality=\"balanced\")\n</code></pre>"},{"location":"api/overview/#batch-processing","title":"Batch Processing","text":"<pre><code>from kinemotion import (\n    DropJumpVideoConfig,\n    CMJVideoConfig,\n    process_dropjump_videos_bulk,\n    process_cmj_videos_bulk\n)\n\n# Batch drop jump analysis\nconfigs = [\n    DropJumpVideoConfig(\"video1.mp4\", quality=\"balanced\"),\n    DropJumpVideoConfig(\"video2.mp4\", quality=\"accurate\"),\n]\nresults = process_dropjump_videos_bulk(configs, max_workers=4)\n\n# Batch CMJ analysis\ncmj_configs = [\n    CMJVideoConfig(\"cmj1.mp4\", quality=\"balanced\"),\n    CMJVideoConfig(\"cmj2.mp4\", quality=\"accurate\"),\n]\ncmj_results = process_cmj_videos_bulk(cmj_configs, max_workers=4)\n</code></pre>"},{"location":"api/overview/#core-utilities","title":"Core Utilities","text":"<p>For advanced usage, you can access lower-level utilities:</p> <ul> <li>Pose detection and tracking</li> <li>Velocity computation</li> <li>Smoothing and filtering</li> <li>Video I/O with rotation handling</li> </ul> <p>See Core Utilities for detailed documentation.</p>"},{"location":"development/errors-findings/","title":"Kinemotion Project - Error and Inconsistency Findings","text":"<p>Date: 2025-01-26 Reviewer: Code Review Analysis Status: Critical issues identified requiring immediate attention</p>"},{"location":"development/errors-findings/#executive-summary","title":"Executive Summary","text":"<p>A comprehensive review of the Kinemotion project revealed significant inconsistencies between documentation and implementation, unsubstantiated accuracy claims, and architectural issues that prevent users from accessing implemented features. While code quality metrics are good (tests pass, type checking compliant, linting clean), the project suffers from fundamental credibility and usability problems.</p>"},{"location":"development/errors-findings/#critical-issues-by-category","title":"Critical Issues by Category","text":""},{"location":"development/errors-findings/#1-documentation-vs-implementation-mismatches","title":"1. Documentation vs Implementation Mismatches","text":""},{"location":"development/errors-findings/#11-file-location-errors","title":"1.1 File Location Errors","text":"<ul> <li>Issue: CLAUDE.md incorrectly states <code>video_io.py</code> is in <code>dropjump/</code> directory</li> <li>Reality: File is actually located in <code>core/video_io.py</code></li> <li>Evidence:</li> <li>CLAUDE.md lines referencing \"dropjump/video_io.py\"</li> <li>Actual file path: <code>src/kinemotion/core/video_io.py</code></li> <li>Impact: Misleads developers about project structure</li> </ul>"},{"location":"development/errors-findings/#12-non-existent-cli-features","title":"1.2 Non-Existent CLI Features","text":"<ul> <li>Issue: Documentation references <code>--use-com</code> and <code>--adaptive-threshold</code> parameters</li> <li>Reality: These CLI parameters do not exist</li> <li>Evidence:</li> <li><code>cli.py</code>: No such parameters defined</li> <li><code>cli.py:342</code>: Hardcoded <code>use_com=False</code></li> <li>PARAMETERS.md:18 admits these are \"only in core/ modules\"</li> <li>Impact: Users cannot access advertised features</li> </ul>"},{"location":"development/errors-findings/#2-unvalidated-accuracy-claims","title":"2. Unvalidated Accuracy Claims","text":""},{"location":"development/errors-findings/#21-fabricated-accuracy-percentages","title":"2.1 Fabricated Accuracy Percentages","text":"<ul> <li>Issue: Specific accuracy claims without any validation data</li> <li>Claims:</li> <li>README.md: \"~88% accuracy (vs 71% uncalibrated)\"</li> <li>Various features claim \"+1-2% accuracy improvement\"</li> <li>Evidence:</li> <li>No validation tests in codebase</li> <li>No comparison to ground truth data</li> <li>PARAMETERS.md:3 admits \"accuracy is currently unvalidated\"</li> <li>Impact: Undermines project credibility</li> </ul>"},{"location":"development/errors-findings/#22-arbitrary-correction-factor","title":"2.2 Arbitrary Correction Factor","text":"<ul> <li>Issue: 1.35x \"empirical correction factor\" without justification</li> <li>Location: <code>kinematics.py:323-324</code></li> <li>Claim: \"empirical correction factor\"</li> <li>Reality: No empirical data provided</li> <li>Impact: 35% adjustment suggests fundamental measurement errors</li> </ul>"},{"location":"development/errors-findings/#3-algorithmic-inconsistencies","title":"3. Algorithmic Inconsistencies","text":""},{"location":"development/errors-findings/#31-velocity-calculation-methods","title":"3.1 Velocity Calculation Methods","text":"<ul> <li>Documentation Claims: Uses derivative-based velocity from Savitzky-Golay filter</li> <li>Initial Detection Reality: <code>analysis.py:119</code> uses simple <code>np.diff()</code></li> <li>Interpolation Phase: Does use <code>compute_velocity_from_derivative()</code></li> <li>Impact: Inconsistent velocity calculation methods within same pipeline</li> </ul>"},{"location":"development/errors-findings/#32-inconsistent-smoothing-application","title":"3.2 Inconsistent Smoothing Application","text":"<ul> <li>Issue: Documentation emphasizes smooth velocity, but initial detection uses raw differences</li> <li>Evidence:</li> <li><code>detect_ground_contact()</code> at <code>analysis.py:119</code>: <code>velocities = np.diff(foot_positions, prepend=foot_positions[0])</code></li> <li><code>find_interpolated_phase_transitions()</code> at <code>analysis.py:252</code>: Uses <code>compute_velocity_from_derivative()</code></li> <li>Impact: Noisy initial detection, smooth interpolation - algorithmic inconsistency</li> </ul>"},{"location":"development/errors-findings/#4-implemented-but-inaccessible-features","title":"4. Implemented but Inaccessible Features","text":""},{"location":"development/errors-findings/#41-center-of-mass-com-tracking","title":"4.1 Center of Mass (CoM) Tracking","text":"<ul> <li>Implementation Status: Fully implemented</li> <li>Location: <code>core/pose.py:85-221</code> - <code>compute_center_of_mass()</code></li> <li>Quality: Uses proper Dempster's biomechanical segment parameters</li> <li>Visualization: <code>debug_overlay.py:88-100</code> supports CoM rendering</li> <li>Problem: No CLI parameter to enable it</li> <li>Evidence: <code>cli.py:342</code> hardcodes <code>use_com=False</code></li> </ul>"},{"location":"development/errors-findings/#42-adaptive-threshold-calculation","title":"4.2 Adaptive Threshold Calculation","text":"<ul> <li>Implementation Status: Fully implemented and tested</li> <li>Location: <code>analysis.py:21-89</code> - <code>calculate_adaptive_threshold()</code></li> <li>Tests: 10 comprehensive tests in <code>test_adaptive_threshold.py</code></li> <li>Problem: Never called by CLI or analysis pipeline</li> <li>Impact: Advanced feature sits unused</li> </ul>"},{"location":"development/errors-findings/#5-physics-and-mathematical-concerns","title":"5. Physics and Mathematical Concerns","text":""},{"location":"development/errors-findings/#51-jump-height-formula-compensation","title":"5.1 Jump Height Formula Compensation","text":"<ul> <li>Correct Formula: <code>h = (g * t\u00b2) / 8</code> at <code>kinematics.py:294</code></li> <li>Problem: Applies 1.35x multiplier to \"correct\" the result</li> <li>Implication: 35% correction suggests:</li> <li>Contact detection timing errors</li> <li>Frame rate limitations not properly handled</li> <li>Foot vs CoM tracking discrepancies</li> <li>Proper Solution: Fix root causes, not apply arbitrary multipliers</li> </ul>"},{"location":"development/errors-findings/#6-type-safety-issues","title":"6. Type Safety Issues","text":""},{"location":"development/errors-findings/#61-return-type-inconsistencies","title":"6.1 Return Type Inconsistencies","text":"<ul> <li>Location: <code>smoothing.py:209</code></li> <li>Issue: Returns numpy array from fallback without proper type annotation</li> <li>Pattern: Multiple <code># type: ignore</code> comments throughout codebase</li> <li>Impact: Reduces benefits of strict type checking</li> </ul>"},{"location":"development/errors-findings/#62-type-ignore-proliferation","title":"6.2 Type Ignore Proliferation","text":"<ul> <li>Count: Numerous <code># type: ignore</code> annotations</li> <li>Locations: Throughout smoothing.py and analysis.py</li> <li>Concern: Masking potential type errors rather than fixing them</li> </ul>"},{"location":"development/errors-findings/#7-test-coverage-gaps","title":"7. Test Coverage Gaps","text":""},{"location":"development/errors-findings/#71-missing-integration-tests","title":"7.1 Missing Integration Tests","text":"<ul> <li>No end-to-end CLI testing</li> <li>No video processing validation</li> <li>No accuracy verification tests</li> <li>No ground truth comparison</li> </ul>"},{"location":"development/errors-findings/#72-tests-for-unused-features","title":"7.2 Tests for Unused Features","text":"<ul> <li>Adaptive Threshold: 10 tests for feature never exposed to users</li> <li>CoM Calculation: 6 tests for feature users cannot access</li> <li>Waste: Testing code that provides no user value</li> </ul>"},{"location":"development/errors-findings/#8-architecture-and-design-issues","title":"8. Architecture and Design Issues","text":""},{"location":"development/errors-findings/#81-poor-feature-integration","title":"8.1 Poor Feature Integration","text":"<ul> <li>Pattern: Features implemented in core/ but not connected to CLI</li> <li>Examples: CoM tracking, adaptive thresholds</li> <li>Impact: Significant development effort wasted on inaccessible features</li> </ul>"},{"location":"development/errors-findings/#82-misleading-modularity","title":"8.2 Misleading Modularity","text":"<ul> <li>Appearance: Well-organized module structure</li> <li>Reality: Modules don't properly integrate</li> <li>Result: Features exist in isolation, not as cohesive system</li> </ul>"},{"location":"development/errors-findings/#issue-severity-classification","title":"Issue Severity Classification","text":""},{"location":"development/errors-findings/#critical-blocks-usagecredibility","title":"\ud83d\udd34 Critical (Blocks Usage/Credibility)","text":"<ol> <li>False accuracy claims (71%, 88%, 1.35x factor)</li> <li>Documented features that don't exist</li> <li>Arbitrary correction factors without validation</li> </ol>"},{"location":"development/errors-findings/#major-significant-problems","title":"\ud83d\udfe0 Major (Significant Problems)","text":"<ol> <li>File location documentation errors</li> <li>Velocity calculation inconsistency</li> <li>Fully implemented features not exposed</li> <li>No integration testing</li> <li>Algorithmic inconsistencies</li> </ol>"},{"location":"development/errors-findings/#minor-quality-issues","title":"\ud83d\udfe1 Minor (Quality Issues)","text":"<ol> <li>Type annotation gaps</li> <li>Excessive type ignore usage</li> <li>Unclear architectural decisions</li> </ol>"},{"location":"development/errors-findings/#impact-analysis","title":"Impact Analysis","text":""},{"location":"development/errors-findings/#user-impact","title":"User Impact","text":"<ul> <li>Cannot access advertised features (CoM, adaptive threshold)</li> <li>May trust inaccurate accuracy claims</li> <li>Confusion from documentation/reality mismatch</li> </ul>"},{"location":"development/errors-findings/#developer-impact","title":"Developer Impact","text":"<ul> <li>Misleading documentation wastes time</li> <li>Unclear which features are actually available</li> <li>Type safety compromised by workarounds</li> </ul>"},{"location":"development/errors-findings/#project-credibility","title":"Project Credibility","text":"<ul> <li>Unsubstantiated claims damage trust</li> <li>Gap between implementation and accessibility suggests poor planning</li> <li>35% correction factor indicates fundamental problems</li> </ul>"},{"location":"development/errors-findings/#root-cause-analysis","title":"Root Cause Analysis","text":""},{"location":"development/errors-findings/#primary-causes","title":"Primary Causes","text":"<ol> <li>Evolution without refactoring: Features added to core without CLI integration</li> <li>Documentation drift: Docs not updated as code changed</li> <li>No validation culture: Accuracy claims made without testing</li> <li>Poor architectural planning: No clear path from feature to user</li> </ol>"},{"location":"development/errors-findings/#contributing-factors","title":"Contributing Factors","text":"<ul> <li>Lack of integration tests to catch disconnects</li> <li>No continuous documentation validation</li> <li>Possible rush to claim features before implementation complete</li> </ul>"},{"location":"development/errors-findings/#positive-aspects-not-to-lose-during-fixes","title":"Positive Aspects (Not to Lose During Fixes)","text":"<p>\u2705 Code Quality</p> <ul> <li>All 47 unit tests pass</li> <li>Full mypy strict mode compliance</li> <li>Clean ruff linting</li> <li>Good module separation</li> </ul> <p>\u2705 Technical Implementation</p> <ul> <li>Sophisticated algorithms (when connected)</li> <li>Proper biomechanical calculations</li> <li>Advanced filtering options</li> <li>Sub-frame interpolation</li> </ul> <p>\u2705 Documentation Effort</p> <ul> <li>Comprehensive PARAMETERS.md</li> <li>Detailed technical explanations</li> <li>Good inline code comments</li> </ul>"},{"location":"development/errors-findings/#recommendations-priority","title":"Recommendations Priority","text":""},{"location":"development/errors-findings/#immediate-day-1","title":"Immediate (Day 1)","text":"<ol> <li>Remove or caveat all accuracy claims</li> <li>Fix file location documentation</li> <li>Add warning about unvalidated accuracy</li> </ol>"},{"location":"development/errors-findings/#short-term-week-1","title":"Short-term (Week 1)","text":"<ol> <li>Connect CoM tracking to CLI</li> <li>Connect adaptive threshold to CLI</li> <li>Fix velocity calculation consistency</li> <li>Update all documentation</li> </ol>"},{"location":"development/errors-findings/#medium-term-month-1","title":"Medium-term (Month 1)","text":"<ol> <li>Conduct validation study</li> <li>Add integration tests</li> <li>Remove or justify correction factor</li> <li>Refactor feature architecture</li> </ol>"},{"location":"development/errors-findings/#long-term-quarter-1","title":"Long-term (Quarter 1)","text":"<ol> <li>Establish continuous documentation validation</li> <li>Create ground truth test dataset</li> <li>Implement proper accuracy benchmarking</li> <li>Redesign CLI/core integration</li> </ol>"},{"location":"development/errors-findings/#conclusion","title":"Conclusion","text":"<p>The Kinemotion project demonstrates strong technical implementation skills but suffers from poor system integration and unsubstantiated claims. The disconnect between sophisticated core features and user accessibility, combined with specific accuracy claims lacking validation, creates a credibility crisis that must be addressed before the tool can be considered reliable for research or practical use.</p> <p>The most critical issue is the false accuracy claims - these should be removed immediately or replaced with clear disclaimers about the unvalidated nature of the measurements.</p>"},{"location":"development/validation-plan/","title":"Kinemotion Validation Plan","text":"<p>Status: \ud83d\udccb HOBBY PROJECT - Practical Validation Approach Created: 2025-01-26 Last Updated: 2025-11-01 (Auto-tuning system added) Purpose: Realistic, low-cost validation roadmap for a hobby project</p>"},{"location":"development/validation-plan/#current-status","title":"\u26a0\ufe0f Current Status","text":"<p>IMPORTANT: Kinemotion measurements are currently unvalidated. This document outlines a practical, affordable validation approach suitable for a hobby project. The goal is to establish \"reasonable accuracy\" rather than research-grade validation.</p> <p>NEW (November 2025): Kinemotion now features intelligent auto-tuning that eliminates manual parameter adjustment. The tool automatically:</p> <ul> <li>Detects video FPS and adjusts velocity thresholds accordingly (30/60/120fps)</li> <li>Analyzes tracking quality and adapts smoothing</li> <li>Auto-detects drop start frame (no manual specification needed)</li> <li>Handles iPhone rotation metadata automatically</li> </ul> <p>This makes validation testing easier - no need to guess optimal parameters!</p>"},{"location":"development/validation-plan/#1-validation-philosophy-for-hobby-projects","title":"1. Validation Philosophy for Hobby Projects","text":""},{"location":"development/validation-plan/#what-validation-means-here","title":"What \"Validation\" Means Here","text":"<p>For a hobby project, validation means:</p> <ul> <li>\u2705 Reasonable accuracy: Measurements are \"in the ballpark\" for practical use</li> <li>\u2705 Consistency: Repeated measurements give similar results</li> <li>\u2705 Sanity checks: Results make physical sense (jump height from flight time checks out)</li> <li>\u2705 Comparative accuracy: Similar to other free/affordable tools (My Jump Lab, jump mats)</li> <li>\u274c NOT research-grade: Not validated against force plates or motion capture systems</li> </ul>"},{"location":"development/validation-plan/#realistic-goals","title":"Realistic Goals","text":"<ul> <li>Establish that Kinemotion provides useful estimates for athletes and coaches</li> <li>Identify conditions where it works well (and where it doesn't)</li> <li>Compare against other accessible tools</li> <li>Build confidence through community validation</li> <li>No academic publication required - just honest assessment</li> </ul>"},{"location":"development/validation-plan/#2-frame-rate-recommendations","title":"2. Frame Rate Recommendations","text":"<p>IMPORTANT: Frame rate significantly impacts timing accuracy for drop jump analysis.</p>"},{"location":"development/validation-plan/#frame-rate-vs-timing-accuracy","title":"Frame Rate vs. Timing Accuracy","text":"Frame Rate Time per Frame Event Detection Error Contact Time Error* Recommendation 30fps 33.3ms \u00b117-33ms \u00b110-20% \u26a0\ufe0f Minimum viable 60fps 16.7ms \u00b18-17ms \u00b15-10% \u2705 Recommended 120fps 8.3ms \u00b14-8ms \u00b12-5% \ud83c\udfaf Ideal 240fps 4.2ms \u00b12-4ms \u00b11-3% \ud83c\udfc6 Excellent <p>*For typical drop jump contact times of 150-300ms</p>"},{"location":"development/validation-plan/#our-recommendation","title":"Our Recommendation","text":"<p>For validation and general use:</p> <ul> <li>\ud83c\udfaf Target: 60fps or higher (most modern phones support this)</li> <li>\u2705 Minimum: 30fps (works but with reduced timing accuracy)</li> <li>\ud83c\udfc6 Ideal: 120fps slow-motion if available (best precision)</li> </ul> <p>Why 60fps minimum:</p> <ul> <li>Ground contact times are often 150-300ms</li> <li>At 30fps: \u00b130ms timing error = 10-20% uncertainty</li> <li>At 60fps: \u00b115ms timing error = 5-10% uncertainty</li> <li>Modern smartphones easily support 60fps recording</li> </ul> <p>Note: All validation in this document assumes 60fps as the baseline unless otherwise specified. Adjust acceptance criteria if using 30fps (double the timing error tolerances).</p>"},{"location":"development/validation-plan/#3-free-validation-methods-budget-0","title":"3. Free Validation Methods (Budget: $0)","text":"<p>These methods cost nothing and can be done immediately by any user.</p>"},{"location":"development/validation-plan/#31-compare-against-my-jump-lab-app","title":"3.1 Compare Against My Jump Lab App","text":"<p>What it is: My Jump Lab (also known as My Jump 3) is a popular iPhone/Android app that calculates jump height from slow-motion video. It's been validated in multiple research studies (ICC &gt; 0.95 vs. force plates).</p> <p>Download: iOS App Store | Google Play</p> <p>How to do it:</p> <ol> <li>Download My Jump Lab app from the links above</li> <li>Record 10-20 jumps (countermovement or drop jumps)</li> <li>Process same video with both My Jump Lab and Kinemotion</li> <li>Compare jump heights, contact times, flight times</li> <li>Calculate: mean difference, correlation, percentage error</li> </ol> <p>What to expect:</p> <ul> <li>If correlation r &gt; 0.85: Good agreement</li> <li>If mean difference \\&lt; 5cm for jump height: Acceptable</li> <li>If contact/flight times within \u00b130ms: Reasonable for 30fps video</li> </ul> <p>Time required: 2-3 hours</p> <p>Validation value: \u2b50\u2b50\u2b50\u2b50\u2b50 (High - My Jump Lab is well-validated)</p>"},{"location":"development/validation-plan/#32-tracker-video-analysis-tool","title":"3.2 Tracker Video Analysis Tool","text":"<p>What it is: Tracker (https://physlets.org/tracker/) is a free, open-source video analysis tool widely used in physics education. It allows frame-by-frame position tracking with sub-pixel accuracy.</p> <p>Why it's excellent for validation:</p> <ul> <li>\u2705 Free and open-source</li> <li>\u2705 Used in academic settings (established credibility)</li> <li>\u2705 Tracks full trajectories (not just flight time)</li> <li>\u2705 Export position data for detailed comparison</li> <li>\u2705 Calibration tools (use known reference length)</li> <li>\u2705 Sub-pixel tracking accuracy</li> </ul> <p>How to do it:</p> <ol> <li>Download Tracker from https://physlets.org/tracker/</li> <li>Open your drop jump video in Tracker</li> <li>Set coordinate system (origin at ground level)</li> <li>Calibrate scale using known reference (e.g., drop box height)</li> <li>Track ankle/heel position frame-by-frame (autotrack feature available)</li> <li>Export position data (time vs. vertical position)</li> <li>Identify takeoff/landing from position data</li> <li>Calculate jump height from trajectory</li> <li>Compare with Kinemotion results</li> </ol> <p>What to compare:</p> <ul> <li>Jump height: Direct position measurement vs. Kinemotion</li> <li>Event timing: Landing/takeoff frames from trajectory analysis</li> <li>Trajectory shape: Compare full position curves</li> <li>Contact/flight times: Derived from position tracking</li> </ul> <p>What to expect:</p> <ul> <li>Tracker accuracy: \u00b10.5-1cm (with proper calibration)</li> <li>Should agree within \u00b13-5cm with Kinemotion</li> <li>Validates both position tracking AND event detection</li> <li>Can identify systematic biases (e.g., Kinemotion consistently 2cm higher)</li> </ul> <p>Time required: 2-3 hours (1 hour learning Tracker + 1-2 hours analysis)</p> <p>Validation value: \u2b50\u2b50\u2b50\u2b50\u2b50 (Excellent - rigorous position-based validation, free)</p> <p>Advantages over My Jump Lab:</p> <ul> <li>Tracks full trajectory (not just flight time)</li> <li>Open-source and free (My Jump Lab costs $10-15)</li> <li>More detailed analysis capabilities</li> <li>Established in academic settings</li> </ul>"},{"location":"development/validation-plan/#33-manual-slow-motion-video-analysis","title":"3.3 Manual Slow-Motion Video Analysis","text":"<p>What it is: Frame-by-frame inspection of video to manually identify landing/takeoff frames.</p> <p>How to do it:</p> <ol> <li>Record 5-10 jumps at 60fps or 120fps (slow-motion on phone)</li> <li>Use free video player (VLC, QuickTime) to step through frame-by-frame</li> <li>Manually mark takeoff frame and landing frame</li> <li>Calculate flight time: (frames_between / frame_rate)</li> <li>Calculate jump height: h = g \u00d7 t\u00b2 / 8 (where t = flight time, g = 9.81 m/s\u00b2)</li> <li>Compare with Kinemotion results</li> </ol> <p>What to expect:</p> <ul> <li>Manual analysis accuracy: \u00b11-2 frames (\u00b116-33ms at 60fps)</li> <li>Should agree within \u00b12 frames of Kinemotion's detected events</li> <li>Validates event detection accuracy</li> </ul> <p>Time required: 3-4 hours</p> <p>Validation value: \u2b50\u2b50\u2b50\u2b50 (Good - validates event detection directly)</p>"},{"location":"development/validation-plan/#34-physics-sanity-checks","title":"3.4 Physics Sanity Checks","text":"<p>What it is: Verify measurements make physical sense.</p> <p>Checks to perform:</p> <ol> <li> <p>Jump height from flight time:</p> </li> <li> <p>Calculate: h = g \u00d7 t\u00b2 / 8</p> </li> <li>Compare with position-based height</li> <li> <p>Should agree within 10-15%</p> </li> <li> <p>Velocity at takeoff:</p> </li> <li> <p>Calculate: v = g \u00d7 t / 2 (where t = flight time)</p> </li> <li> <p>Check: Is takeoff velocity reasonable? (1.5-2.5 m/s for typical jumps)</p> </li> <li> <p>Drop height calibration:</p> </li> <li> <p>If you know drop box height (e.g., 40cm)</p> </li> <li>Does calibrated measurement match reality?</li> <li> <p>Test with known reference heights</p> </li> <li> <p>Repeatability:</p> </li> <li> <p>Do 3 identical jumps</p> </li> <li>Results should be within 5-10% of each other</li> <li>High variation suggests measurement issues</li> </ol> <p>Time required: 1-2 hours</p> <p>Validation value: \u2b50\u2b50\u2b50 (Moderate - catches obvious errors)</p>"},{"location":"development/validation-plan/#35-test-retest-reliability","title":"3.5 Test-Retest Reliability","text":"<p>What it is: Measure same jumps twice to check consistency.</p> <p>How to do it:</p> <ol> <li>Record 5 jumps</li> <li>Process with Kinemotion</li> <li>Process same videos again (fresh analysis)</li> <li>Compare results - should be identical (deterministic algorithm)</li> <li>If using different videos of same jumps:</li> <li>Calculate ICC (intraclass correlation)</li> <li>Target: ICC &gt; 0.90 (excellent reliability)</li> </ol> <p>Time required: 1 hour</p> <p>Validation value: \u2b50\u2b50\u2b50\u2b50 (Good - validates algorithm consistency)</p>"},{"location":"development/validation-plan/#4-low-cost-validation-methods-budget-100-500","title":"4. Low-Cost Validation Methods (Budget: $100-500)","text":"<p>Optional methods if you have budget for equipment.</p>"},{"location":"development/validation-plan/#41-jump-mat-comparison-200-500","title":"4.1 Jump Mat Comparison (~$200-500)","text":"<p>What it is: Affordable switch mats that measure contact time and flight time.</p> <p>Options:</p> <ul> <li>DIY jump mat: Build your own with pressure sensors and Arduino (~$50-100)</li> <li>Commercial jump mats:</li> <li>Basic models: $200-400</li> <li>Plyomat: ~$900 (validated against force plates)</li> <li>Just Jump mat: ~$300-400</li> </ul> <p>How to do it:</p> <ol> <li>Purchase or build jump mat</li> <li>Record 20-30 jumps with simultaneous video + jump mat</li> <li>Compare contact times and flight times</li> <li>Calculate correlation and agreement statistics</li> </ol> <p>What to expect:</p> <ul> <li>Jump mats accurate to \u00b110-20ms typically</li> <li>Good target for video-based validation</li> <li>If agreement within \u00b130ms: Acceptable for 30fps video</li> </ul> <p>Time required: 4-6 hours (after equipment acquisition)</p> <p>Validation value: \u2b50\u2b50\u2b50\u2b50\u2b50 (Excellent - independent reference system)</p>"},{"location":"development/validation-plan/#42-optical-timing-gates-100-300","title":"4.2 Optical Timing Gates (~$100-300)","text":"<p>What it is: Infrared sensors that detect when you break a light beam.</p> <p>How to use:</p> <ul> <li>Set up 2 timing gates at ankle height</li> <li>Jump through gates (break beam on takeoff and landing)</li> <li>Measures flight time directly</li> <li>Compare with video-based flight time</li> </ul> <p>Time required: 3-4 hours</p> <p>Validation value: \u2b50\u2b50\u2b50\u2b50 (Good - direct flight time measurement)</p>"},{"location":"development/validation-plan/#5-simple-statistical-analysis","title":"5. Simple Statistical Analysis","text":"<p>No need for complex statistics! Basic comparisons are sufficient:</p>"},{"location":"development/validation-plan/#51-correlation","title":"5.1 Correlation","text":"<ul> <li>Calculate Pearson correlation (r) between Kinemotion and reference</li> <li>Interpretation:</li> <li>r &gt; 0.90: Excellent</li> <li>r = 0.80-0.90: Good</li> <li>r = 0.70-0.80: Acceptable</li> <li>r \\&lt; 0.70: Needs improvement</li> </ul> <p>Tool: Any spreadsheet (Excel, Google Sheets) or Python pandas</p>"},{"location":"development/validation-plan/#52-mean-difference","title":"5.2 Mean Difference","text":"<ul> <li>Calculate: mean(Kinemotion - Reference)</li> <li>Shows systematic bias</li> <li>Example: If mean difference = +3cm, Kinemotion overestimates by 3cm on average</li> </ul>"},{"location":"development/validation-plan/#53-mean-absolute-error-mae","title":"5.3 Mean Absolute Error (MAE)","text":"<ul> <li>Calculate: mean(abs(Kinemotion - Reference))</li> <li>Shows typical error magnitude</li> <li>Target: MAE \\&lt; 5cm for jump height, \\&lt; 30ms for timing at 30fps</li> </ul>"},{"location":"development/validation-plan/#54-percentage-error","title":"5.4 Percentage Error","text":"<ul> <li>Calculate: mean(abs((Kinemotion - Reference) / Reference) \u00d7 100)</li> <li>Shows relative error</li> <li>Target: \\&lt; 10% for practical use</li> </ul> <p>Example Python Code:</p> <pre><code>import numpy as np\nfrom scipy.stats import pearsonr\n\nkinemotion = [28.5, 32.1, 30.4, 29.8, 31.2]  # cm\nreference = [27.9, 31.5, 29.8, 29.2, 30.5]   # cm\n\nr, p = pearsonr(kinemotion, reference)\nmean_diff = np.mean(np.array(kinemotion) - np.array(reference))\nmae = np.mean(np.abs(np.array(kinemotion) - np.array(reference)))\n\nprint(f\"Correlation: r = {r:.3f}, p = {p:.3f}\")\nprint(f\"Mean difference: {mean_diff:.2f} cm\")\nprint(f\"Mean absolute error: {mae:.2f} cm\")\n</code></pre>"},{"location":"development/validation-plan/#6-diy-validation-protocol-for-developer","title":"6. DIY Validation Protocol (For Developer)","text":"<p>Goal: Quick self-validation with minimal resources</p>"},{"location":"development/validation-plan/#phase-1-initial-testing-week-1","title":"Phase 1: Initial Testing (Week 1)","text":""},{"location":"development/validation-plan/#day-1-2-setup","title":"Day 1-2: Setup","text":"<ul> <li>[ ] Download Tracker (https://physlets.org/tracker/) - free and open-source</li> <li>[ ] Alternative: Install My Jump Lab app if iOS device available</li> <li>[ ] Set up recording environment (good lighting, clear background)</li> <li>[ ] Test camera angles and distances</li> </ul>"},{"location":"development/validation-plan/#day-3-5-data-collection","title":"Day 3-5: Data Collection","text":"<ul> <li>[ ] Record 10 countermovement jumps or drop jumps</li> <li>[ ] Include known reference in video (ruler, measuring tape, or known drop box height)</li> <li>[ ] Ensure good video quality (no motion blur, clear foot position)</li> <li>[ ] Use 60fps or higher (recommended for accurate timing measurements)</li> <li>[ ] 30fps acceptable as minimum, but expect \u00b130ms timing error vs \u00b115ms at 60fps</li> </ul>"},{"location":"development/validation-plan/#day-6-7-analysis","title":"Day 6-7: Analysis","text":"<ul> <li>[ ] Process all videos with Kinemotion</li> <li>[ ] Analyze same videos with Tracker (track ankle/heel position)</li> <li>[ ] Alternative: Use My Jump Lab if available</li> <li>[ ] Export Tracker position data and calculate jump height</li> <li>[ ] Calculate correlation, mean difference, MAE</li> <li>[ ] Document results in validation notes</li> </ul>"},{"location":"development/validation-plan/#phase-1-success-criteria","title":"Phase 1 Success Criteria","text":"<ul> <li>Correlation r &gt; 0.85 with Tracker/My Jump Lab</li> <li>Mean difference \\&lt; 5cm for jump height</li> <li>Contact/flight times within \u00b130ms</li> <li>Event detection within \u00b12-3 frames</li> </ul>"},{"location":"development/validation-plan/#phase-2-manual-verification-week-2-3","title":"Phase 2: Manual Verification (Week 2-3)","text":""},{"location":"development/validation-plan/#week-2-frame-by-frame-analysis","title":"Week 2: Frame-by-Frame Analysis","text":"<ul> <li>[ ] Select 5 best quality videos</li> <li>[ ] Manually identify takeoff/landing frames</li> <li>[ ] Compare with Kinemotion's detected events</li> <li>[ ] Document frame differences</li> </ul>"},{"location":"development/validation-plan/#week-3-physics-checks","title":"Week 3: Physics Checks","text":"<ul> <li>[ ] Calculate jump height from flight time (h = g\u00d7t\u00b2/8)</li> <li>[ ] Compare with position-based estimates</li> <li>[ ] Verify velocity calculations make sense</li> <li>[ ] Test repeatability (process same video 3 times)</li> </ul>"},{"location":"development/validation-plan/#phase-2-success-criteria","title":"Phase 2 Success Criteria","text":"<ul> <li>Event detection within \u00b12 frames of manual analysis</li> <li>Physics calculations internally consistent (\\&lt;10% difference)</li> <li>Perfect repeatability (deterministic algorithm)</li> </ul>"},{"location":"development/validation-plan/#phase-3-documentation-week-4","title":"Phase 3: Documentation (Week 4)","text":"<ul> <li>[ ] Write up results in docs/VALIDATION_RESULTS.md</li> <li>[ ] Update README.md with honest accuracy claims</li> <li>[ ] Document conditions where tool works well</li> <li>[ ] Document limitations and error sources</li> <li>[ ] Add disclaimer with validation status</li> </ul>"},{"location":"development/validation-plan/#7-community-validation","title":"7. Community Validation","text":""},{"location":"development/validation-plan/#leverage-user-contributions-to-expand-validation-data","title":"Leverage user contributions to expand validation data","text":""},{"location":"development/validation-plan/#71-invite-user-comparisons","title":"7.1 Invite User Comparisons","text":"<p>Create a validation issue on GitHub:</p> <p>Title: \"Community Validation: Share Your Comparison Data\"</p> <p>Template:</p> <pre><code>Help validate Kinemotion by comparing it with other tools!\n\n**What to do**:\n1. Record jump videos (with phone/camera at 60fps+)\n2. Analyze videos with Kinemotion\n3. Compare with My Jump Lab, jump mat, or manual analysis\n4. Share your results here\n\n**Data to share**:\n- Number of jumps analyzed\n- Reference tool used (My Jump Lab, jump mat, manual)\n- Correlation (if calculated)\n- Mean difference\n- Your assessment (good agreement? systematic bias?)\n\n**Example**:\n- 15 jumps compared with My Jump Lab\n- Correlation: r = 0.91\n- Mean difference: -2.3cm (Kinemotion slightly lower)\n- Assessment: Good agreement for practical use\n</code></pre>"},{"location":"development/validation-plan/#72-aggregate-community-data","title":"7.2 Aggregate Community Data","text":"<ul> <li>Collect user reports in validation spreadsheet</li> <li>Calculate overall statistics across all users</li> <li>Identify patterns (works better at 60fps vs 30fps, etc.)</li> <li>Build confidence through multiple independent validations</li> </ul>"},{"location":"development/validation-plan/#8-acceptance-criteria-for-hobby-project","title":"8. Acceptance Criteria for Hobby Project","text":"<p>\"Good Enough\" Thresholds:</p>"},{"location":"development/validation-plan/#jump-height","title":"Jump Height","text":"<ul> <li>\u2705 Acceptable: MAE \\&lt; 5cm, r &gt; 0.85</li> <li>\u2b50 Good: MAE \\&lt; 3cm, r &gt; 0.90</li> <li>\ud83c\udfc6 Excellent: MAE \\&lt; 2cm, r &gt; 0.95</li> </ul>"},{"location":"development/validation-plan/#contact-time-flight-time","title":"Contact Time / Flight Time","text":"<ul> <li>\u2705 Acceptable: MAE \\&lt; 30ms (at 30fps), r &gt; 0.80</li> <li>\u2b50 Good: MAE \\&lt; 20ms (at 30fps), r &gt; 0.85</li> <li>\ud83c\udfc6 Excellent: MAE \\&lt; 10ms (at 60fps), r &gt; 0.90</li> </ul>"},{"location":"development/validation-plan/#event-detection","title":"Event Detection","text":"<ul> <li>\u2705 Acceptable: Within \u00b13 frames of manual analysis</li> <li>\u2b50 Good: Within \u00b12 frames of manual analysis</li> <li>\ud83c\udfc6 Excellent: Within \u00b11 frame of manual analysis</li> </ul> <p>If these criteria are met: Can claim \"validated for practical use\" with appropriate caveats about video quality, frame rate, and conditions.</p>"},{"location":"development/validation-plan/#9-timeline-resource-summary","title":"9. Timeline &amp; Resource Summary","text":""},{"location":"development/validation-plan/#realistic-timeline-solo-developer","title":"Realistic Timeline (Solo Developer)","text":"<p>Week 1: My Jump Lab comparison (2-3 hours total) Week 2: Manual video analysis (3-4 hours total) Week 3: Physics checks and repeatability (2-3 hours total) Week 4: Documentation and results write-up (2-3 hours total)</p> <p>Total time investment: 10-15 hours over 1 month</p> <p>Optional: Purchase jump mat if budget allows (adds 4-6 hours for testing)</p>"},{"location":"development/validation-plan/#budget-summary","title":"Budget Summary","text":"<p>Minimum (Free):</p> <ul> <li>My Jump Lab app (or use free trial): $0-15</li> <li>Time: 10-15 hours</li> <li>Total: $0-15</li> </ul> <p>Recommended (Low-Cost):</p> <ul> <li>My Jump Lab app: $10-15</li> <li>Basic jump mat: $200-400 (optional)</li> <li>Time: 15-20 hours</li> <li>Total: $10-415</li> </ul> <p>Aspirational (Research-Grade):</p> <ul> <li>All of the above: $10-415</li> <li>Plyomat validated jump mat: $900</li> <li>Lab access for force plate comparison: $0-5000 (if opportunity arises)</li> <li>Total: $910-6315</li> </ul> <p>For hobby project: Stick to minimum or recommended budget!</p>"},{"location":"development/validation-plan/#10-opportunistic-validation","title":"10. Opportunistic Validation","text":""},{"location":"development/validation-plan/#if-lab-access-becomes-available","title":"If Lab Access Becomes Available","text":"<p>Sometimes opportunities arise unexpectedly:</p> <ul> <li>Friend/colleague has access to biomechanics lab</li> <li>Local university offers community access</li> <li>Contact with sports science researcher</li> </ul> <p>If this happens:</p> <ol> <li>Explain your tool and validation goal</li> <li>Ask if you can run 10-20 test jumps for comparison</li> <li>Offer to share results (may help their research too)</li> <li>Be flexible with timing (work around their schedule)</li> </ol> <p>Cost: Usually free if someone gives you access, just your time</p>"},{"location":"development/validation-plan/#11-documenting-results","title":"11. Documenting Results","text":""},{"location":"development/validation-plan/#what-to-document-honest-assessment","title":"What to Document (Honest Assessment)","text":"<p>If validation successful (meets acceptance criteria):</p> <ul> <li>\u2705 State which reference tools were used</li> <li>\u2705 Report correlation and error statistics</li> <li>\u2705 Specify video conditions tested (frame rate, lighting, etc.)</li> <li>\u2705 Note limitations and caveats</li> <li>\u2705 Update README.md with accuracy claims</li> </ul> <p>Example README update:</p> <pre><code>## Validation Status\n\nKinemotion has been validated through comparison with My Jump Lab app and manual video analysis:\n\n- **Jump height**: Correlation r = 0.88, MAE = 4.2cm (n=25 jumps, 30fps video)\n- **Flight time**: Correlation r = 0.91, MAE = 24ms (n=25 jumps, 30fps video)\n- **Event detection**: Within \u00b12 frames of manual analysis\n\n**Tested conditions**: Indoor lighting, 1080p video at 30fps, front camera view\n\n**Limitations**:\n- Not validated against force plates or motion capture\n- Accuracy decreases with poor lighting or motion blur\n- Lower accuracy at 30fps vs 60fps (\u00b130ms vs \u00b115ms timing)\n\nSee docs/VALIDATION_RESULTS.md for full details.\n</code></pre> <p>If validation shows issues:</p> <ul> <li>\u2757 Be transparent about problems found</li> <li>\u2757 Document conditions where tool doesn't work well</li> <li>\u2757 Keep prominent \u26a0\ufe0f warnings in documentation</li> <li>\u2757 Explain what needs improvement</li> </ul>"},{"location":"development/validation-plan/#create-docsvalidation_resultsmd","title":"Create docs/VALIDATION_RESULTS.md","text":"<p>Template:</p> <pre><code># Validation Results\n\n**Date**: [YYYY-MM-DD]\n**Validated by**: [Your name/GitHub handle]\n**Version tested**: [Kinemotion version]\n\n## Summary\n\n[Brief overview of validation approach and key findings]\n\n## Methods\n\n### Reference Tools\n- [List tools used: My Jump Lab, manual analysis, etc.]\n\n### Testing Protocol\n- [Number of jumps, video settings, conditions]\n\n## Results\n\n### Jump Height\n- Correlation: r = [value]\n- Mean difference: [value] cm\n- Mean absolute error: [value] cm\n- Interpretation: [good/acceptable/needs work]\n\n### Contact Time\n- [Same format]\n\n### Flight Time\n- [Same format]\n\n### Event Detection\n- [Frame accuracy comparison]\n\n## Visualizations\n\n[Include scatter plots, Bland-Altman if desired, or just tables]\n\n## Conclusions\n\n### What Works Well\n- [List conditions/scenarios with good accuracy]\n\n### Limitations Found\n- [List conditions where accuracy suffers]\n\n### Recommendations\n- [Advice for users to get best results]\n\n## Raw Data\n\n[Optional: Include CSV or table of all measurements for transparency]\n</code></pre>"},{"location":"development/validation-plan/#12-alternative-trust-but-verify-approach","title":"12. Alternative: \"Trust but Verify\" Approach","text":"<p>Philosophy: Start using the tool, verify it makes sense through practical use.</p>"},{"location":"development/validation-plan/#practical-verification","title":"Practical Verification","text":"<ol> <li> <p>Does it pass the smell test?</p> </li> <li> <p>Do jump heights seem reasonable? (20-40cm for recreational, 40-70cm for trained athletes)</p> </li> <li>Are contact times sensible? (150-300ms typical for drop jumps)</li> <li> <p>Does flight time correlate with perceived jump height?</p> </li> <li> <p>Internal consistency</p> </li> <li> <p>Do better jumps (feel higher) measure higher?</p> </li> <li>Do repeated similar jumps give similar results?</li> <li> <p>Do trends over time make sense (improving with training)?</p> </li> <li> <p>Comparative validation</p> </li> <li> <p>Does athlete A (known to jump higher) measure higher than athlete B?</p> </li> <li>Do measurements track with performance (vertical jump improvement = better game performance)?</li> </ol> <p>When this is sufficient:</p> <ul> <li>Using tool for personal training feedback</li> <li>Tracking relative improvements over time</li> <li>Not making high-stakes decisions based on measurements</li> <li>Comfortable with \"good enough\" accuracy</li> </ul>"},{"location":"development/validation-plan/#13-conclusion","title":"13. Conclusion","text":""},{"location":"development/validation-plan/#validation-is-iterative","title":"Validation is Iterative","text":"<p>You don't need perfect validation on day one. Start simple:</p> <ol> <li>Phase 1: Compare with My Jump Lab (1 week, free)</li> <li>Phase 2: Manual verification (1 week, free)</li> <li>Phase 3: Document results (1 week)</li> <li>Phase 4: Community validation (ongoing)</li> <li>Phase 5: Opportunistic upgrades (if lab access becomes available)</li> </ol>"},{"location":"development/validation-plan/#honest-limitations","title":"Honest Limitations","text":"<p>A hobby project validation will never match research-grade validation, and that's okay:</p> <ul> <li>\u2705 Can establish \"practical accuracy\"</li> <li>\u2705 Can identify obvious problems</li> <li>\u2705 Can build user confidence</li> <li>\u274c Cannot claim research-grade validation</li> <li>\u274c Cannot recommend for scientific studies (without further validation)</li> </ul>"},{"location":"development/validation-plan/#the-goal","title":"The Goal","text":"<p>Provide users with honest, evidence-based information about tool accuracy so they can make informed decisions about whether it meets their needs.</p>"},{"location":"development/validation-plan/#appendix-quick-comparison-hobby-vs-research-validation","title":"Appendix: Quick Comparison - Hobby vs Research Validation","text":"Aspect Hobby Approach Research Approach Budget $0-500 $15,000-30,000 Time 1-3 months 6-12 months Reference My Jump Lab, jump mat Force plates, motion capture Participants Self + volunteers 30-50 recruited participants Statistics Correlation, MAE ICC, Bland-Altman, LOA Ethics None required IRB approval needed Publication GitHub documentation Peer-reviewed journal Outcome \"Practical accuracy\" \"Research-grade validated\" Use cases Personal training, coaching Scientific studies, research <p>For a hobby project: Left column is perfectly appropriate!</p> <p>Document Version: 2.1 (Frame Rate Guidance Added) Last Updated: 2025-01-26 Status: \ud83d\udccb Practical Validation Approach Key Updates:</p> <ul> <li>Added Section 2: Frame Rate Recommendations (60fps recommended, not 30fps)</li> <li>Added Section 3.2: Tracker video analysis tool (free, open-source, rigorous)</li> <li>Clarified that validation assumes 60fps baseline for accuracy targets</li> </ul> <p>Next Steps: Start with Phase 1 (Tracker or My Jump Lab comparison at 60fps+)</p>"},{"location":"development/wallball-norep-detection/","title":"Wall Ball No-Rep Detection - Feature Implementation Plan","text":"<p>Status: \ud83d\udccb PLANNED - Future Feature Created: 2025-11-07 Feasibility: MODERATE-CHALLENGING \u2705 Estimated Effort: 3-4 weeks full implementation</p>"},{"location":"development/wallball-norep-detection/#executive-summary","title":"Executive Summary","text":"<p>This document outlines the implementation plan for adding HYROX Wall Ball no-rep detection to kinemotion. Wall ball is one of eight workout stations in HYROX races, requiring athletes to complete 100 wall ball shots with specific standards:</p> <ul> <li>Squat depth: Hip crease must go below knee</li> <li>Ball height: Must hit target (3m men / 2.7m women)</li> <li>No resting: Ball cannot rest on ground between reps</li> </ul> <p>Verdict: Implementation is feasible with kinemotion's existing architecture. The project already has proven pose tracking (MediaPipe), squat analysis (CMJ), and phase detection algorithms that can be extended for wall ball analysis.</p> <p>Key insight: Use 45-degree camera angle instead of pure lateral view for superior ball tracking while using joint angles for camera-agnostic squat depth validation.</p>"},{"location":"development/wallball-norep-detection/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Core Objective: Rep Counting &amp; Validation Pipeline</li> <li>Wall Ball Exercise Standards</li> <li>Technical Feasibility Analysis</li> <li>Camera Setup - 45\u00b0 Angle Advantage</li> <li>Proposed Architecture</li> <li>Implementation Phases</li> <li>Technical Challenges &amp; Mitigation</li> <li>Testing Strategy</li> <li>Research References</li> <li>Future Enhancements</li> <li>Conclusion</li> </ol>"},{"location":"development/wallball-norep-detection/#1-core-objective-rep-counting-validation-pipeline","title":"1. Core Objective: Rep Counting &amp; Validation Pipeline","text":""},{"location":"development/wallball-norep-detection/#11-the-problem","title":"1.1 The Problem","text":"<p>Input: Video containing 10-100 wall ball reps (typical range, algorithm handles any number)</p> <p>Objective:</p> <ol> <li>Identify each attempted rep (segment video into individual reps)</li> <li>Validate each rep against HYROX standards</li> <li>Report which reps are valid and which are no-reps (with reasons)</li> </ol> <p>Formula: <code>valid_reps = attempted_reps - no_reps</code></p>"},{"location":"development/wallball-norep-detection/#12-two-stage-pipeline","title":"1.2 Two-Stage Pipeline","text":"<p>The implementation uses a detection-then-validation pipeline:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         INPUT VIDEO                              \u2502\n\u2502                    (10-100 wall ball reps)                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    STAGE 1: REP DETECTION                        \u2502\n\u2502                                                                  \u2502\n\u2502  \u2022 Extract pose landmarks per frame (MediaPipe)                 \u2502\n\u2502  \u2022 State machine detects rep boundaries                         \u2502\n\u2502  \u2022 Segment video into individual attempts                       \u2502\n\u2502                                                                  \u2502\n\u2502  Output: List of rep segments                                   \u2502\n\u2502    - Rep 1: frames 0-89 (timestamp 0.0s - 3.0s)                \u2502\n\u2502    - Rep 2: frames 90-185 (timestamp 3.0s - 6.2s)              \u2502\n\u2502    - Rep 3: frames 186-278 (timestamp 6.2s - 9.3s)             \u2502\n\u2502    - ...                                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   STAGE 2: REP VALIDATION                        \u2502\n\u2502                                                                  \u2502\n\u2502  For each detected rep:                                         \u2502\n\u2502    \u2713 Validate squat depth (knee angle &lt; 90\u00b0)                   \u2502\n\u2502    \u2713 Validate ball height (peak \u2265 target)                      \u2502\n\u2502    \u2713 Validate no resting (ball stationary on ground)           \u2502\n\u2502                                                                  \u2502\n\u2502  Output: Per-rep results                                        \u2502\n\u2502    - Rep 1: VALID (all checks passed)                          \u2502\n\u2502    - Rep 2: NO-REP (squat depth insufficient: 95\u00b0 vs 90\u00b0)     \u2502\n\u2502    - Rep 3: VALID                                               \u2502\n\u2502    - Rep 4: NO-REP (ball height: 2.8m vs 3.0m target)         \u2502\n\u2502    - ...                                                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      FINAL OUTPUT                                \u2502\n\u2502                                                                  \u2502\n\u2502  Summary:                                                        \u2502\n\u2502    \u2022 Total attempted: 50 reps                                   \u2502\n\u2502    \u2022 Valid reps: 47                                             \u2502\n\u2502    \u2022 No-reps: 3                                                 \u2502\n\u2502    \u2022 Violation breakdown:                                       \u2502\n\u2502        - Squat depth: 2 violations                              \u2502\n\u2502        - Ball height: 1 violation                               \u2502\n\u2502        - Ball resting: 0 violations                             \u2502\n\u2502                                                                  \u2502\n\u2502  Per-rep details: [see JSON output for complete data]          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"development/wallball-norep-detection/#13-stage-1-rep-detection-state-machine","title":"1.3 Stage 1: Rep Detection (State Machine)","text":"<p>Approach: State machine tracks rep phases to identify complete cycles</p> <p>Rep phases (inspired by YOLOv8 AI Gym exercise counters):</p> <pre><code>STANDING \u2192 DESCENDING \u2192 BOTTOM \u2192 ASCENDING \u2192 THROWING \u2192 FLIGHT \u2192 CATCHING \u2192 RECOVERY \u2192 STANDING\n    \u2191___________________________________________________________________________________|\n                                (one complete rep)\n</code></pre> <p>State transitions based on:</p> <ul> <li>Hip vertical position: Detect squat descent/ascent</li> <li>Hip vertical velocity: Detect direction changes (descent \u2194 ascent)</li> <li>Hand height: Detect throw (hands up) and catch (hands down)</li> <li>Ball position: Detect flight phase</li> </ul> <p>Rep boundary detection:</p> <ul> <li>Rep start: Transition from STANDING to DESCENDING</li> <li>Rep end: Transition from RECOVERY to STANDING (completion of next rep)</li> <li>Edge cases:</li> <li>Partial rep at video start \u2192 ignore (need complete cycle)</li> <li>Partial rep at video end \u2192 ignore</li> <li>Abandoned rep (athlete stops mid-motion) \u2192 count as attempted, mark violation</li> </ul> <p>Output data structure:</p> <pre><code>@dataclass\nclass RepSegment:\n    \"\"\"Single detected rep with frame boundaries\"\"\"\n    rep_number: int           # 1, 2, 3, ...\n    start_frame: int          # First frame of rep\n    end_frame: int            # Last frame of rep\n    start_time: float         # Timestamp in seconds\n    end_time: float           # Timestamp in seconds\n    duration: float           # Duration in seconds\n    phases: List[PhaseInfo]   # Detected phases with frame ranges\n</code></pre> <p>Algorithm pseudocode:</p> <pre><code>def detect_reps(landmarks_sequence: List[PoseLandmarks], fps: float) -&gt; List[RepSegment]:\n    \"\"\"\n    Detect rep boundaries using state machine.\n\n    Returns list of detected reps with frame ranges.\n    \"\"\"\n    state = WallBallState.STANDING\n    current_rep_start = None\n    detected_reps = []\n    rep_number = 0\n\n    for frame_idx, landmarks in enumerate(landmarks_sequence):\n        # Calculate features\n        hip_y = landmarks[HIP].y\n        hip_velocity = compute_velocity(hip_positions, frame_idx)\n        hand_y = landmarks[WRIST].y\n\n        # State transitions\n        if state == STANDING and hip_velocity &gt; descent_threshold:\n            # Start new rep\n            state = DESCENDING\n            current_rep_start = frame_idx\n\n        elif state == DESCENDING and hip_y &gt; squat_depth_threshold:\n            state = BOTTOM\n\n        elif state == BOTTOM and hip_velocity &lt; -ascent_threshold:\n            state = ASCENDING\n\n        elif state == ASCENDING and hand_y &lt; throw_threshold:\n            state = THROWING\n\n        # ... more transitions ...\n\n        elif state == RECOVERY and hip_velocity == 0 and hands_at_rest:\n            # Complete rep\n            state = STANDING\n            rep_number += 1\n            detected_reps.append(RepSegment(\n                rep_number=rep_number,\n                start_frame=current_rep_start,\n                end_frame=frame_idx,\n                start_time=current_rep_start / fps,\n                end_time=frame_idx / fps,\n                duration=(frame_idx - current_rep_start) / fps\n            ))\n            current_rep_start = None\n\n    return detected_reps\n</code></pre>"},{"location":"development/wallball-norep-detection/#14-stage-2-rep-validation-independent-checkers","title":"1.4 Stage 2: Rep Validation (Independent Checkers)","text":"<p>Approach: For each detected rep, run three independent validators</p> <p>Validation is per-rep, not global:</p> <ul> <li>Each rep is evaluated independently</li> <li>One rep can have multiple violations (still counts as single no-rep)</li> <li>Violations are tracked with specific details (e.g., actual angle vs threshold)</li> </ul>"},{"location":"development/wallball-norep-detection/#validator-1-squat-depth","title":"Validator 1: Squat Depth","text":"<pre><code>def validate_squat_depth(\n    rep_segment: RepSegment,\n    landmarks_sequence: List[PoseLandmarks]\n) -&gt; Tuple[bool, Optional[ViolationDetails]]:\n    \"\"\"\n    Check if hip crease went below knee during bottom phase.\n    Uses knee angle (hip-knee-ankle) &lt; 90\u00b0.\n    \"\"\"\n    # Find BOTTOM phase in rep\n    bottom_frame = find_phase(rep_segment, WallBallPhase.BOTTOM)\n\n    # Get landmarks at bottom\n    hip = landmarks_sequence[bottom_frame][HIP]\n    knee = landmarks_sequence[bottom_frame][KNEE]\n    ankle = landmarks_sequence[bottom_frame][ANKLE]\n\n    # Calculate knee angle\n    angle = calculate_joint_angle(hip, knee, ankle)\n    threshold = 90.0\n\n    is_valid = angle &lt; threshold\n\n    if not is_valid:\n        violation = ViolationDetails(\n            type=\"squat_depth\",\n            frame=bottom_frame,\n            details={\"knee_angle\": angle, \"threshold\": threshold}\n        )\n        return False, violation\n\n    return True, None\n</code></pre>"},{"location":"development/wallball-norep-detection/#validator-2-ball-height","title":"Validator 2: Ball Height","text":"<pre><code>def validate_ball_height(\n    rep_segment: RepSegment,\n    ball_trajectory: List[BallPosition],\n    target_height_pixels: float\n) -&gt; Tuple[bool, Optional[ViolationDetails]]:\n    \"\"\"\n    Check if ball reached target height during flight phase.\n    \"\"\"\n    # Find FLIGHT phase trajectory\n    flight_positions = filter_by_phase(ball_trajectory, WallBallPhase.FLIGHT)\n\n    # Find peak (minimum y-coordinate in image space)\n    peak_position = min(flight_positions, key=lambda p: p.y)\n\n    # Check if peak reached target (with tolerance)\n    tolerance = 10  # pixels\n    is_valid = peak_position.y &lt;= (target_height_pixels + tolerance)\n\n    if not is_valid:\n        violation = ViolationDetails(\n            type=\"ball_height\",\n            frame=peak_position.frame,\n            details={\n                \"peak_y\": peak_position.y,\n                \"target_y\": target_height_pixels,\n                \"difference_pixels\": peak_position.y - target_height_pixels\n            }\n        )\n        return False, violation\n\n    return True, None\n</code></pre>"},{"location":"development/wallball-norep-detection/#validator-3-ball-resting","title":"Validator 3: Ball Resting","text":"<pre><code>def validate_no_resting(\n    rep_segment: RepSegment,\n    ball_trajectory: List[BallPosition],\n    ground_y: float\n) -&gt; Tuple[bool, Optional[ViolationDetails]]:\n    \"\"\"\n    Check if ball rested on ground during recovery phase.\n    Ball is \"resting\" if: low position + stationary + duration &gt; threshold.\n    \"\"\"\n    # Find RECOVERY phase positions\n    recovery_positions = filter_by_phase(ball_trajectory, WallBallPhase.RECOVERY)\n\n    # Find low, stationary sequences\n    resting_sequences = find_stationary_sequences(\n        recovery_positions,\n        height_threshold=ground_y - 50,  # pixels above ground\n        movement_threshold=5,  # max pixels movement\n        duration_threshold=1.5  # seconds\n    )\n\n    if resting_sequences:\n        violation = ViolationDetails(\n            type=\"ball_resting\",\n            frame=resting_sequences[0].start_frame,\n            details={\n                \"duration_seconds\": resting_sequences[0].duration,\n                \"threshold_seconds\": 1.5\n            }\n        )\n        return False, violation\n\n    return True, None\n</code></pre>"},{"location":"development/wallball-norep-detection/#aggregate-validation-results","title":"Aggregate validation results","text":"<pre><code>def validate_rep(\n    rep_segment: RepSegment,\n    landmarks_sequence: List[PoseLandmarks],\n    ball_trajectory: List[BallPosition],\n    target_height: float,\n    ground_y: float\n) -&gt; WallBallRep:\n    \"\"\"\n    Run all validators on a single rep.\n    \"\"\"\n    violations = []\n\n    # Check squat depth\n    depth_valid, depth_violation = validate_squat_depth(rep_segment, landmarks_sequence)\n    if not depth_valid:\n        violations.append(depth_violation)\n\n    # Check ball height\n    height_valid, height_violation = validate_ball_height(\n        rep_segment, ball_trajectory, target_height\n    )\n    if not height_valid:\n        violations.append(height_violation)\n\n    # Check resting\n    resting_valid, resting_violation = validate_no_resting(\n        rep_segment, ball_trajectory, ground_y\n    )\n    if not resting_valid:\n        violations.append(resting_violation)\n\n    # Rep is valid only if all checks passed\n    is_valid = len(violations) == 0\n\n    return WallBallRep(\n        rep_number=rep_segment.rep_number,\n        start_frame=rep_segment.start_frame,\n        end_frame=rep_segment.end_frame,\n        duration=rep_segment.duration,\n        is_valid=is_valid,\n        violations=violations\n    )\n</code></pre>"},{"location":"development/wallball-norep-detection/#15-output-structure","title":"1.5 Output Structure","text":"<p>Hierarchical output (JSON + CLI summary):</p> <pre><code>{\n  \"summary\": {\n    \"total_attempted_reps\": 50,\n    \"valid_reps\": 47,\n    \"no_reps\": 3,\n    \"completion_percentage\": 94.0,\n    \"total_duration_seconds\": 156.3,\n    \"avg_time_per_rep\": 3.13,\n    \"violation_breakdown\": {\n      \"squat_depth\": 2,\n      \"ball_height\": 1,\n      \"ball_resting\": 0\n    }\n  },\n  \"reps\": [\n    {\n      \"rep_number\": 1,\n      \"start_frame\": 0,\n      \"end_frame\": 89,\n      \"start_time\": 0.0,\n      \"end_time\": 3.0,\n      \"duration\": 3.0,\n      \"is_valid\": true,\n      \"violations\": [],\n      \"metrics\": {\n        \"min_knee_angle\": 85.3,\n        \"ball_peak_y\": 245,\n        \"ball_peak_height_m\": 3.1\n      }\n    },\n    {\n      \"rep_number\": 2,\n      \"start_frame\": 90,\n      \"end_frame\": 185,\n      \"start_time\": 3.0,\n      \"end_time\": 6.2,\n      \"duration\": 3.2,\n      \"is_valid\": false,\n      \"violations\": [\n        {\n          \"type\": \"squat_depth\",\n          \"frame\": 135,\n          \"timestamp\": 4.5,\n          \"details\": {\n            \"knee_angle\": 95.2,\n            \"threshold\": 90.0,\n            \"difference\": 5.2\n          }\n        }\n      ],\n      \"metrics\": {\n        \"min_knee_angle\": 95.2,\n        \"ball_peak_y\": 240,\n        \"ball_peak_height_m\": 3.05\n      }\n    },\n    {\n      \"rep_number\": 3,\n      \"start_frame\": 186,\n      \"end_frame\": 278,\n      \"start_time\": 6.2,\n      \"end_time\": 9.3,\n      \"duration\": 3.1,\n      \"is_valid\": true,\n      \"violations\": [],\n      \"metrics\": {\n        \"min_knee_angle\": 82.1,\n        \"ball_peak_y\": 238,\n        \"ball_peak_height_m\": 3.15\n      }\n    }\n    // ... more reps ...\n  ],\n  \"calibration\": {\n    \"pixels_per_meter\": 100.5,\n    \"target_height_m\": 3.0,\n    \"target_height_pixels\": 301,\n    \"calibration_method\": \"target_based\"\n  }\n}\n</code></pre> <p>CLI output (human-readable):</p> <pre><code>Wall Ball Analysis Complete\n================================================================================\n\nSUMMARY\n  Total attempted reps: 50\n  Valid reps:           47 \u2713\n  No-reps:              3 \u2717\n  Completion rate:      94.0%\n\n  Average time per rep: 3.13s\n  Total duration:       156.3s\n\nVIOLATION BREAKDOWN\n  Squat depth:          2 violations\n  Ball height:          1 violation\n  Ball resting:         0 violations\n\nDETAILED RESULTS\n  Rep  1: \u2713 VALID   (3.0s) | Knee: 85.3\u00b0 | Height: 3.1m\n  Rep  2: \u2717 NO-REP  (3.2s) | Squat depth insufficient (95.2\u00b0 vs 90.0\u00b0)\n  Rep  3: \u2713 VALID   (3.1s) | Knee: 82.1\u00b0 | Height: 3.15m\n  Rep  4: \u2713 VALID   (3.0s) | Knee: 87.5\u00b0 | Height: 3.05m\n  Rep  5: \u2717 NO-REP  (3.4s) | Ball height insufficient (2.85m vs 3.0m)\n  ...\n  Rep 50: \u2713 VALID   (3.2s) | Knee: 88.9\u00b0 | Height: 3.08m\n\nFor detailed JSON output: kinemotion wallball-analyze video.mp4 --json-output results.json\nFor debug video: kinemotion wallball-analyze video.mp4 --output debug.mp4\n</code></pre>"},{"location":"development/wallball-norep-detection/#16-key-design-principles","title":"1.6 Key Design Principles","text":"<ol> <li> <p>Separation of concerns: Detection and validation are independent</p> </li> <li> <p>Can test rep detection without validation logic</p> </li> <li> <p>Can improve validators without changing detection</p> </li> <li> <p>Per-rep granularity: Every attempted rep is identified and evaluated</p> </li> <li> <p>Matches HYROX judging workflow (judge calls each rep)</p> </li> <li> <p>Enables detailed performance analysis</p> </li> <li> <p>Transparent violations: Clear reasons for no-reps</p> </li> <li> <p>Athletes know exactly what to fix</p> </li> <li> <p>Coaches can analyze patterns (e.g., \"depth violations increase with fatigue\")</p> </li> <li> <p>Extensible validators: Easy to add new standards</p> </li> <li> <p>Each validator is independent function</p> </li> <li> <p>Can add timing requirements, tempo, etc. in future</p> </li> <li> <p>Testable components: Clear interfaces enable unit testing</p> </li> <li> <p>Mock pose landmarks to test state machine</p> </li> <li>Mock rep segments to test validators</li> <li>Test aggregation separately</li> </ol>"},{"location":"development/wallball-norep-detection/#2-wall-ball-exercise-standards","title":"2. Wall Ball Exercise Standards","text":""},{"location":"development/wallball-norep-detection/#hyrox-competition-requirements","title":"HYROX Competition Requirements","text":"<p>Reps: 100 wall ball shots (all divisions)</p> <p>Equipment:</p> <ul> <li>Men: 6kg (14 lbs) or 9kg (20 lbs) ball</li> <li>Women: 4kg (9 lbs) or 6kg (14 lbs) ball</li> </ul> <p>Target Height:</p> <ul> <li>Men: 3.0 meters (10 feet)</li> <li>Women: 2.7 meters (9 feet)</li> </ul>"},{"location":"development/wallball-norep-detection/#no-rep-violations","title":"No-Rep Violations","text":"<p>A rep does not count if:</p> <ol> <li>Insufficient squat depth: Hip crease doesn't go below top of knee</li> <li>Target miss: Ball doesn't reach target height on wall</li> <li>Ball resting: Ball rests on ground between reps (not allowed)</li> </ol> <p>Judge requirement: One judge per athlete to call no-reps in real-time</p>"},{"location":"development/wallball-norep-detection/#3-technical-feasibility-analysis","title":"3. Technical Feasibility Analysis","text":""},{"location":"development/wallball-norep-detection/#31-squat-depth-validation-easy-2-3-days","title":"3.1 Squat Depth Validation - EASY (2-3 days)","text":"<p>Challenge: Detect if hip crease goes below knee</p> <p>Approach: Use joint angle calculation (NOT vertical position)</p> <p>Rationale:</p> <ul> <li>Kinemotion already has <code>cmj/joint_angles.py</code> with <code>calculate_joint_angle()</code></li> <li>Joint angles are camera-agnostic (work at any angle, including 45\u00b0)</li> <li>More robust than vertical position checks</li> <li>Standard approach in fitness tracking apps</li> </ul> <p>Implementation:</p> <pre><code>from kinemotion.cmj.joint_angles import calculate_joint_angle\n\ndef validate_squat_depth(\n    hip: PoseLandmark,\n    knee: PoseLandmark,\n    ankle: PoseLandmark,\n    threshold_degrees: float = 90.0\n) -&gt; tuple[bool, float]:\n    \"\"\"\n    Validate squat depth using knee angle.\n    Works with any camera angle (lateral, 45\u00b0, etc.)\n\n    Returns:\n        (is_valid, angle_degrees)\n    \"\"\"\n    angle = calculate_joint_angle(hip, knee, ankle)\n    is_valid = angle &lt; threshold_degrees\n    return is_valid, angle\n</code></pre> <p>Why joint angles &gt; vertical positions:</p> <ul> <li>\u2705 No parallax errors from camera angle</li> <li>\u2705 Measures actual joint flexion (biomechanically correct)</li> <li>\u2705 Works with varied camera positioning</li> <li>\u2705 Reuses existing kinemotion code</li> </ul> <p>Confidence: HIGH - proven technology in kinemotion CMJ module</p>"},{"location":"development/wallball-norep-detection/#32-ball-resting-detection-moderate-2-days","title":"3.2 Ball Resting Detection - MODERATE (2 days)","text":"<p>Challenge: Detect if ball rests on ground between reps</p> <p>Approach: Temporal tracking with position + velocity analysis</p> <p>Algorithm:</p> <ol> <li>Track ball position over time (using ball tracking from 2.3)</li> <li>Detect low vertical position (below waist height)</li> <li>Check for minimal movement (stationary threshold)</li> <li>Measure duration (&gt;1-2 seconds = violation)</li> </ol> <p>Similar to: Phase detection logic in <code>cmj/analysis.py</code> and <code>dropjump/analysis.py</code></p> <p>Pseudo-code:</p> <pre><code>def detect_ball_resting(\n    ball_positions: list,\n    waist_height: float,\n    stationary_threshold: float = 5,  # pixels\n    duration_threshold: float = 1.5   # seconds\n) -&gt; bool:\n    \"\"\"\n    Detect if ball is resting on ground.\n\n    Args:\n        ball_positions: List of (x, y, timestamp) tuples\n        waist_height: Y-coordinate of athlete's waist\n        stationary_threshold: Max movement in pixels to be \"stationary\"\n        duration_threshold: Min duration in seconds to be \"resting\"\n    \"\"\"\n    low_positions = [p for p in ball_positions if p.y &gt; waist_height]\n\n    if not low_positions:\n        return False\n\n    # Check if positions are stationary\n    position_changes = [\n        distance(low_positions[i], low_positions[i+1])\n        for i in range(len(low_positions) - 1)\n    ]\n\n    if max(position_changes) &gt; stationary_threshold:\n        return False  # Ball is moving\n\n    # Check duration\n    duration = low_positions[-1].timestamp - low_positions[0].timestamp\n    return duration &gt; duration_threshold\n</code></pre> <p>Confidence: MEDIUM-HIGH - uses proven phase detection patterns</p>"},{"location":"development/wallball-norep-detection/#33-ball-height-validation-hard-5-7-days","title":"3.3 Ball Height Validation - HARD (5-7 days)","text":"<p>Challenge: Track ball through throw and validate it hits target height</p> <p>Sub-challenges:</p> <ol> <li>Ball tracking: Detect and track ball position (pose estimation doesn't include balls)</li> <li>Trajectory analysis: Detect peak height per rep</li> <li>Calibration: Convert pixel coordinates to real-world meters</li> </ol>"},{"location":"development/wallball-norep-detection/#331-ball-tracking","title":"3.3.1 Ball Tracking","text":"<p>Approach: Hybrid method combining pose tracking with color detection</p> <p>Research findings (from OpenCV ball tracking literature):</p> <ul> <li>Color-based HSV segmentation: Simple, fast, lighting-dependent</li> <li>YOLO object detection: Accurate but requires training data + model dependency</li> <li>Hybrid (chosen): Use pose to guide search, HSV for detection</li> </ul> <p>Implementation:</p> <pre><code>def detect_ball_hsv(\n    frame: np.ndarray,\n    athlete_landmarks: PoseLandmarks,\n    ball_color_range: tuple[HSVRange, HSVRange] = None\n) -&gt; Optional[BallPosition]:\n    \"\"\"\n    Detect ball using HSV color segmentation with pose-guided ROI.\n\n    Args:\n        frame: Video frame (BGR)\n        athlete_landmarks: MediaPipe pose landmarks\n        ball_color_range: Optional (lower_hsv, upper_hsv) for custom ball colors\n\n    Returns:\n        Ball position (x, y) or None if not detected\n    \"\"\"\n    # Step 1: Define search region using pose (reduce false positives)\n    wrist = athlete_landmarks[mp_pose.PoseLandmark.LEFT_WRIST]\n    shoulder = athlete_landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER]\n\n    # ROI: Expand from upper body region (where ball should be)\n    roi_bounds = expand_roi(wrist, shoulder, expansion_factor=2.0)\n    roi = frame[roi_bounds.top:roi_bounds.bottom, roi_bounds.left:roi_bounds.right]\n\n    # Step 2: HSV color segmentation\n    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n\n    # Default: dark colors (common medicine ball colors)\n    if ball_color_range is None:\n        lower_hsv = np.array([0, 0, 0])      # Black/dark gray\n        upper_hsv = np.array([180, 255, 80])\n    else:\n        lower_hsv, upper_hsv = ball_color_range\n\n    mask = cv2.inRange(hsv, lower_hsv, upper_hsv)\n\n    # Step 3: Find circular contours\n    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n    if not contours:\n        return None\n\n    # Find largest circular contour\n    largest_contour = max(contours, key=cv2.contourArea)\n\n    # Circularity check (balls are circular)\n    area = cv2.contourArea(largest_contour)\n    perimeter = cv2.arcLength(largest_contour, True)\n    if perimeter == 0:\n        return None\n\n    circularity = 4 * np.pi * area / (perimeter ** 2)\n    if circularity &lt; 0.7:  # Not circular enough\n        return None\n\n    # Get center\n    M = cv2.moments(largest_contour)\n    if M[\"m00\"] == 0:\n        return None\n\n    cx = int(M[\"m10\"] / M[\"m00\"]) + roi_bounds.left\n    cy = int(M[\"m01\"] / M[\"m00\"]) + roi_bounds.top\n\n    return BallPosition(x=cx, y=cy, frame=frame_number)\n</code></pre> <p>Advantages of hybrid approach:</p> <ul> <li>Pose-guided ROI reduces search space \u2192 faster, fewer false positives</li> <li>HSV segmentation is simple, no extra models needed</li> <li>Works with different ball colors (configurable)</li> </ul> <p>Limitations:</p> <ul> <li>Sensitive to lighting conditions</li> <li>Requires user calibration for non-standard ball colors</li> <li>May struggle with motion blur</li> </ul> <p>Mitigation:</p> <ul> <li>Temporal smoothing (like <code>core/smoothing.py</code>)</li> <li>Adaptive thresholding</li> <li>User-configurable HSV ranges (provide presets for common ball colors)</li> </ul>"},{"location":"development/wallball-norep-detection/#332-trajectory-analysis","title":"3.3.2 Trajectory Analysis","text":"<p>Goal: Detect peak height of ball per rep</p> <p>Approach: Position buffering with peak detection</p> <pre><code>from collections import deque\n\nclass BallTrajectoryAnalyzer:\n    \"\"\"Track ball trajectory and detect peaks\"\"\"\n\n    def __init__(self, buffer_size: int = 30):\n        self.positions = deque(maxlen=buffer_size)  # ~1 second at 30fps\n\n    def add_position(self, position: BallPosition) -&gt; None:\n        \"\"\"Add ball position to trajectory buffer\"\"\"\n        self.positions.append(position)\n\n    def detect_peak(self) -&gt; Optional[float]:\n        \"\"\"\n        Detect if trajectory contains a peak (ball reached apex).\n\n        Returns:\n            Peak height (y-coordinate) or None\n        \"\"\"\n        if len(self.positions) &lt; 10:\n            return None\n\n        # Find local minimum in y-coordinates (peak = lowest y in image coords)\n        y_coords = [p.y for p in self.positions]\n\n        # Simple peak detection: find point lower than both neighbors\n        for i in range(1, len(y_coords) - 1):\n            if y_coords[i] &lt; y_coords[i-1] and y_coords[i] &lt; y_coords[i+1]:\n                return y_coords[i]  # Peak detected\n\n        return None\n</code></pre> <p>Advanced approach (optional enhancement):</p> <ul> <li>Parabolic curve fitting (balls follow parabolic trajectory)</li> <li>Velocity-based detection (velocity crosses zero at peak)</li> </ul>"},{"location":"development/wallball-norep-detection/#333-camera-calibration","title":"3.3.3 Camera Calibration","text":"<p>Challenge: Convert pixel coordinates to real-world meters</p> <p>Approach: Reference-based calibration (similar to force plate height calibration)</p> <p>Options:</p> <p>Option 1: Wall target as reference (preferred for 45\u00b0 angle)</p> <pre><code>def calibrate_from_target(\n    target_pixel_y: int,\n    target_real_height_m: float = 3.0,  # Men's target\n    ground_pixel_y: int = None\n) -&gt; float:\n    \"\"\"\n    Calculate pixels-per-meter using known target height.\n\n    Args:\n        target_pixel_y: Y-coordinate of target in video\n        target_real_height_m: Known target height in meters\n        ground_pixel_y: Y-coordinate of ground (if None, use frame bottom)\n\n    Returns:\n        pixels_per_meter: Conversion factor\n    \"\"\"\n    if ground_pixel_y is None:\n        ground_pixel_y = frame_height  # Bottom of frame\n\n    pixel_height = ground_pixel_y - target_pixel_y\n    pixels_per_meter = pixel_height / target_real_height_m\n\n    return pixels_per_meter\n</code></pre> <p>Option 2: Athlete height as reference (fallback)</p> <pre><code>def calibrate_from_athlete(\n    athlete_landmarks: PoseLandmarks,\n    athlete_height_m: float\n) -&gt; float:\n    \"\"\"Calculate pixels-per-meter using athlete's known height\"\"\"\n    head = athlete_landmarks[mp_pose.PoseLandmark.NOSE]\n    ankle = athlete_landmarks[mp_pose.PoseLandmark.LEFT_ANKLE]\n\n    pixel_height = ankle.y - head.y\n    pixels_per_meter = pixel_height / athlete_height_m\n\n    return pixels_per_meter\n</code></pre> <p>Height validation:</p> <pre><code>def validate_ball_height(\n    ball_peak_y: int,\n    target_y: int,\n    tolerance_pixels: float = 10\n) -&gt; bool:\n    \"\"\"\n    Validate if ball reached target height.\n\n    Args:\n        ball_peak_y: Peak position of ball (y-coordinate)\n        target_y: Target position (y-coordinate)\n        tolerance_pixels: Allowable margin\n\n    Returns:\n        True if ball reached target (ball_peak_y &lt;= target_y + tolerance)\n    \"\"\"\n    # In image coordinates, y increases downward\n    # So peak (top) has LOWER y value\n    return ball_peak_y &lt;= (target_y + tolerance_pixels)\n</code></pre> <p>Confidence: MEDIUM - most complex component, requires user calibration</p>"},{"location":"development/wallball-norep-detection/#4-camera-setup-45-angle-advantage","title":"4. Camera Setup - 45\u00b0 Angle Advantage","text":""},{"location":"development/wallball-norep-detection/#41-why-45-is-superior-to-pure-lateral-view","title":"4.1 Why 45\u00b0 is Superior to Pure Lateral View","text":"<p>Initial assumption: Use lateral (90\u00b0 side) view like CMJ analysis</p> <p>Finding: 45-degree angle is better for wall ball detection</p>"},{"location":"development/wallball-norep-detection/#comparison-table","title":"Comparison Table","text":"Detection Task Pure Lateral (90\u00b0) 45-Degree Winner Squat depth Easy (vertical comparison) Easy (joint angle) Tie \u2705 Ball tracking Hard (heavy occlusion) Easy (clear visibility) 45\u00b0 \u2705 Target height Indirect (calibration only) Direct (see target hit) 45\u00b0 \u2705 Ball resting Moderate (feet may hide) Easy (clear ground view) 45\u00b0 \u2705 Setup flexibility Strict positioning Forgiving 45\u00b0 \u2705"},{"location":"development/wallball-norep-detection/#result-45-wins-45-categories","title":"Result: 45\u00b0 wins 4/5 categories","text":""},{"location":"development/wallball-norep-detection/#42-45-angle-advantages","title":"4.2 45\u00b0 Angle Advantages","text":""},{"location":"development/wallball-norep-detection/#1-ball-visibility-critical","title":"1. Ball Visibility (Critical)","text":"<ul> <li>Pure lateral: Ball hidden behind athlete's torso/arms during most of motion</li> <li>45-degree: Clear line of sight throughout catch \u2192 squat \u2192 throw trajectory</li> <li>Less occlusion = more reliable tracking</li> </ul>"},{"location":"development/wallball-norep-detection/#2-target-visibility-major-benefit","title":"2. Target Visibility (Major benefit)","text":"<ul> <li>Can see wall target marker in frame</li> <li>Enables visual confirmation of ball hitting target</li> <li>Could use computer vision on target itself (detect ball-target intersection)</li> <li>Pure lateral: wall is perpendicular to camera, target invisible</li> </ul>"},{"location":"development/wallball-norep-detection/#3-ground-plane-visibility","title":"3. Ground Plane Visibility","text":"<ul> <li>Clear view of floor for ball resting detection</li> <li>Better spatial awareness of ball position</li> </ul>"},{"location":"development/wallball-norep-detection/#4-practical-setup","title":"4. Practical Setup","text":"<ul> <li>Matches real-world HYROX gym setup (camera in corner)</li> <li>More forgiving of positioning errors</li> <li>Easier to frame athlete + wall + ground in single shot</li> </ul>"},{"location":"development/wallball-norep-detection/#43-solving-the-parallax-problem","title":"4.3 Solving the Parallax Problem","text":"<p>Issue: At 45\u00b0, hip/knee vertical positions have parallax error (depth from camera affects 2D projection)</p> <p>Solution: Use joint angles instead of vertical positions</p> <p>Why this works:</p> <ul> <li>Joint angles are camera-angle agnostic</li> <li>Measures actual biomechanical flexion (more accurate)</li> <li>Already implemented in <code>cmj/joint_angles.py</code></li> <li>Standard approach in fitness apps</li> </ul> <p>Comparison:</p> <pre><code># \u274c OLD: Vertical position (lateral view only)\ndef validate_squat_lateral(hip, knee):\n    return hip.y &gt; knee.y  # Fails at 45\u00b0 due to parallax\n\n# \u2705 NEW: Joint angle (any camera angle)\ndef validate_squat_angle(hip, knee, ankle):\n    angle = calculate_joint_angle(hip, knee, ankle)\n    return angle &lt; 90  # Works at any angle\n</code></pre>"},{"location":"development/wallball-norep-detection/#44-recommended-camera-setup","title":"4.4 Recommended Camera Setup","text":"<pre><code>Camera position: 45-degree angle from wall\nDistance: 3-5 meters from athlete\nHeight: Waist level (capture full squat to overhead throw)\nFrame composition:\n  - Athlete's full body (head to feet)\n  - Wall target visible in upper frame\n  - Ground plane visible in lower frame\n</code></pre> <p>Diagram:</p> <pre><code>        Wall\n         |\n      Target \u25cf (visible in frame)\n         |\n         |\n    Athlete \ud83e\uddcd (side-front view)\n         \u2571\n        \u2571\n       \u2571 45\u00b0\n      \ud83d\udcf7 Camera\n</code></pre> <p>Frame example:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Wall  [\u25cfTarget]       \u2502  \u2190 Target visible\n\u2502                         \u2502\n\u2502        \ud83e\uddcd              \u2502  \u2190 Athlete (side-front view)\n\u2502       /\u2502\\              \u2502  \u2190 Ball visible during throw\n\u2502       / \\              \u2502\n\u2502    \u25cf\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500         \u2502  \u2190 Ball visible on ground\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"development/wallball-norep-detection/#5-proposed-architecture","title":"5. Proposed Architecture","text":""},{"location":"development/wallball-norep-detection/#51-module-structure","title":"5.1 Module Structure","text":"<p>Following kinemotion patterns (sibling to <code>cmj/</code> and <code>dropjump/</code>):</p> <pre><code>src/kinemotion/wallball/\n\u251c\u2500\u2500 __init__.py              # Module exports\n\u251c\u2500\u2500 cli.py                   # CLI command: kinemotion wallball-analyze\n\u251c\u2500\u2500 analysis.py              # Rep detection, phase detection\n\u251c\u2500\u2500 ball_tracking.py         # Ball detection (HSV + pose-guided ROI)\n\u251c\u2500\u2500 validation.py            # No-rep validation logic\n\u251c\u2500\u2500 calibration.py           # Pixel-to-meter conversion\n\u251c\u2500\u2500 metrics.py               # WallBallMetrics dataclass\n\u2514\u2500\u2500 debug_overlay.py         # Visualization renderer\n\ntests/wallball/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 test_analysis.py         # Rep detection tests\n\u251c\u2500\u2500 test_ball_tracking.py    # Ball detection tests\n\u251c\u2500\u2500 test_validation.py       # No-rep logic tests\n\u251c\u2500\u2500 test_calibration.py      # Calibration tests\n\u251c\u2500\u2500 test_metrics.py          # Metrics serialization tests\n\u2514\u2500\u2500 fixtures/                # Test videos\n    \u251c\u2500\u2500 good_reps.mp4\n    \u251c\u2500\u2500 depth_norep.mp4\n    \u251c\u2500\u2500 height_norep.mp4\n    \u2514\u2500\u2500 resting_norep.mp4\n\ndocs/guides/\n\u2514\u2500\u2500 wallball-guide.md        # User guide\n\ndocs/reference/\n\u2514\u2500\u2500 wallball-standards.md    # HYROX standards reference\n</code></pre>"},{"location":"development/wallball-norep-detection/#52-key-classes-and-functions","title":"5.2 Key Classes and Functions","text":""},{"location":"development/wallball-norep-detection/#metricspy","title":"<code>metrics.py</code>","text":"<pre><code>from dataclasses import dataclass\nfrom typing import Optional, List\n\n@dataclass\nclass WallBallViolation:\n    \"\"\"Single no-rep violation\"\"\"\n    frame: int\n    timestamp: float  # seconds\n    violation_type: str  # \"squat_depth\", \"ball_height\", \"ball_resting\"\n    details: dict  # e.g., {\"knee_angle\": 95.0, \"threshold\": 90.0}\n\n    def to_dict(self) -&gt; dict:\n        return {\n            \"frame\": int(self.frame),\n            \"timestamp\": float(self.timestamp),\n            \"violation_type\": self.violation_type,\n            \"details\": self.details\n        }\n\n@dataclass\nclass WallBallRep:\n    \"\"\"Single wall ball rep\"\"\"\n    rep_number: int\n    start_frame: int\n    end_frame: int\n    is_valid: bool\n    violations: List[WallBallViolation]\n    duration: float  # seconds\n    min_knee_angle: float  # degrees\n    ball_peak_height: Optional[float]  # pixels or meters\n\n    def to_dict(self) -&gt; dict:\n        return {\n            \"rep_number\": int(self.rep_number),\n            \"start_frame\": int(self.start_frame),\n            \"end_frame\": int(self.end_frame),\n            \"is_valid\": bool(self.is_valid),\n            \"violations\": [v.to_dict() for v in self.violations],\n            \"duration\": float(self.duration),\n            \"min_knee_angle\": float(self.min_knee_angle),\n            \"ball_peak_height\": float(self.ball_peak_height) if self.ball_peak_height else None\n        }\n\n@dataclass\nclass WallBallMetrics:\n    \"\"\"Complete wall ball analysis results\"\"\"\n    total_reps_attempted: int\n    valid_reps: int\n    no_reps: int\n    reps: List[WallBallRep]\n    total_duration: float  # seconds\n    avg_time_per_rep: float  # seconds\n\n    # Violation breakdown\n    squat_depth_violations: int\n    ball_height_violations: int\n    ball_resting_violations: int\n\n    # Camera calibration\n    pixels_per_meter: Optional[float]\n    target_height_m: float\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Convert to JSON-serializable dict\"\"\"\n        return {\n            \"total_reps_attempted\": int(self.total_reps_attempted),\n            \"valid_reps\": int(self.valid_reps),\n            \"no_reps\": int(self.no_reps),\n            \"reps\": [r.to_dict() for r in self.reps],\n            \"total_duration\": float(self.total_duration),\n            \"avg_time_per_rep\": float(self.avg_time_per_rep),\n            \"violation_breakdown\": {\n                \"squat_depth\": int(self.squat_depth_violations),\n                \"ball_height\": int(self.ball_height_violations),\n                \"ball_resting\": int(self.ball_resting_violations)\n            },\n            \"calibration\": {\n                \"pixels_per_meter\": float(self.pixels_per_meter) if self.pixels_per_meter else None,\n                \"target_height_m\": float(self.target_height_m)\n            }\n        }\n</code></pre>"},{"location":"development/wallball-norep-detection/#analysispy","title":"<code>analysis.py</code>","text":"<pre><code>from enum import Enum\nfrom typing import List, Tuple\nimport numpy as np\nfrom kinemotion.core.pose import PoseLandmarks\nfrom kinemotion.cmj.joint_angles import calculate_joint_angle\n\nclass WallBallPhase(Enum):\n    \"\"\"Wall ball rep phases\"\"\"\n    STANDING = \"standing\"\n    DESCENT = \"descent\"        # Lowering into squat\n    BOTTOM = \"bottom\"          # At squat depth\n    ASCENT = \"ascent\"          # Standing up\n    THROW = \"throw\"            # Ball release\n    FLIGHT = \"flight\"          # Ball in air\n    CATCH = \"catch\"            # Ball contact\n    RECOVERY = \"recovery\"      # Return to standing\n\ndef detect_wallball_reps(\n    landmarks: List[PoseLandmarks],\n    velocities: np.ndarray,\n    fps: float\n) -&gt; List[Tuple[int, int]]:\n    \"\"\"\n    Detect wall ball reps from pose landmarks.\n\n    Args:\n        landmarks: List of pose landmarks per frame\n        velocities: Hip vertical velocities\n        fps: Video frame rate\n\n    Returns:\n        List of (start_frame, end_frame) tuples for each rep\n    \"\"\"\n    # Similar to CMJ detection: look for squat cycles\n    # 1. Detect descent (negative velocity)\n    # 2. Detect bottom (velocity crosses zero)\n    # 3. Detect ascent (positive velocity)\n    # 4. Detect throw (hands go up)\n    # 5. Detect catch (hands come down)\n    pass\n\ndef analyze_wallball_video(\n    video_path: str,\n    target_height_m: float = 3.0,\n    squat_depth_threshold: float = 90.0,\n    quality: str = \"balanced\"\n) -&gt; WallBallMetrics:\n    \"\"\"\n    Main analysis function.\n\n    Args:\n        video_path: Path to video file\n        target_height_m: Target height in meters (3.0 men, 2.7 women)\n        squat_depth_threshold: Knee angle threshold in degrees\n        quality: Auto-tuning quality preset\n\n    Returns:\n        WallBallMetrics with complete analysis\n    \"\"\"\n    # 1. Extract pose landmarks (reuse core/pose.py)\n    # 2. Detect reps\n    # 3. Track ball (ball_tracking.py)\n    # 4. Validate each rep (validation.py)\n    # 5. Calculate metrics\n    pass\n</code></pre>"},{"location":"development/wallball-norep-detection/#validationpy","title":"<code>validation.py</code>","text":"<pre><code>from typing import Optional, List\nimport numpy as np\nfrom kinemotion.cmj.joint_angles import calculate_joint_angle\n\ndef validate_squat_depth(\n    hip_landmark,\n    knee_landmark,\n    ankle_landmark,\n    threshold_degrees: float = 90.0\n) -&gt; Tuple[bool, float]:\n    \"\"\"\n    Validate squat depth using knee angle.\n    Works with any camera angle.\n\n    Returns:\n        (is_valid, knee_angle_degrees)\n    \"\"\"\n    angle = calculate_joint_angle(hip_landmark, knee_landmark, ankle_landmark)\n    return angle &lt; threshold_degrees, angle\n\ndef validate_ball_height(\n    ball_peak_y: float,\n    target_y: float,\n    tolerance_pixels: float = 10\n) -&gt; bool:\n    \"\"\"\n    Validate if ball reached target height.\n\n    Note: In image coordinates, y increases downward.\n    Peak (top) has LOWER y value.\n    \"\"\"\n    return ball_peak_y &lt;= (target_y + tolerance_pixels)\n\ndef validate_no_resting(\n    ball_trajectory: List[BallPosition],\n    ground_threshold_y: float,\n    stationary_threshold_pixels: float = 5,\n    duration_threshold_seconds: float = 1.5\n) -&gt; bool:\n    \"\"\"\n    Validate ball didn't rest on ground.\n\n    Returns:\n        True if no resting violation detected\n    \"\"\"\n    # Check for sustained stationary position near ground\n    pass\n\ndef validate_rep(\n    landmarks_sequence: List[PoseLandmarks],\n    ball_trajectory: List[BallPosition],\n    target_y: float,\n    squat_threshold: float = 90.0\n) -&gt; Tuple[bool, List[WallBallViolation]]:\n    \"\"\"\n    Validate complete rep against all standards.\n\n    Returns:\n        (is_valid, violations_list)\n    \"\"\"\n    violations = []\n\n    # Check squat depth at bottom position\n    # Check ball height at peak\n    # Check for resting violations\n\n    return len(violations) == 0, violations\n</code></pre>"},{"location":"development/wallball-norep-detection/#ball_trackingpy","title":"<code>ball_tracking.py</code>","text":"<pre><code>import cv2\nimport numpy as np\nfrom typing import Optional, Tuple\nfrom dataclasses import dataclass\n\n@dataclass\nclass HSVRange:\n    \"\"\"HSV color range for ball detection\"\"\"\n    lower: np.ndarray  # [H, S, V] lower bounds\n    upper: np.ndarray  # [H, S, V] upper bounds\n\n# Preset color ranges for common medicine balls\nBALL_COLOR_PRESETS = {\n    \"black\": HSVRange(\n        lower=np.array([0, 0, 0]),\n        upper=np.array([180, 255, 80])\n    ),\n    \"dark_gray\": HSVRange(\n        lower=np.array([0, 0, 50]),\n        upper=np.array([180, 50, 150])\n    ),\n    \"brown\": HSVRange(\n        lower=np.array([10, 100, 20]),\n        upper=np.array([20, 255, 200])\n    ),\n    \"blue\": HSVRange(\n        lower=np.array([100, 150, 50]),\n        upper=np.array([130, 255, 255])\n    )\n}\n\n@dataclass\nclass BallPosition:\n    \"\"\"Ball position in frame\"\"\"\n    x: int\n    y: int\n    frame: int\n    confidence: float  # 0-1, based on circularity and contour area\n\nclass BallTracker:\n    \"\"\"Track ball using HSV color detection with pose-guided ROI\"\"\"\n\n    def __init__(\n        self,\n        color_preset: str = \"black\",\n        custom_hsv_range: Optional[HSVRange] = None,\n        min_circularity: float = 0.7,\n        min_area: int = 100\n    ):\n        self.hsv_range = custom_hsv_range or BALL_COLOR_PRESETS[color_preset]\n        self.min_circularity = min_circularity\n        self.min_area = min_area\n        self.last_position: Optional[BallPosition] = None\n\n    def detect_ball(\n        self,\n        frame: np.ndarray,\n        frame_number: int,\n        athlete_landmarks: Optional[PoseLandmarks] = None\n    ) -&gt; Optional[BallPosition]:\n        \"\"\"\n        Detect ball in frame using HSV color segmentation.\n\n        Args:\n            frame: Video frame (BGR)\n            frame_number: Current frame number\n            athlete_landmarks: Optional pose landmarks for ROI guidance\n\n        Returns:\n            BallPosition or None if not detected\n        \"\"\"\n        # Step 1: Define ROI (if landmarks available)\n        if athlete_landmarks is not None:\n            roi, roi_offset = self._get_pose_guided_roi(frame, athlete_landmarks)\n        else:\n            roi = frame\n            roi_offset = (0, 0)\n\n        # Step 2: HSV color segmentation\n        hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n        mask = cv2.inRange(hsv, self.hsv_range.lower, self.hsv_range.upper)\n\n        # Step 3: Find contours\n        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        if not contours:\n            return None\n\n        # Step 4: Filter by size and circularity\n        valid_contours = []\n        for contour in contours:\n            area = cv2.contourArea(contour)\n            if area &lt; self.min_area:\n                continue\n\n            perimeter = cv2.arcLength(contour, True)\n            if perimeter == 0:\n                continue\n\n            circularity = 4 * np.pi * area / (perimeter ** 2)\n            if circularity &gt;= self.min_circularity:\n                valid_contours.append((contour, area, circularity))\n\n        if not valid_contours:\n            return None\n\n        # Step 5: Choose largest valid contour\n        best_contour, area, circularity = max(valid_contours, key=lambda x: x[1])\n\n        # Step 6: Get center\n        M = cv2.moments(best_contour)\n        if M[\"m00\"] == 0:\n            return None\n\n        cx = int(M[\"m10\"] / M[\"m00\"]) + roi_offset[0]\n        cy = int(M[\"m01\"] / M[\"m00\"]) + roi_offset[1]\n\n        # Step 7: Confidence based on circularity\n        confidence = min(1.0, circularity)\n\n        position = BallPosition(x=cx, y=cy, frame=frame_number, confidence=confidence)\n        self.last_position = position\n\n        return position\n\n    def _get_pose_guided_roi(\n        self,\n        frame: np.ndarray,\n        landmarks: PoseLandmarks\n    ) -&gt; Tuple[np.ndarray, Tuple[int, int]]:\n        \"\"\"\n        Get region of interest around upper body.\n        Reduces false positives by limiting search area.\n        \"\"\"\n        # Use shoulder and wrist to define upper body region\n        # Expand region to account for ball trajectory\n        pass\n</code></pre>"},{"location":"development/wallball-norep-detection/#calibrationpy","title":"<code>calibration.py</code>","text":"<pre><code>import numpy as np\nfrom typing import Optional\n\ndef calibrate_from_target(\n    target_pixel_y: int,\n    ground_pixel_y: int,\n    target_real_height_m: float = 3.0\n) -&gt; float:\n    \"\"\"\n    Calculate pixels-per-meter using known target height.\n\n    Best approach when using 45\u00b0 angle (target visible in frame).\n    \"\"\"\n    pixel_height = ground_pixel_y - target_pixel_y\n    pixels_per_meter = pixel_height / target_real_height_m\n    return pixels_per_meter\n\ndef calibrate_from_athlete(\n    athlete_landmarks: PoseLandmarks,\n    athlete_height_m: float\n) -&gt; float:\n    \"\"\"\n    Calculate pixels-per-meter using athlete's known height.\n\n    Fallback when target not visible or athlete height provided.\n    \"\"\"\n    head = athlete_landmarks[mp_pose.PoseLandmark.NOSE]\n    ankle = athlete_landmarks[mp_pose.PoseLandmark.LEFT_ANKLE]\n\n    pixel_height = ankle.y - head.y\n    pixels_per_meter = pixel_height / athlete_height_m\n\n    return pixels_per_meter\n\ndef pixels_to_meters(\n    pixel_value: float,\n    pixels_per_meter: float\n) -&gt; float:\n    \"\"\"Convert pixel measurement to meters\"\"\"\n    return pixel_value / pixels_per_meter\n\ndef meters_to_pixels(\n    meter_value: float,\n    pixels_per_meter: float\n) -&gt; float:\n    \"\"\"Convert meter measurement to pixels\"\"\"\n    return meter_value * pixels_per_meter\n</code></pre>"},{"location":"development/wallball-norep-detection/#debug_overlaypy","title":"<code>debug_overlay.py</code>","text":"<pre><code>from kinemotion.core.debug_overlay_utils import BaseDebugOverlayRenderer\nimport cv2\n\nclass WallBallDebugOverlayRenderer(BaseDebugOverlayRenderer):\n    \"\"\"Render debug overlay for wall ball analysis\"\"\"\n\n    def render_frame(\n        self,\n        frame: np.ndarray,\n        frame_number: int,\n        landmarks: PoseLandmarks,\n        ball_position: Optional[BallPosition],\n        current_phase: WallBallPhase,\n        rep_count: int,\n        violations: List[WallBallViolation],\n        target_y: int\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Render debug overlay on frame.\n\n        Visualizations:\n        - Pose skeleton\n        - Ball position (circle + trajectory trail)\n        - Target height line\n        - Rep counter\n        - Current phase\n        - Violations (red highlights)\n        - Knee angle (text)\n        \"\"\"\n        # Draw pose skeleton\n        self._draw_pose_skeleton(frame, landmarks)\n\n        # Draw ball\n        if ball_position:\n            cv2.circle(frame, (ball_position.x, ball_position.y), 10, (0, 255, 0), -1)\n            self._draw_ball_trajectory(frame)\n\n        # Draw target line\n        cv2.line(frame, (0, target_y), (frame.shape[1], target_y), (255, 255, 0), 2)\n        cv2.putText(frame, \"TARGET\", (10, target_y - 10),\n                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n\n        # Draw rep counter\n        cv2.putText(frame, f\"Reps: {rep_count}\", (10, 30),\n                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n        # Draw current phase\n        cv2.putText(frame, f\"Phase: {current_phase.value}\", (10, 70),\n                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n\n        # Draw violations\n        if violations:\n            y_offset = 110\n            cv2.putText(frame, \"NO-REP!\", (10, y_offset),\n                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 3)\n            for violation in violations[-3:]:  # Show last 3\n                y_offset += 40\n                cv2.putText(frame, violation.violation_type, (10, y_offset),\n                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n\n        return frame\n</code></pre>"},{"location":"development/wallball-norep-detection/#clipy","title":"<code>cli.py</code>","text":"<pre><code>import click\nfrom pathlib import Path\nfrom kinemotion.wallball.analysis import analyze_wallball_video\n\n@click.command(name=\"wallball-analyze\")\n@click.argument(\"video_path\", type=click.Path(exists=True))\n@click.option(\"--output\", \"-o\", type=click.Path(), help=\"Debug video output path\")\n@click.option(\"--target-height\", type=float, default=3.0,\n              help=\"Target height in meters (3.0 men, 2.7 women)\")\n@click.option(\"--athlete-height\", type=float, help=\"Athlete height in meters (for calibration)\")\n@click.option(\"--ball-color\", type=click.Choice([\"black\", \"dark_gray\", \"brown\", \"blue\"]),\n              default=\"black\", help=\"Medicine ball color preset\")\n@click.option(\"--squat-threshold\", type=float, default=90.0,\n              help=\"Knee angle threshold for squat depth (degrees)\")\n@click.option(\"--quality\", type=click.Choice([\"fast\", \"balanced\", \"accurate\"]),\n              default=\"balanced\", help=\"Auto-tuning quality preset\")\n@click.option(\"--json-output\", type=click.Path(), help=\"Save metrics as JSON\")\ndef wallball_analyze(\n    video_path: str,\n    output: str,\n    target_height: float,\n    athlete_height: float,\n    ball_color: str,\n    squat_threshold: float,\n    quality: str,\n    json_output: str\n):\n    \"\"\"\n    Analyze HYROX wall ball video for no-rep detection.\n\n    Example:\n        kinemotion wallball-analyze video.mp4 --output debug.mp4\n        kinemotion wallball-analyze video.mp4 --target-height 2.7 --ball-color blue\n    \"\"\"\n    click.echo(f\"Analyzing wall ball video: {video_path}\")\n    click.echo(f\"Target height: {target_height}m\")\n    click.echo(f\"Quality preset: {quality}\")\n\n    # Run analysis\n    metrics = analyze_wallball_video(\n        video_path=video_path,\n        target_height_m=target_height,\n        athlete_height_m=athlete_height,\n        ball_color_preset=ball_color,\n        squat_depth_threshold=squat_threshold,\n        quality=quality,\n        debug_video_path=output\n    )\n\n    # Display results\n    click.echo(\"\\n\" + \"=\"*50)\n    click.echo(\"WALL BALL ANALYSIS RESULTS\")\n    click.echo(\"=\"*50)\n    click.echo(f\"Total reps attempted: {metrics.total_reps_attempted}\")\n    click.echo(f\"Valid reps: {metrics.valid_reps}\")\n    click.echo(f\"No-reps: {metrics.no_reps}\")\n    click.echo(f\"\\nViolation breakdown:\")\n    click.echo(f\"  - Squat depth: {metrics.squat_depth_violations}\")\n    click.echo(f\"  - Ball height: {metrics.ball_height_violations}\")\n    click.echo(f\"  - Ball resting: {metrics.ball_resting_violations}\")\n    click.echo(f\"\\nAverage time per rep: {metrics.avg_time_per_rep:.2f}s\")\n\n    # Save JSON\n    if json_output:\n        import json\n        with open(json_output, 'w') as f:\n            json.dump(metrics.to_dict(), f, indent=2)\n        click.echo(f\"\\nMetrics saved to: {json_output}\")\n\n    if output:\n        click.echo(f\"Debug video saved to: {output}\")\n</code></pre>"},{"location":"development/wallball-norep-detection/#43-integration-points","title":"4.3 Integration Points","text":""},{"location":"development/wallball-norep-detection/#register-in-srckinemotionclipy","title":"Register in <code>src/kinemotion/cli.py</code>","text":"<pre><code>from kinemotion.wallball.cli import wallball_analyze\n\ndef create_cli():\n    @click.group()\n    def cli():\n        \"\"\"Kinemotion: Video-based kinematic analysis\"\"\"\n        pass\n\n    # Existing commands\n    cli.add_command(dropjump_analyze)\n    cli.add_command(cmj_analyze)\n\n    # New command\n    cli.add_command(wallball_analyze)\n\n    return cli\n</code></pre>"},{"location":"development/wallball-norep-detection/#export-in-srckinemotion__init__py","title":"Export in <code>src/kinemotion/__init__.py</code>","text":"<pre><code>from kinemotion.wallball.analysis import analyze_wallball_video\nfrom kinemotion.wallball.metrics import WallBallMetrics, WallBallRep, WallBallViolation\n\n__all__ = [\n    # Existing exports...\n    \"analyze_wallball_video\",\n    \"WallBallMetrics\",\n    \"WallBallRep\",\n    \"WallBallViolation\",\n]\n</code></pre>"},{"location":"development/wallball-norep-detection/#add-api-function-in-srckinemotionapipy","title":"Add API function in <code>src/kinemotion/api.py</code>","text":"<pre><code>def process_wallball_video(\n    video_path: str,\n    target_height_m: float = 3.0,\n    athlete_height_m: Optional[float] = None,\n    ball_color_preset: str = \"black\",\n    squat_depth_threshold: float = 90.0,\n    quality: str = \"balanced\",\n    debug_video_path: Optional[str] = None\n) -&gt; WallBallMetrics:\n    \"\"\"\n    Python API for wall ball analysis.\n\n    Args:\n        video_path: Path to video file\n        target_height_m: Target height in meters (3.0 men, 2.7 women)\n        athlete_height_m: Optional athlete height for calibration\n        ball_color_preset: Ball color (\"black\", \"dark_gray\", \"brown\", \"blue\")\n        squat_depth_threshold: Knee angle threshold in degrees\n        quality: Auto-tuning quality preset\n        debug_video_path: Optional debug video output\n\n    Returns:\n        WallBallMetrics with analysis results\n\n    Example:\n        &gt;&gt;&gt; from kinemotion import process_wallball_video\n        &gt;&gt;&gt; metrics = process_wallball_video(\"wallball.mp4\", target_height_m=3.0)\n        &gt;&gt;&gt; print(f\"Valid reps: {metrics.valid_reps}/{metrics.total_reps_attempted}\")\n    \"\"\"\n    from kinemotion.wallball.analysis import analyze_wallball_video\n\n    return analyze_wallball_video(\n        video_path=video_path,\n        target_height_m=target_height_m,\n        athlete_height_m=athlete_height_m,\n        ball_color_preset=ball_color_preset,\n        squat_depth_threshold=squat_depth_threshold,\n        quality=quality,\n        debug_video_path=debug_video_path\n    )\n</code></pre>"},{"location":"development/wallball-norep-detection/#6-implementation-phases","title":"6. Implementation Phases","text":""},{"location":"development/wallball-norep-detection/#phase-1-mvp-squat-depth-detection-only-1-week","title":"Phase 1: MVP - Squat Depth Detection Only (~1 week)","text":"<p>Goal: Basic rep counting with squat depth validation</p> <p>Deliverables:</p> <ul> <li>\u2705 Rep detection using squat cycles (reuse CMJ patterns)</li> <li>\u2705 Squat depth validation using joint angles</li> <li>\u2705 Basic metrics: total reps, valid reps, squat depth violations</li> <li>\u2705 Simple CLI: <code>kinemotion wallball-analyze video.mp4</code></li> <li>\u2705 Python API: <code>process_wallball_video()</code></li> <li>\u2705 Unit tests for squat detection</li> </ul> <p>Value: Immediate feedback on squat technique (33% of no-rep detection)</p> <p>Success criteria:</p> <ul> <li>Correctly counts reps from squat cycles</li> <li>Detects squat depth violations with 90% accuracy</li> <li>Works with 45\u00b0 camera angle</li> </ul>"},{"location":"development/wallball-norep-detection/#phase-2-ball-tracking-resting-detection-1-week","title":"Phase 2: Ball Tracking &amp; Resting Detection (~1 week)","text":"<p>Goal: Add ball tracking and resting violations</p> <p>Deliverables:</p> <ul> <li>\u2705 HSV-based ball detection with pose-guided ROI</li> <li>\u2705 Ball trajectory tracking</li> <li>\u2705 Ball resting detection (stationary on ground)</li> <li>\u2705 Enhanced metrics: ball tracking quality, resting violations</li> <li>\u2705 CLI options: <code>--ball-color</code>, <code>--resting-threshold</code></li> <li>\u2705 Unit tests for ball tracking</li> </ul> <p>Value: Complete no-rep detection except height (67% complete)</p> <p>Success criteria:</p> <ul> <li>Ball tracked successfully in 80%+ of frames</li> <li>Resting violations detected accurately</li> <li>Works with common ball colors (black, brown, blue)</li> </ul>"},{"location":"development/wallball-norep-detection/#phase-3-height-validation-1-week","title":"Phase 3: Height Validation (~1 week)","text":"<p>Goal: Add target height validation</p> <p>Deliverables:</p> <ul> <li>\u2705 Camera calibration (target-based and athlete-based)</li> <li>\u2705 Peak trajectory detection</li> <li>\u2705 Height violation detection</li> <li>\u2705 Complete metrics: all violation types</li> <li>\u2705 CLI options: <code>--target-height</code>, <code>--athlete-height</code></li> <li>\u2705 Unit tests for calibration and height validation</li> </ul> <p>Value: Full competition-standard validation (100% complete)</p> <p>Success criteria:</p> <ul> <li>Calibration accuracy within 5cm for known distances</li> <li>Height violations detected with 85%+ accuracy</li> <li>User can easily calibrate with target or athlete height</li> </ul>"},{"location":"development/wallball-norep-detection/#phase-4-polish-documentation-2-3-days","title":"Phase 4: Polish &amp; Documentation (~2-3 days)","text":"<p>Goal: Production-ready release</p> <p>Deliverables:</p> <ul> <li>\u2705 Debug overlay visualization (ball track, target line, violations)</li> <li>\u2705 Comprehensive tests (target: 20-30 tests, following kinemotion patterns)</li> <li>\u2705 User guide: <code>docs/guides/wallball-guide.md</code></li> <li>\u2705 Reference: <code>docs/reference/wallball-standards.md</code></li> <li>\u2705 Auto-tuning parameters (quality presets like dropjump)</li> <li>\u2705 Batch processing support</li> <li>\u2705 Performance optimization</li> </ul> <p>Value: Professional, well-documented feature</p> <p>Success criteria:</p> <ul> <li>All tests pass (including CI)</li> <li>Documentation complete and clear</li> <li>Performance: processes 60s video in \\&lt;2 minutes</li> <li>Code duplication stays \\&lt;3%</li> </ul>"},{"location":"development/wallball-norep-detection/#7-technical-challenges-mitigation","title":"7. Technical Challenges &amp; Mitigation","text":""},{"location":"development/wallball-norep-detection/#71-ball-tracking-reliability","title":"7.1 Ball Tracking Reliability","text":"<p>Challenge: Ball detection may fail due to lighting, motion blur, occlusion</p> <p>Risk Level: HIGH</p> <p>Mitigation strategies:</p> <ol> <li> <p>Temporal smoothing:</p> </li> <li> <p>Use <code>core/smoothing.py</code> patterns</p> </li> <li> <p>Interpolate missing frames using trajectory prediction</p> </li> <li> <p>Adaptive thresholding:</p> </li> <li> <p>Dynamically adjust HSV range based on lighting</p> </li> <li> <p>Learn ball color from initial frames</p> </li> <li> <p>Pose-guided ROI:</p> </li> <li> <p>Limit search area to reduce false positives</p> </li> <li> <p>Predict ball location based on hand position</p> </li> <li> <p>User calibration:</p> </li> <li> <p>Allow custom HSV ranges via CLI</p> </li> <li> <p>Provide visualization tool to help users select color range</p> </li> <li> <p>Fallback strategies:</p> </li> <li> <p>If tracking fails, rely on squat depth only (partial validation)</p> </li> <li>Warn user about tracking quality in metrics</li> </ol> <p>Acceptance criteria:</p> <ul> <li>Ball tracked successfully in 80%+ of frames (good conditions)</li> <li>Graceful degradation when tracking fails</li> <li>Clear user feedback about tracking quality</li> </ul>"},{"location":"development/wallball-norep-detection/#72-calibration-accuracy","title":"7.2 Calibration Accuracy","text":"<p>Challenge: Converting pixels to meters requires accurate calibration</p> <p>Risk Level: MEDIUM</p> <p>Mitigation strategies:</p> <ol> <li> <p>Multiple calibration methods:</p> </li> <li> <p>Target-based (preferred for 45\u00b0 angle)</p> </li> <li>Athlete height-based (fallback)</li> <li> <p>Manual override (user provides pixels-per-meter)</p> </li> <li> <p>Validation:</p> </li> <li> <p>Sanity checks on calibration values</p> </li> <li>Compare athlete height in video to known height</li> <li> <p>Warn if calibration seems off</p> </li> <li> <p>Tolerance margins:</p> </li> <li> <p>Don't require pixel-perfect height</p> </li> <li> <p>Use reasonable tolerances (e.g., \u00b15cm)</p> </li> <li> <p>User guidance:</p> </li> <li> <p>Clear documentation on camera setup</p> </li> <li>Tips for improving calibration accuracy</li> </ol> <p>Acceptance criteria:</p> <ul> <li>Calibration accuracy within 5cm for known distances</li> <li>Clear error messages for bad calibration</li> <li>Documented process for manual calibration</li> </ul>"},{"location":"development/wallball-norep-detection/#73-lighting-variations","title":"7.3 Lighting Variations","text":"<p>Challenge: HSV color detection sensitive to lighting changes</p> <p>Risk Level: MEDIUM</p> <p>Mitigation strategies:</p> <ol> <li> <p>Preprocessing:</p> </li> <li> <p>Histogram equalization</p> </li> <li> <p>Adaptive brightness/contrast adjustment</p> </li> <li> <p>Multiple color models:</p> </li> <li> <p>HSV (primary)</p> </li> <li> <p>LAB color space (lighting-independent, if HSV fails)</p> </li> <li> <p>User presets:</p> </li> <li> <p>Provide presets for common conditions (indoor, outdoor, bright, dim)</p> </li> <li> <p>Real-time feedback:</p> </li> <li> <p>Show ball detection in debug overlay</p> </li> <li>Allow user to adjust color range interactively</li> </ol> <p>Acceptance criteria:</p> <ul> <li>Works in typical gym lighting</li> <li>User can calibrate for unusual lighting</li> <li>Clear documentation about lighting requirements</li> </ul>"},{"location":"development/wallball-norep-detection/#74-occlusion-handling","title":"7.4 Occlusion Handling","text":"<p>Challenge: Ball may be hidden behind athlete during motion</p> <p>Risk Level: LOW (45\u00b0 angle reduces this)</p> <p>Mitigation strategies:</p> <ol> <li> <p>Trajectory prediction:</p> </li> <li> <p>Use physics (parabolic motion) to predict occluded frames</p> </li> <li> <p>Interpolate position during brief occlusions</p> </li> <li> <p>Phase awareness:</p> </li> <li> <p>Know when ball should be visible (flight phase)</p> </li> <li> <p>Don't penalize missing detections during expected occlusion</p> </li> <li> <p>Multiple tracking approaches:</p> </li> <li> <p>Optical flow as backup</p> </li> <li>Kalman filter for prediction</li> </ol> <p>Acceptance criteria:</p> <ul> <li>Handle brief occlusions (\\&lt; 0.5 seconds)</li> <li>Accurate trajectory reconstruction</li> </ul>"},{"location":"development/wallball-norep-detection/#75-different-ball-colorsmaterials","title":"7.5 Different Ball Colors/Materials","text":"<p>Challenge: Medicine balls come in many colors (black, brown, blue, red, camouflage)</p> <p>Risk Level: LOW</p> <p>Mitigation strategies:</p> <ol> <li> <p>Color presets:</p> </li> <li> <p>Provide presets for common balls</p> </li> <li> <p>Make presets easy to add</p> </li> <li> <p>Learning mode:</p> </li> <li> <p>Let user click on ball in first frame</p> </li> <li> <p>Auto-detect HSV range from selection</p> </li> <li> <p>Documentation:</p> </li> <li> <p>List tested ball colors</p> </li> <li>Instructions for custom colors</li> </ol> <p>Acceptance criteria:</p> <ul> <li>Works with 4+ common ball colors out-of-box</li> <li>Easy calibration for custom colors</li> </ul>"},{"location":"development/wallball-norep-detection/#8-testing-strategy","title":"8. Testing Strategy","text":""},{"location":"development/wallball-norep-detection/#81-test-coverage-goals","title":"8.1 Test Coverage Goals","text":"<p>Following kinemotion standards (70 tests for 2 jump types):</p> <ul> <li>Target: 20-30 tests for wall ball module</li> <li>Code duplication: Keep \\&lt; 3%</li> <li>Type coverage: 100% (pyright strict)</li> </ul>"},{"location":"development/wallball-norep-detection/#82-unit-tests","title":"8.2 Unit Tests","text":""},{"location":"development/wallball-norep-detection/#testswallballtest_analysispy","title":"<code>tests/wallball/test_analysis.py</code>","text":"<ul> <li>Rep detection from squat cycles</li> <li>Phase detection (standing \u2192 squat \u2192 throw \u2192 catch)</li> <li>Integration with auto-tuning system</li> </ul>"},{"location":"development/wallball-norep-detection/#testswallballtest_ball_trackingpy","title":"<code>tests/wallball/test_ball_tracking.py</code>","text":"<ul> <li>HSV color detection</li> <li>Circularity filtering</li> <li>Pose-guided ROI calculation</li> <li>Trajectory tracking and peak detection</li> <li>Handling of missing frames</li> </ul>"},{"location":"development/wallball-norep-detection/#testswallballtest_validationpy","title":"<code>tests/wallball/test_validation.py</code>","text":"<ul> <li>Squat depth validation (joint angles)</li> <li>Ball height validation</li> <li>Ball resting detection</li> <li>Complete rep validation</li> </ul>"},{"location":"development/wallball-norep-detection/#testswallballtest_calibrationpy","title":"<code>tests/wallball/test_calibration.py</code>","text":"<ul> <li>Target-based calibration</li> <li>Athlete-based calibration</li> <li>Pixels-to-meters conversion</li> <li>Edge cases (bad calibration values)</li> </ul>"},{"location":"development/wallball-norep-detection/#testswallballtest_metricspy","title":"<code>tests/wallball/test_metrics.py</code>","text":"<ul> <li>Dataclass JSON serialization</li> <li>NumPy type conversion</li> <li>Metrics calculation</li> </ul>"},{"location":"development/wallball-norep-detection/#83-integration-tests","title":"8.3 Integration Tests","text":""},{"location":"development/wallball-norep-detection/#testswallballtest_integrationpy","title":"<code>tests/wallball/test_integration.py</code>","text":"<ul> <li>End-to-end video processing</li> <li>CLI invocation</li> <li>Python API</li> <li>Debug overlay rendering</li> </ul>"},{"location":"development/wallball-norep-detection/#84-test-fixtures","title":"8.4 Test Fixtures","text":""},{"location":"development/wallball-norep-detection/#required-test-videos-create-or-source","title":"Required test videos (create or source)","text":"<ol> <li> <p><code>good_reps.mp4</code>: 10 perfect reps</p> </li> <li> <p>All reps valid</p> </li> <li> <p>Good lighting, clear ball visibility</p> </li> <li> <p><code>depth_norep.mp4</code>: Squat depth violations</p> </li> <li> <p>5 reps with insufficient depth</p> </li> <li> <p>Various knee angles (85\u00b0, 95\u00b0, 100\u00b0)</p> </li> <li> <p><code>height_norep.mp4</code>: Ball height violations</p> </li> <li> <p>5 reps where ball doesn't reach target</p> </li> <li> <p>Various heights (2.5m, 2.7m, 2.9m for 3m target)</p> </li> <li> <p><code>resting_norep.mp4</code>: Ball resting violations</p> </li> <li> <p>3 reps with ball resting on ground</p> </li> <li> <p>Various rest durations (1s, 2s, 3s)</p> </li> <li> <p><code>mixed_noreps.mp4</code>: Multiple violation types</p> </li> <li> <p>Real-world scenario with various issues</p> </li> <li> <p><code>difficult_lighting.mp4</code>: Challenging conditions</p> </li> <li> <p>Bright/dim lighting, shadows</p> </li> <li> <p>Tests robustness</p> </li> <li> <p><code>occlusion.mp4</code>: Ball partially occluded</p> </li> <li> <p>Tests trajectory prediction</p> </li> </ol>"},{"location":"development/wallball-norep-detection/#fixture-metadata-fixturesmetadatajson","title":"Fixture metadata (<code>fixtures/metadata.json</code>)","text":"<pre><code>{\n  \"good_reps.mp4\": {\n    \"fps\": 30,\n    \"total_reps\": 10,\n    \"expected_valid_reps\": 10,\n    \"target_height_m\": 3.0,\n    \"ball_color\": \"black\"\n  },\n  \"depth_norep.mp4\": {\n    \"fps\": 30,\n    \"total_reps\": 5,\n    \"expected_valid_reps\": 0,\n    \"expected_violations\": {\n      \"squat_depth\": 5\n    }\n  }\n}\n</code></pre>"},{"location":"development/wallball-norep-detection/#85-manual-testing-checklist","title":"8.5 Manual Testing Checklist","text":"<p>Before each release:</p> <ul> <li>[ ] Test with all color presets (black, dark_gray, brown, blue)</li> <li>[ ] Test with different target heights (2.7m, 3.0m)</li> <li>[ ] Test with different camera angles (pure lateral, 45\u00b0, 60\u00b0)</li> <li>[ ] Test with different video qualities (720p, 1080p, 4K)</li> <li>[ ] Test with different frame rates (30fps, 60fps, 120fps)</li> <li>[ ] Test calibration methods (target-based, athlete-based)</li> <li>[ ] Test debug overlay rendering</li> <li>[ ] Test JSON output format</li> <li>[ ] Verify code duplication \\&lt; 3% (<code>npx jscpd src/kinemotion</code>)</li> </ul>"},{"location":"development/wallball-norep-detection/#86-performance-benchmarks","title":"8.6 Performance Benchmarks","text":"<p>Target performance (60-second video, 1080p, 30fps):</p> <ul> <li>Processing time: \\&lt; 2 minutes</li> <li>Memory usage: \\&lt; 1GB</li> <li>Ball tracking success rate: &gt; 80%</li> </ul>"},{"location":"development/wallball-norep-detection/#87-cicd-integration","title":"8.7 CI/CD Integration","text":"<p>Add to existing CI pipeline:</p> <pre><code># .github/workflows/test.yml\n- name: Run wall ball tests\n  run: uv run pytest tests/wallball/ -v\n\n- name: Check code duplication\n  run: npx jscpd src/kinemotion --threshold 3\n</code></pre>"},{"location":"development/wallball-norep-detection/#9-research-references","title":"9. Research References","text":""},{"location":"development/wallball-norep-detection/#91-ball-tracking-research","title":"9.1 Ball Tracking Research","text":"<p>Color-based detection (HSV):</p> <ul> <li>OpenCV tutorials: HSV color segmentation for sports ball tracking</li> <li>Roboflow blog: \"Tracking Ball in Sports with Computer Vision\"</li> <li>Proved effective for basketball, volleyball, snooker</li> </ul> <p>Key findings:</p> <ul> <li>HSV more robust than RGB for color detection</li> <li>Circularity metric (4\u03c0A/P\u00b2) filters non-spherical objects</li> <li>Pose-guided ROI reduces false positives by 70%+</li> </ul> <p>Relevant code examples:</p> <pre><code># From Microsoft PromptCraft-Robotics\n# HSV-based basketball detection\nlower_orange = np.array([10, 100, 100])\nupper_orange = np.array([30, 255, 255])\nmask = cv2.inRange(hsv, lower_orange, upper_orange)\n\n# From Roboflow ball tracking blog\n# Trajectory visualization with deque buffer\nbuffer = deque(maxlen=buffer_size)\nbuffer.append(ball_position)\n</code></pre>"},{"location":"development/wallball-norep-detection/#92-squat-depth-detection-research","title":"9.2 Squat Depth Detection Research","text":"<p>Joint angle approaches:</p> <ul> <li>MediaPipe Pose: 33-landmark model with angle calculation utilities</li> <li>YOLOv8-Pose AI Gym: Uses joint angles for exercise counting</li> <li>cvzone PoseModule: <code>findAngle()</code> for exercise form validation</li> </ul> <p>Key findings:</p> <ul> <li>Joint angles more robust than vertical positions</li> <li>Knee angle \\&lt; 90\u00b0 indicates full squat depth</li> <li>Works with any camera angle (not just lateral)</li> </ul> <p>Relevant code examples:</p> <pre><code># From Ultralytics AI Gym\ngym_object = solutions.AIGym(\n    pose_type=\"squat\",\n    kpts_to_check=[6, 8, 10]  # Hip, knee, ankle\n)\n\n# From cvzone\nangle = detector.findAngle(\n    hip_landmark,\n    knee_landmark,\n    ankle_landmark\n)\n</code></pre>"},{"location":"development/wallball-norep-detection/#93-camera-calibration-research","title":"9.3 Camera Calibration Research","text":"<p>Pose-based calibration:</p> <ul> <li>AthletePose3D: Uses known body segment lengths</li> <li>Azure Kinect Jump Analysis: Reference-based calibration</li> <li>Common approach: Use athlete height or known reference objects</li> </ul> <p>Key findings:</p> <ul> <li>Single-camera calibration achieves \u00b15cm accuracy</li> <li>Target-based calibration more accurate than athlete-based</li> <li>Multiple reference points improve robustness</li> </ul>"},{"location":"development/wallball-norep-detection/#94-kinemotion-internal-references","title":"9.4 Kinemotion Internal References","text":"<p>Existing modules to reuse:</p> <ul> <li><code>core/pose.py</code>: MediaPipe pose tracking</li> <li><code>core/smoothing.py</code>: Savitzky-Golay filtering</li> <li><code>core/video_io.py</code>: Video reading/writing, rotation handling</li> <li><code>cmj/joint_angles.py</code>: Joint angle calculation</li> <li><code>cmj/analysis.py</code>: Phase detection patterns</li> <li><code>dropjump/analysis.py</code>: Auto-tuning system</li> <li><code>core/debug_overlay_utils.py</code>: Base visualization renderer</li> </ul> <p>Design patterns to follow:</p> <ul> <li>Auto-tuning quality presets (fast/balanced/accurate)</li> <li>Backward search for phase detection (CMJ pattern)</li> <li>Dataclass metrics with <code>to_dict()</code> for JSON serialization</li> <li>NumPy type conversion in serialization</li> <li>CLI structure with <code>@click.command</code></li> <li>Context managers for video I/O</li> </ul>"},{"location":"development/wallball-norep-detection/#10-future-enhancements","title":"10. Future Enhancements","text":""},{"location":"development/wallball-norep-detection/#101-short-term-6-months","title":"10.1 Short-term (6 months)","text":"<ol> <li> <p>Multiple ball tracking algorithms:</p> </li> <li> <p>Add YOLO as optional backend (more accurate, requires model)</p> </li> <li> <p>Add optical flow tracking (backup for occlusion)</p> </li> <li> <p>Improved calibration UI:</p> </li> <li> <p>Interactive calibration tool</p> </li> <li> <p>Click target/reference in video frame</p> </li> <li> <p>Real-time processing:</p> </li> <li> <p>Live webcam analysis (see <code>docs/technical/real-time-analysis.md</code>)</p> </li> <li> <p>Immediate no-rep feedback</p> </li> <li> <p>Mobile app integration:</p> </li> <li> <p>iOS/Android app with kinemotion backend</p> </li> <li>HYROX-specific UI</li> </ol>"},{"location":"development/wallball-norep-detection/#102-long-term-1-years","title":"10.2 Long-term (1+ years)","text":"<ol> <li> <p>Other HYROX exercises:</p> </li> <li> <p>Burpee broad jump</p> </li> <li>Sandbag lunges</li> <li>Sled push/pull</li> <li> <p>(Full HYROX workout analysis)</p> </li> <li> <p>3D tracking:</p> </li> <li> <p>Dual-camera stereo setup</p> </li> <li>Improved depth perception</li> <li> <p>Better height validation</p> </li> <li> <p>ML-based ball detection:</p> </li> <li> <p>Train custom YOLO model on medicine balls</p> </li> <li> <p>Improve robustness to lighting/occlusion</p> </li> <li> <p>Competition integration:</p> </li> <li> <p>Official HYROX judge tool</p> </li> <li>Real-time leaderboard integration</li> </ol>"},{"location":"development/wallball-norep-detection/#11-conclusion","title":"11. Conclusion","text":""},{"location":"development/wallball-norep-detection/#111-summary","title":"11.1 Summary","text":"<p>Wall ball no-rep detection is feasible and valuable for kinemotion:</p> <p>\u2705 Technical feasibility: Moderate-challenging, but achievable with existing architecture \u2705 Reusability: Leverages 70% of existing code (pose, angles, smoothing, video I/O) \u2705 User value: Immediate training feedback, competition preparation \u2705 HYROX relevance: One of 8 stations, popular exercise</p> <p>Key insights:</p> <ol> <li>Use 45\u00b0 camera angle for superior ball visibility</li> <li>Use joint angles for camera-agnostic squat validation</li> <li>Hybrid pose-guided HSV tracking balances simplicity and accuracy</li> <li>Phased implementation delivers value incrementally</li> </ol>"},{"location":"development/wallball-norep-detection/#112-estimated-effort","title":"11.2 Estimated Effort","text":"<p>Total: 3-4 weeks full-time development</p> Phase Effort Deliverable Phase 1: MVP (squat only) 1 week Basic rep counting + squat validation Phase 2: Ball tracking 1 week Ball detection + resting violations Phase 3: Height validation 1 week Complete no-rep detection Phase 4: Polish 2-3 days Documentation, tests, optimization"},{"location":"development/wallball-norep-detection/#113-recommendation","title":"11.3 Recommendation","text":"<p>Status: APPROVED for implementation \u2705</p> <p>Rationale:</p> <ul> <li>Clear user demand (HYROX growing sport)</li> <li>Technical feasibility confirmed</li> <li>Fits kinemotion's architecture perfectly</li> <li>Incremental delivery reduces risk</li> </ul> <p>Next steps:</p> <ol> <li>Create test fixtures (record wall ball videos)</li> <li>Implement Phase 1 MVP (1 week sprint)</li> <li>Validate with real athletes</li> <li>Iterate based on feedback</li> <li>Continue with Phases 2-4</li> </ol>"},{"location":"development/wallball-norep-detection/#114-success-metrics","title":"11.4 Success Metrics","text":"<p>Phase 1 success:</p> <ul> <li>[ ] Correctly counts reps from squat cycles</li> <li>[ ] Detects squat depth violations with 90% accuracy</li> <li>[ ] Works with 45\u00b0 camera angle</li> <li>[ ] Unit tests pass</li> </ul> <p>Phase 2 success:</p> <ul> <li>[ ] Ball tracked in 80%+ of frames</li> <li>[ ] Resting violations detected accurately</li> <li>[ ] Works with 4+ ball colors</li> </ul> <p>Phase 3 success:</p> <ul> <li>[ ] Height calibration within 5cm accuracy</li> <li>[ ] Height violations detected with 85% accuracy</li> <li>[ ] Easy user calibration process</li> </ul> <p>Overall success:</p> <ul> <li>[ ] All 20-30 tests pass</li> <li>[ ] Code duplication \\&lt; 3%</li> <li>[ ] Documentation complete</li> <li>[ ] Positive user feedback from HYROX athletes</li> </ul>"},{"location":"development/wallball-norep-detection/#appendix-a-cli-examples","title":"Appendix A: CLI Examples","text":"<pre><code># Basic usage\nkinemotion wallball-analyze video.mp4\n\n# With debug video output\nkinemotion wallball-analyze video.mp4 --output debug.mp4\n\n# Women's target height\nkinemotion wallball-analyze video.mp4 --target-height 2.7\n\n# Custom ball color\nkinemotion wallball-analyze video.mp4 --ball-color blue\n\n# With athlete height calibration\nkinemotion wallball-analyze video.mp4 --athlete-height 1.75\n\n# Save JSON metrics\nkinemotion wallball-analyze video.mp4 --json-output metrics.json\n\n# Batch processing\nkinemotion wallball-analyze videos/*.mp4 --batch --workers 4\n</code></pre>"},{"location":"development/wallball-norep-detection/#appendix-b-python-api-examples","title":"Appendix B: Python API Examples","text":"<pre><code>from kinemotion import process_wallball_video\n\n# Basic usage\nmetrics = process_wallball_video(\"wallball.mp4\")\nprint(f\"Valid reps: {metrics.valid_reps}/{metrics.total_reps_attempted}\")\n\n# With debug video\nmetrics = process_wallball_video(\n    \"wallball.mp4\",\n    debug_video_path=\"debug.mp4\"\n)\n\n# Women's competition\nmetrics = process_wallball_video(\n    \"wallball.mp4\",\n    target_height_m=2.7  # Women's target\n)\n\n# Custom ball color\nmetrics = process_wallball_video(\n    \"wallball.mp4\",\n    ball_color_preset=\"blue\"\n)\n\n# With calibration\nmetrics = process_wallball_video(\n    \"wallball.mp4\",\n    athlete_height_m=1.75  # Athlete's known height\n)\n\n# Access detailed results\nfor rep in metrics.reps:\n    if not rep.is_valid:\n        print(f\"Rep {rep.rep_number}: NO-REP\")\n        for violation in rep.violations:\n            print(f\"  - {violation.violation_type}: {violation.details}\")\n</code></pre>"},{"location":"development/wallball-norep-detection/#appendix-c-related-documentation","title":"Appendix C: Related Documentation","text":"<p>After implementation, create these user-facing docs:</p> <ul> <li><code>docs/guides/wallball-guide.md</code>: Step-by-step user guide</li> <li><code>docs/reference/wallball-standards.md</code>: HYROX standards reference</li> <li><code>docs/technical/wallball-detection.md</code>: Technical implementation details</li> </ul> <p>Update existing docs:</p> <ul> <li><code>docs/README.md</code>: Add wall ball to navigation</li> <li><code>CLAUDE.md</code>: Add wall ball to quick reference</li> <li><code>docs/guides/camera-setup.md</code>: Add wall ball camera setup section</li> </ul> <p>Document version: 1.0 Last updated: 2025-11-07 Author: Development team Status: Ready for implementation</p>"},{"location":"guides/bulk-processing/","title":"Bulk Video Processing with Kinemotion","text":"<p>This guide covers different approaches for processing large batches of videos using kinemotion as a library.</p> <p>Note: Some code examples in this guide use low-level API calls and may reference the removed <code>drop_height</code> parameter. For production use, prefer the high-level <code>process_dropjump_video()</code> and <code>process_cmj_video()</code> functions with <code>quality</code> presets (<code>\"fast\"</code>, <code>\"balanced\"</code>, or <code>\"accurate\"</code>) instead. See the API documentation for current best practices.</p>"},{"location":"guides/bulk-processing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Quick Start: Local Parallel Processing</li> <li>Cloud Platform: Modal.com</li> <li>Cloud Platform: AWS Batch</li> <li>Cloud Platform: Google Cloud Run</li> <li>Jupyter Notebook Examples</li> <li>Performance Comparison</li> <li>GPU Acceleration: Why Not?</li> </ul>"},{"location":"guides/bulk-processing/#quick-start-local-parallel-processing","title":"Quick Start: Local Parallel Processing","text":"<p>The simplest approach uses Python's built-in multiprocessing to analyze multiple videos in parallel.</p>"},{"location":"guides/bulk-processing/#basic-example","title":"Basic Example","text":"<pre><code>from concurrent.futures import ProcessPoolExecutor\nfrom pathlib import Path\nfrom kinemotion.core import VideoProcessor, PoseTracker\nfrom kinemotion.dropjump import (\n    detect_ground_contact,\n    calculate_drop_jump_metrics,\n    compute_average_foot_position\n)\nfrom kinemotion.core.auto_tuning import auto_tune_parameters\n\ndef analyze_video(video_path: str, quality: str = \"balanced\") -&gt; dict:\n    \"\"\"Analyze a single drop jump video.\"\"\"\n    try:\n        # Use the high-level API which handles auto-tuning\n        from kinemotion import process_dropjump_video\n\n        metrics = process_dropjump_video(\n            video_path,\n            quality=quality,\n            verbose=False\n        )\n\n        return {\n            \"video\": video_path,\n            \"success\": True,\n            **metrics.to_dict()\n        }\n    except Exception as e:\n        return {\n            \"video\": video_path,\n            \"success\": False,\n            \"error\": str(e)\n        }\n\n# Process videos in parallel\nvideo_dir = Path(\"./videos\")\nvideo_files = list(video_dir.glob(\"*.mp4\"))\n\nwith ProcessPoolExecutor(max_workers=8) as executor:\n    results = list(executor.map(\n        lambda p: analyze_video(str(p), \"balanced\"),\n        video_files\n    ))\n\n# Print summary\nsuccessful = sum(1 for r in results if r[\"success\"])\nprint(f\"Processed {successful}/{len(results)} videos successfully\")\n</code></pre>"},{"location":"guides/bulk-processing/#performance","title":"Performance","text":"<ul> <li>8 cores: ~30x faster than sequential processing</li> <li>16 cores: ~50x faster</li> <li>Cost: $0 (local machine) or ~$50/month (cloud VM)</li> </ul>"},{"location":"guides/bulk-processing/#cloud-platform-modalcom","title":"Cloud Platform: Modal.com","text":"<p>Modal is a Python-native serverless platform that makes it easy to run batch processing jobs in the cloud with automatic scaling.</p>"},{"location":"guides/bulk-processing/#why-modal","title":"Why Modal?","text":"<ul> <li>Simple Python API: Decorators turn functions into cloud jobs</li> <li>Auto-scaling: Automatically scales to 100s of parallel workers</li> <li>No infrastructure: No Docker, Kubernetes, or server management</li> <li>Pay per use: Only pay for compute time used</li> <li>Fast cold starts: Jobs start in seconds</li> </ul>"},{"location":"guides/bulk-processing/#installation","title":"Installation","text":"<pre><code>pip install modal\nmodal setup  # Configure API key (free tier available)\n</code></pre>"},{"location":"guides/bulk-processing/#complete-example","title":"Complete Example","text":"<p>Create a file <code>batch_processor.py</code>:</p> <pre><code>\"\"\"\nBulk video processing with Kinemotion on Modal.com\n\nUsage:\n    # Process videos from URLs\n    modal run batch_processor.py --video-list videos.txt --quality balanced\n\n    # Process videos from S3 bucket\n    modal run batch_processor.py --s3-bucket my-videos --quality accurate\n\"\"\"\n\nimport modal\nfrom pathlib import Path\n\n# Define the Modal app\napp = modal.App(\"kinemotion-bulk-processor\")\n\n# Define the container image with dependencies\nimage = (\n    modal.Image.debian_slim()\n    .apt_install(\"ffmpeg\")  # Required for video processing\n    .pip_install(\n        \"kinemotion\",\n        \"opencv-python-headless\",  # Headless version for cloud\n        \"mediapipe\",\n        \"numpy\",\n        \"scipy\",\n        \"boto3\",  # For S3 support\n    )\n)\n\n@app.function(\n    image=image,\n    cpu=4,  # 4 CPU cores per job\n    memory=8192,  # 8GB RAM\n    timeout=900,  # 15 minute timeout\n    retries=2,  # Retry failed jobs twice\n)\ndef process_single_video(\n    video_url: str,\n    drop_height: float = 0.40,\n    quality: str = \"balanced\",\n    output_json: bool = False\n) -&gt; dict:\n    \"\"\"\n    Process a single video and return metrics.\n\n    Args:\n        video_url: URL to video file (http://, https://, or s3://)\n        drop_height: Drop box height in meters\n        quality: Quality preset (fast/balanced/accurate)\n        output_json: Whether to save JSON output\n\n    Returns:\n        Dictionary with analysis results\n    \"\"\"\n    import tempfile\n    import urllib.request\n    import json\n    from kinemotion.core import VideoProcessor, PoseTracker\n    from kinemotion.dropjump import (\n        detect_ground_contact,\n        calculate_drop_jump_metrics,\n        compute_average_foot_position\n    )\n    from kinemotion.core.auto_tuning import auto_tune_parameters\n\n    print(f\"Processing: {video_url}\")\n\n    try:\n        # Download video to temporary file\n        with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp:\n            if video_url.startswith('s3://'):\n                # Download from S3\n                import boto3\n                bucket, key = video_url.replace('s3://', '').split('/', 1)\n                s3 = boto3.client('s3')\n                s3.download_file(bucket, key, tmp.name)\n            else:\n                # Download from HTTP(S)\n                urllib.request.urlretrieve(video_url, tmp.name)\n\n            video_path = tmp.name\n\n        # Auto-tune parameters\n        params = auto_tune_parameters(video_path, quality_preset=quality)\n        print(f\"  FPS: {params['fps']}, Quality: {quality}\")\n\n        # Process video\n        video = VideoProcessor(video_path)\n        tracker = PoseTracker(\n            detection_confidence=params[\"detection_confidence\"],\n            tracking_confidence=params[\"tracking_confidence\"]\n        )\n\n        # Extract landmarks\n        landmarks = []\n        frame_count = 0\n        for frame in video.read_frames():\n            pose_result = tracker.process_frame(frame)\n            if pose_result:\n                landmarks.append(pose_result)\n            frame_count += 1\n            if frame_count % 100 == 0:\n                print(f\"  Processed {frame_count} frames...\")\n\n        print(f\"  Total frames: {frame_count}\")\n\n        # Detect contact states\n        foot_positions = [compute_average_foot_position(lm) for lm in landmarks]\n        contact_states = detect_ground_contact(\n            foot_positions,\n            video.fps,\n            velocity_threshold=params[\"velocity_threshold\"],\n            min_contact_frames=params[\"min_contact_frames\"],\n            visibility_threshold=params[\"visibility_threshold\"]\n        )\n\n        # Calculate metrics\n        metrics = calculate_drop_jump_metrics(\n            landmarks=landmarks,\n            contact_states=contact_states,\n            fps=video.fps,\n            drop_height_m=drop_height\n        )\n\n        result = {\n            \"video_url\": video_url,\n            \"success\": True,\n            \"fps\": video.fps,\n            \"frame_count\": frame_count,\n            **metrics.to_dict()\n        }\n\n        # Save JSON if requested\n        if output_json:\n            json_path = video_path.replace('.mp4', '_metrics.json')\n            with open(json_path, 'w') as f:\n                json.dump(result, f, indent=2)\n            print(f\"  Saved: {json_path}\")\n\n        print(f\"  \u2713 Success - Contact: {metrics.ground_contact_time_ms}ms, \"\n              f\"Jump: {metrics.jump_height_m:.3f}m\")\n\n        return result\n\n    except Exception as e:\n        print(f\"  \u2717 Error: {e}\")\n        return {\n            \"video_url\": video_url,\n            \"success\": False,\n            \"error\": str(e)\n        }\n\n@app.local_entrypoint()\ndef main(\n    video_list: str = None,\n    s3_bucket: str = None,\n    s3_prefix: str = \"\",\n    drop_height: float = 0.40,\n    quality: str = \"balanced\",\n    output_dir: str = None\n):\n    \"\"\"\n    Process multiple videos in parallel.\n\n    Args:\n        video_list: Path to text file with video URLs (one per line)\n        s3_bucket: S3 bucket name to process all videos from\n        s3_prefix: S3 prefix/folder to filter videos\n        drop_height: Drop box height in meters\n        quality: Quality preset (fast/balanced/accurate)\n        output_dir: Directory to save results CSV\n    \"\"\"\n    import json\n    from datetime import datetime\n\n    # Collect video URLs\n    video_urls = []\n\n    if video_list:\n        # Load from file\n        with open(video_list) as f:\n            video_urls = [line.strip() for line in f if line.strip()]\n        print(f\"Loaded {len(video_urls)} videos from {video_list}\")\n\n    elif s3_bucket:\n        # List from S3 bucket\n        import boto3\n        s3 = boto3.client('s3')\n        paginator = s3.get_paginator('list_objects_v2')\n\n        for page in paginator.paginate(Bucket=s3_bucket, Prefix=s3_prefix):\n            for obj in page.get('Contents', []):\n                key = obj['Key']\n                if key.endswith(('.mp4', '.mov', '.avi')):\n                    video_urls.append(f\"s3://{s3_bucket}/{key}\")\n\n        print(f\"Found {len(video_urls)} videos in s3://{s3_bucket}/{s3_prefix}\")\n\n    else:\n        raise ValueError(\"Must provide --video-list or --s3-bucket\")\n\n    if not video_urls:\n        print(\"No videos to process!\")\n        return\n\n    # Process all videos in parallel\n    print(f\"\\nProcessing {len(video_urls)} videos with quality={quality}, \"\n          f\"drop_height={drop_height}m...\")\n    print(f\"Modal will auto-scale to process videos in parallel.\\n\")\n\n    start_time = datetime.now()\n\n    # Map processes all videos in parallel (Modal handles scaling)\n    results = list(process_single_video.map(\n        video_urls,\n        [drop_height] * len(video_urls),\n        [quality] * len(video_urls),\n        [False] * len(video_urls)  # output_json\n    ))\n\n    elapsed = (datetime.now() - start_time).total_seconds()\n\n    # Print summary\n    successful = sum(1 for r in results if r[\"success\"])\n    failed = len(results) - successful\n\n    print(f\"\\n{'='*70}\")\n    print(f\"PROCESSING COMPLETE\")\n    print(f\"{'='*70}\")\n    print(f\"Total videos: {len(results)}\")\n    print(f\"Successful: {successful}\")\n    print(f\"Failed: {failed}\")\n    print(f\"Time: {elapsed:.1f}s ({elapsed/len(results):.1f}s per video average)\")\n    print(f\"{'='*70}\\n\")\n\n    # Save results to CSV\n    if output_dir:\n        import csv\n        from pathlib import Path\n\n        output_path = Path(output_dir)\n        output_path.mkdir(exist_ok=True)\n\n        csv_file = output_path / f\"results_{datetime.now():%Y%m%d_%H%M%S}.csv\"\n\n        with open(csv_file, 'w', newline='') as f:\n            if results:\n                writer = csv.DictWriter(f, fieldnames=results[0].keys())\n                writer.writeheader()\n                writer.writerows(results)\n\n        print(f\"Results saved to: {csv_file}\")\n\n    # Print failures\n    if failed &gt; 0:\n        print(\"\\nFailed videos:\")\n        for r in results:\n            if not r[\"success\"]:\n                print(f\"  - {r['video_url']}: {r.get('error', 'Unknown error')}\")\n</code></pre>"},{"location":"guides/bulk-processing/#usage-examples","title":"Usage Examples","text":"<pre><code># Process videos from URLs in a text file\nmodal run batch_processor.py \\\n    --video-list videos.txt \\\n    --drop-height 0.40 \\\n    --quality balanced \\\n    --output-dir results/\n\n# Process all videos in an S3 bucket\nmodal run batch_processor.py \\\n    --s3-bucket my-training-videos \\\n    --s3-prefix \"athletes/2024/\" \\\n    --drop-height 0.60 \\\n    --quality accurate\n\n# Fast processing for quick analysis\nmodal run batch_processor.py \\\n    --video-list videos.txt \\\n    --drop-height 0.40 \\\n    --quality fast\n</code></pre>"},{"location":"guides/bulk-processing/#creating-videostxt","title":"Creating videos.txt","text":"<pre><code>https://example.com/videos/athlete1.mp4\nhttps://example.com/videos/athlete2.mp4\ns3://my-bucket/video1.mp4\ns3://my-bucket/video2.mp4\n</code></pre>"},{"location":"guides/bulk-processing/#cost-estimation","title":"Cost Estimation","text":"<p>Modal pricing (as of 2024):</p> <ul> <li>CPU: $0.000025 per core-second</li> <li>Memory: $0.000003 per GB-second</li> <li>Free tier: 30 free GPU-hours per month</li> </ul> <p>Example calculation for 100 videos (30 seconds each):</p> <ul> <li>Processing time per video: ~60 seconds (4 cores)</li> <li>Total compute: 100 videos \u00d7 60s \u00d7 4 cores = 24,000 core-seconds</li> <li>Cost: 24,000 \u00d7 $0.000025 = $0.60 for 100 videos</li> <li>Plus memory: 24,000 \u00d7 8GB \u00d7 $0.000003 = $0.58</li> <li>Total: ~$1.20 for 100 videos</li> </ul>"},{"location":"guides/bulk-processing/#monitoring","title":"Monitoring","text":"<p>Modal provides a web dashboard to monitor jobs:</p> <pre><code># View running jobs\nmodal app list\n\n# View logs\nmodal app logs kinemotion-bulk-processor\n\n# Stop all jobs\nmodal app stop kinemotion-bulk-processor\n</code></pre>"},{"location":"guides/bulk-processing/#cloud-platform-aws-batch","title":"Cloud Platform: AWS Batch","text":"<p>For organizations already using AWS, AWS Batch provides a fully managed batch processing service.</p>"},{"location":"guides/bulk-processing/#setup-overview","title":"Setup Overview","text":"<ol> <li>Create Docker image with kinemotion</li> <li>Push to Amazon ECR</li> <li>Create AWS Batch job definition</li> <li>Submit jobs via boto3 or AWS CLI</li> </ol>"},{"location":"guides/bulk-processing/#dockerfile","title":"Dockerfile","text":"<p>Create <code>Dockerfile</code>:</p> <pre><code>FROM python:3.12-slim\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    ffmpeg \\\n    libgl1-mesa-glx \\\n    libglib2.0-0 \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Install Python dependencies\nRUN pip install --no-cache-dir \\\n    kinemotion \\\n    opencv-python-headless \\\n    mediapipe \\\n    boto3\n\n# Copy processing script\nCOPY process_video.py /app/process_video.py\nWORKDIR /app\n\nENTRYPOINT [\"python\", \"process_video.py\"]\n</code></pre>"},{"location":"guides/bulk-processing/#processing-script","title":"Processing Script","text":"<p>Create <code>process_video.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nAWS Batch job for processing a single video.\n\nEnvironment variables:\n    VIDEO_URL: S3 URL of video to process\n    DROP_HEIGHT: Drop height in meters\n    QUALITY: Quality preset (fast/balanced/accurate)\n    OUTPUT_BUCKET: S3 bucket for results\n\"\"\"\n\nimport os\nimport sys\nimport json\nimport tempfile\nimport boto3\nfrom kinemotion.core import VideoProcessor, PoseTracker\nfrom kinemotion.dropjump import (\n    detect_ground_contact,\n    calculate_drop_jump_metrics,\n    compute_average_foot_position\n)\nfrom kinemotion.core.auto_tuning import auto_tune_parameters\n\ndef main():\n    # Get parameters from environment\n    video_url = os.environ['VIDEO_URL']\n    drop_height = float(os.environ.get('DROP_HEIGHT', '0.40'))\n    quality = os.environ.get('QUALITY', 'balanced')\n    output_bucket = os.environ['OUTPUT_BUCKET']\n\n    print(f\"Processing: {video_url}\")\n    print(f\"Drop height: {drop_height}m, Quality: {quality}\")\n\n    # Parse S3 URL\n    bucket, key = video_url.replace('s3://', '').split('/', 1)\n\n    # Download video\n    s3 = boto3.client('s3')\n    with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp:\n        print(f\"Downloading from S3...\")\n        s3.download_file(bucket, key, tmp.name)\n        video_path = tmp.name\n\n    # Process video\n    params = auto_tune_parameters(video_path, quality_preset=quality)\n\n    video = VideoProcessor(video_path)\n    tracker = PoseTracker(\n        detection_confidence=params[\"detection_confidence\"],\n        tracking_confidence=params[\"tracking_confidence\"]\n    )\n\n    landmarks = []\n    for frame in video.read_frames():\n        pose_result = tracker.process_frame(frame)\n        if pose_result:\n            landmarks.append(pose_result)\n\n    foot_positions = [compute_average_foot_position(lm) for lm in landmarks]\n    contact_states = detect_ground_contact(\n        foot_positions,\n        video.fps,\n        velocity_threshold=params[\"velocity_threshold\"],\n        min_contact_frames=params[\"min_contact_frames\"],\n        visibility_threshold=params[\"visibility_threshold\"]\n    )\n\n    metrics = calculate_drop_jump_metrics(\n        landmarks=landmarks,\n        contact_states=contact_states,\n        fps=video.fps,\n        drop_height_m=drop_height\n    )\n\n    # Upload results to S3\n    result = {\n        \"video_url\": video_url,\n        \"success\": True,\n        **metrics.to_dict()\n    }\n\n    output_key = key.replace('.mp4', '_metrics.json')\n    s3.put_object(\n        Bucket=output_bucket,\n        Key=output_key,\n        Body=json.dumps(result, indent=2),\n        ContentType='application/json'\n    )\n\n    print(f\"Results uploaded to s3://{output_bucket}/{output_key}\")\n    print(f\"Contact time: {metrics.ground_contact_time_ms}ms\")\n    print(f\"Jump height: {metrics.jump_height_m:.3f}m\")\n\nif __name__ == '__main__':\n    try:\n        main()\n    except Exception as e:\n        print(f\"Error: {e}\", file=sys.stderr)\n        sys.exit(1)\n</code></pre>"},{"location":"guides/bulk-processing/#submit-jobs","title":"Submit Jobs","text":"<pre><code>import boto3\n\nbatch = boto3.client('batch')\ns3 = boto3.client('s3')\n\n# List videos to process\nvideos = []\npaginator = s3.get_paginator('list_objects_v2')\nfor page in paginator.paginate(Bucket='my-videos', Prefix='dropjumps/'):\n    for obj in page.get('Contents', []):\n        if obj['Key'].endswith('.mp4'):\n            videos.append(f\"s3://my-videos/{obj['Key']}\")\n\n# Submit batch jobs\njob_ids = []\nfor video_url in videos:\n    response = batch.submit_job(\n        jobName=f\"kinemotion-{video_url.split('/')[-1]}\",\n        jobQueue='video-processing-queue',\n        jobDefinition='kinemotion-processor:1',\n        containerOverrides={\n            'environment': [\n                {'name': 'VIDEO_URL', 'value': video_url},\n                {'name': 'DROP_HEIGHT', 'value': '0.40'},\n                {'name': 'QUALITY', 'value': 'balanced'},\n                {'name': 'OUTPUT_BUCKET', 'value': 'my-results'},\n            ]\n        }\n    )\n    job_ids.append(response['jobId'])\n\nprint(f\"Submitted {len(job_ids)} jobs\")\n</code></pre>"},{"location":"guides/bulk-processing/#cloud-platform-google-cloud-run","title":"Cloud Platform: Google Cloud Run","text":"<p>Google Cloud Run Jobs provide a simpler alternative to AWS Batch.</p>"},{"location":"guides/bulk-processing/#create-container","title":"Create Container","text":"<p>Same Dockerfile as AWS Batch section.</p>"},{"location":"guides/bulk-processing/#deploy-job","title":"Deploy Job","text":"<pre><code># Build and push container\ngcloud builds submit --tag gcr.io/PROJECT_ID/kinemotion-processor\n\n# Create job\ngcloud run jobs create kinemotion-batch \\\n    --image gcr.io/PROJECT_ID/kinemotion-processor \\\n    --tasks 100 \\\n    --max-retries 2 \\\n    --task-timeout 15m \\\n    --cpu 4 \\\n    --memory 8Gi \\\n    --set-env-vars DROP_HEIGHT=0.40,QUALITY=balanced\n</code></pre>"},{"location":"guides/bulk-processing/#execute-job","title":"Execute Job","text":"<pre><code># Execute with environment variables\ngcloud run jobs execute kinemotion-batch \\\n    --set-env-vars VIDEO_URL=gs://bucket/video1.mp4\n\n# Or use task parallelism\ngcloud run jobs execute kinemotion-batch \\\n    --tasks 50 \\\n    --set-env-vars VIDEO_LIST=gs://bucket/videos.txt\n</code></pre>"},{"location":"guides/bulk-processing/#jupyter-notebook-examples","title":"Jupyter Notebook Examples","text":"<p>Perfect for exploratory analysis and visualization.</p>"},{"location":"guides/bulk-processing/#basic-batch-analysis","title":"Basic Batch Analysis","text":"<pre><code># notebook.ipynb\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom concurrent.futures import ProcessPoolExecutor\nfrom kinemotion.dropjump import analyze_video\n\n# Process videos\nvideo_dir = Path(\"./videos\")\nvideos = list(video_dir.glob(\"*.mp4\"))\n\nwith ProcessPoolExecutor(max_workers=8) as executor:\n    results = list(executor.map(\n        lambda p: analyze_video(str(p), drop_height=0.40),\n        videos\n    ))\n\n# Create DataFrame\ndf = pd.DataFrame([r.to_dict() for r in results if r])\ndf['filename'] = [v.name for v in videos]\n\n# Display results\ndisplay(df[['filename', 'ground_contact_time_ms', 'flight_time_ms', 'jump_height_m']])\n\n# Visualize\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nsns.histplot(df['ground_contact_time_ms'], ax=axes[0], kde=True)\naxes[0].set_title('Ground Contact Time')\naxes[0].set_xlabel('Time (ms)')\n\nsns.histplot(df['flight_time_ms'], ax=axes[1], kde=True)\naxes[1].set_title('Flight Time')\naxes[1].set_xlabel('Time (ms)')\n\nsns.histplot(df['jump_height_m'], ax=axes[2], kde=True)\naxes[2].set_title('Jump Height')\naxes[2].set_xlabel('Height (m)')\n\nplt.tight_layout()\nplt.show()\n\n# Export\ndf.to_csv('results.csv', index=False)\ndf.to_excel('results.xlsx', index=False)\n</code></pre>"},{"location":"guides/bulk-processing/#performance-comparison","title":"Performance Comparison","text":"<p>Based on typical 30fps, 10-second videos (~300 frames):</p> Method Setup Time Processing Time (100 videos) Cost Scalability Sequential (1 core) 0 min ~30 min $0 Poor Local Parallel (8 cores) 0 min ~4 min $0 Limited Local Parallel (16 cores) 0 min ~2 min $50/mo VM Limited Modal.com 10 min ~1-2 min ~$1.20 Excellent AWS Batch 60 min ~1-2 min ~$2-5 Excellent Google Cloud Run 30 min ~1-2 min ~$2-5 Excellent <p>Recommendation: Start with local parallel processing, move to Modal.com when you need cloud scale.</p>"},{"location":"guides/bulk-processing/#gpu-acceleration-why-not","title":"GPU Acceleration: Why Not?","text":"<p>You might think GPU acceleration would speed up video processing. Here's why it won't:</p>"},{"location":"guides/bulk-processing/#mediapipe-gpu-reality","title":"MediaPipe GPU Reality","text":"<ol> <li>Python package limitations: pip-installed MediaPipe doesn't include GPU support</li> <li>Complex setup: Requires custom build with OpenGL/CUDA</li> <li>Limited speedup: Benchmarks show 1.5-2x at best, sometimes SLOWER:</li> <li>CPU: 19ms per frame</li> <li>GPU: 23ms per frame (overhead from data transfer)</li> <li>Batch size: MediaPipe processes one frame at a time, negating GPU batch benefits</li> </ol>"},{"location":"guides/bulk-processing/#cpu-parallelism-is-superior","title":"CPU Parallelism is Superior","text":"<ul> <li>8x CPU cores: 8x speedup (trivial to implement)</li> <li>GPU acceleration: 1.5-2x speedup (complex, unreliable)</li> <li>Result: CPU parallelism is 4-5x better!</li> </ul>"},{"location":"guides/bulk-processing/#when-gpu-might-help","title":"When GPU Might Help","text":"<p>GPU acceleration could theoretically help if:</p> <ul> <li>You're processing 4K/8K videos (larger frames)</li> <li>You're using batch-optimized models (MediaPipe isn't)</li> <li>You have 100+ videos queued per GPU (amortize overhead)</li> </ul> <p>But even then, the complexity rarely justifies the marginal gains.</p>"},{"location":"guides/bulk-processing/#recommendation","title":"Recommendation","text":"<p>Stick with CPU parallelism. It's simpler, faster, and more cost-effective for kinemotion.</p>"},{"location":"guides/bulk-processing/#best-practices","title":"Best Practices","text":""},{"location":"guides/bulk-processing/#error-handling","title":"Error Handling","text":"<p>Always wrap video processing in try-except to handle corrupt videos:</p> <pre><code>def process_video_safe(video_path: str) -&gt; dict:\n    try:\n        result = analyze_video(video_path)\n        return {\"success\": True, **result.to_dict()}\n    except Exception as e:\n        return {\n            \"success\": False,\n            \"video\": video_path,\n            \"error\": str(e)\n        }\n</code></pre>"},{"location":"guides/bulk-processing/#progress-tracking","title":"Progress Tracking","text":"<p>Use tqdm for progress bars:</p> <pre><code>from tqdm import tqdm\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\n\nwith ProcessPoolExecutor(max_workers=8) as executor:\n    futures = {\n        executor.submit(analyze_video, str(p)): p\n        for p in video_files\n    }\n\n    results = []\n    for future in tqdm(as_completed(futures), total=len(futures)):\n        results.append(future.result())\n</code></pre>"},{"location":"guides/bulk-processing/#memory-management","title":"Memory Management","text":"<p>For large batches, process in chunks to avoid memory issues:</p> <pre><code>def process_in_chunks(videos, chunk_size=100):\n    all_results = []\n\n    for i in range(0, len(videos), chunk_size):\n        chunk = videos[i:i + chunk_size]\n        print(f\"Processing chunk {i//chunk_size + 1}...\")\n\n        with ProcessPoolExecutor(max_workers=8) as executor:\n            results = list(executor.map(analyze_video, chunk))\n\n        all_results.extend(results)\n\n    return all_results\n</code></pre>"},{"location":"guides/bulk-processing/#result-validation","title":"Result Validation","text":"<p>Check for common issues:</p> <pre><code>def validate_results(results):\n    issues = []\n\n    for r in results:\n        if not r[\"success\"]:\n            issues.append(f\"Failed: {r['video']}\")\n        elif r.get(\"ground_contact_time_ms\", 0) &lt; 50:\n            issues.append(f\"Suspiciously short contact: {r['video']}\")\n        elif r.get(\"jump_height_m\", 0) &gt; 1.0:\n            issues.append(f\"Suspiciously high jump: {r['video']}\")\n\n    return issues\n</code></pre>"},{"location":"guides/bulk-processing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/bulk-processing/#too-many-open-files-error","title":"\"Too many open files\" error","text":"<p>Increase file descriptor limit:</p> <pre><code>ulimit -n 4096\n</code></pre> <p>Or in Python:</p> <pre><code>import resource\nresource.setrlimit(resource.RLIMIT_NOFILE, (4096, 4096))\n</code></pre>"},{"location":"guides/bulk-processing/#memory-issues","title":"Memory issues","text":"<p>Reduce max_workers or use chunked processing:</p> <pre><code># Instead of max_workers=16, use fewer\nwith ProcessPoolExecutor(max_workers=4) as executor:\n    ...\n</code></pre>"},{"location":"guides/bulk-processing/#slow-s3-downloads","title":"Slow S3 downloads","text":"<p>Use concurrent downloads:</p> <pre><code>import concurrent.futures\n\ndef download_from_s3(url):\n    # Download logic\n    pass\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n    local_paths = list(executor.map(download_from_s3, s3_urls))\n</code></pre>"},{"location":"guides/bulk-processing/#summary","title":"Summary","text":"<p>For most users:</p> <ol> <li>Start with local parallel processing (free, immediate)</li> <li>Scale to Modal.com when you need cloud processing (easiest)</li> <li>Skip GPU acceleration (not worth the complexity)</li> </ol> <p>For enterprise:</p> <ol> <li>Use AWS Batch or Google Cloud Run for integration with existing infrastructure</li> <li>Set up monitoring and error handling</li> <li>Implement retry logic and result validation</li> </ol> <p>The beauty of kinemotion being a library is its flexibility - choose the approach that fits your needs and infrastructure!</p>"},{"location":"guides/camera-setup/","title":"Camera Setup Guide","text":"<p>Versi\u00f3n en espa\u00f1ol disponible: camera-setup.md</p> <p>This guide provides best practices for recording drop jump and CMJ videos to ensure accurate analysis with kinemotion.</p>"},{"location":"guides/camera-setup/#overview","title":"Overview","text":"<p>Kinemotion now supports 45\u00b0 angle camera positioning as the standard setup, providing better landmark visibility and tracking accuracy compared to pure lateral views. This guide covers:</p> <ol> <li>Single iPhone at 45\u00b0 (recommended standard setup)</li> <li>Dual iPhone stereo setup (advanced - for improved accuracy)</li> </ol> <p>Why 45\u00b0 instead of lateral (90\u00b0)?</p> <p>Research shows that camera viewing angle significantly affects pose estimation accuracy. The 45\u00b0 angle provides:</p> <ul> <li>Better visibility: 40-60% ankle/knee visibility vs 18-27% in lateral view</li> <li>Reduced occlusion: Both legs more visible (less self-occlusion)</li> <li>Good sagittal plane capture: Still measures jump height and vertical motion accurately</li> <li>Practical compromise: Between frontal (high visibility, poor depth) and lateral (pure sagittal, high occlusion)</li> </ul>"},{"location":"guides/camera-setup/#setup-1-single-iphone-at-45-standard","title":"Setup 1: Single iPhone at 45\u00b0 (Standard)","text":""},{"location":"guides/camera-setup/#single-camera-positioning","title":"Single Camera Positioning","text":"<p>Recommended for: Most users, training environments, individual athlete assessment</p>"},{"location":"guides/camera-setup/#top-view-diagram-single-camera","title":"Top View Diagram (Single Camera)","text":"<pre><code>                    N (North - Athlete faces forward)\n                    \u2191\n\n        [Drop Box]  |\n            |       |\n            \u2193       |\n           \u2b24 Athlete (jumps straight up/down)\n            \u2198\n             \u2198 45\u00b0 angle\n              \u2198\n            [iPhone on Tripod]\n\nSide view visualization:\n\n    Athlete           iPhone\n       \u2b24  - - - - - - [\ud83d\udcf1]\n                      \u2191\n                   3-5m distance\n                   Hip height (130-150cm)\n</code></pre> <p>Key positioning:</p> <ul> <li>Angle: 45\u00b0 from athlete's sagittal plane (between side and front)</li> <li>Distance: 3-5 meters (optimal: 4 meters)</li> <li>Height: Hip level (130-150 cm from floor)</li> <li>Orientation: Landscape mode (horizontal)</li> </ul>"},{"location":"guides/camera-setup/#detailed-setup-instructions","title":"Detailed Setup Instructions","text":""},{"location":"guides/camera-setup/#1-physical-placement","title":"1. Physical Placement","text":"<p>Step-by-step:</p> <ol> <li>Position athlete at drop box - Have athlete stand at their jumping position</li> <li>Identify sagittal plane - Imagine a line from front to back through athlete's center</li> <li>Mark 45\u00b0 position - From athlete's side, move 45\u00b0 toward the front</li> <li>If athlete faces North, camera should be Southeast or Southwest</li> <li>Camera sees athlete's front-side (not pure profile)</li> <li>Set distance - Measure 3-5m from athlete's jumping position</li> <li>Set height - Camera lens at athlete's hip height (130-150 cm typical)</li> <li>Level tripod - Ensure camera is level (not tilted up/down)</li> </ol>"},{"location":"guides/camera-setup/#2-frame-composition","title":"2. Frame Composition","text":"<p>At 1080p (1920x1080), frame athlete like this:</p> <pre><code>|--------------------------|\n|    [10-15% margin top]   |\n|                          |\n|         \ud83d\udc64 Athlete       | \u2190 Full body visible\n|          \u2195               | \u2190 Entire jump height\n|         / \\              | \u2190 Both legs visible\n|        /   \\             |\n|    [Landing Area]        | \u2190 Floor visible\n|   [10-15% margin bottom] |\n|--------------------------|\n</code></pre> <p>Checklist:</p> <ul> <li>\u2705 Entire body visible (head to feet)</li> <li>\u2705 10-15% margin above head (for jump height)</li> <li>\u2705 Landing surface visible in frame</li> <li>\u2705 Athlete stays centered throughout movement</li> <li>\u2705 Both legs visible (key advantage of 45\u00b0 angle)</li> <li>\u274c Don't crop body parts</li> <li>\u274c Don't pan or zoom during recording</li> </ul>"},{"location":"guides/camera-setup/#3-camera-settings","title":"3. Camera Settings","text":"Setting Specification Reason Resolution 1080p (1920x1080) Minimum for accurate landmark detection Frame Rate 60 fps (30 fps minimum) 60 fps recommended for short ground contact times Orientation Landscape (horizontal) Wider field of view Focus Manual (locked on athlete) Prevents autofocus hunting Exposure Locked/manual Consistent brightness throughout video Shutter Speed 1/120s or faster Reduces motion blur Stabilization Tripod (required) Eliminates camera shake <p>iPhone-specific settings:</p> <pre><code>Camera app \u2192 Settings:\n- Format: Most Compatible (H.264)\n- Record Video: 1080p at 60fps\n- Lock Focus: Tap and hold on athlete\n- Lock Exposure: Swipe up/down to adjust, then lock\n</code></pre>"},{"location":"guides/camera-setup/#4-lighting","title":"4. Lighting","text":"<p>Best practices:</p> <ul> <li>Even lighting across athlete's body</li> <li>Avoid backlighting (athlete as silhouette)</li> <li>Indoor: Overhead gym lights typically sufficient</li> <li>Outdoor: Overcast conditions ideal (soft, even light)</li> </ul> <p>Why it matters: MediaPipe relies on visual contrast. Poor lighting reduces landmark visibility scores and analysis accuracy.</p>"},{"location":"guides/camera-setup/#5-background","title":"5. Background","text":"<p>Optimal:</p> <ul> <li>Plain wall or solid color background</li> <li>High contrast with athlete's clothing</li> <li>Minimal movement in background</li> </ul> <p>Avoid:</p> <ul> <li>Busy backgrounds (equipment, other people)</li> <li>Similar colors to athlete's clothing</li> <li>Reflective surfaces (mirrors, windows)</li> </ul>"},{"location":"guides/camera-setup/#expected-performance","title":"Expected Performance","text":"<p>Improvements over lateral (90\u00b0) view:</p> Metric Lateral View (90\u00b0) 45\u00b0 Angle Improvement Ankle/Knee Visibility 18-27% 40-60% +100-150% Joint Angle Accuracy ~10-15\u00b0 error ~8-12\u00b0 error ~20-30% better Detection Reliability Good Excellent More consistent Ground Contact Detection Challenging Easier More robust <p>Limitations:</p> <ul> <li>Still monocular (depth estimation noisy)</li> <li>No biomechanical constraints (vs Pose2Sim)</li> <li>Not research-grade (for that, use dual camera setup)</li> </ul>"},{"location":"guides/camera-setup/#camera-setup-checklist","title":"Camera Setup Checklist","text":"<p>Before recording, verify:</p> <ul> <li>[ ] iPhone on stable tripod (no movement during recording)</li> <li>[ ] Camera at 45\u00b0 angle from athlete's sagittal plane</li> <li>[ ] Distance: 3-5 meters from landing area</li> <li>[ ] Height: Camera lens at athlete's hip height (130-150cm)</li> <li>[ ] Framing: Full body visible (head to feet + 10-15% margins)</li> <li>[ ] Settings: 1080p, 60 fps, landscape orientation</li> <li>[ ] Focus: Locked on athlete (tap and hold)</li> <li>[ ] Exposure: Locked (consistent lighting)</li> <li>[ ] Lighting: Even, no harsh shadows or backlighting</li> <li>[ ] Background: Plain, minimal distractions</li> <li>[ ] Test recording: Athlete stays in frame throughout jump</li> </ul>"},{"location":"guides/camera-setup/#setup-2-dual-iphone-stereo-advanced","title":"Setup 2: Dual iPhone Stereo (Advanced)","text":""},{"location":"guides/camera-setup/#when-to-use-dual-camera-setup","title":"When to Use Dual Camera Setup","text":"<p>Recommended for:</p> <ul> <li>Research applications requiring higher accuracy</li> <li>Elite athlete assessment</li> <li>When depth accuracy is critical</li> <li>Biomechanical analysis requiring joint angles</li> </ul> <p>Benefits over single camera:</p> <ul> <li>~50% error reduction (30.1mm RMSE vs 56.3mm monocular)</li> <li>Accurate 3D reconstruction (eliminates depth ambiguity)</li> <li>Better landmark visibility (each camera sees different angles)</li> <li>Research-grade accuracy (with proper calibration and processing)</li> </ul> <p>Requirements:</p> <ul> <li>2 iPhones (same model recommended for matching settings)</li> <li>2 tripods</li> <li>Calibration pattern (ChArUco board or checkerboard)</li> <li>More complex processing workflow</li> </ul>"},{"location":"guides/camera-setup/#dual-camera-positioning","title":"Dual Camera Positioning","text":""},{"location":"guides/camera-setup/#optimal-configuration-45-from-sagittal-plane-90-separation","title":"Optimal configuration: \u00b145\u00b0 from sagittal plane, 90\u00b0 separation","text":""},{"location":"guides/camera-setup/#top-view-diagram-dual-camera","title":"Top View Diagram (Dual Camera)","text":"<pre><code>                    N (Athlete faces forward)\n                    \u2191\n\n    [iPhone 2]      |      [iPhone 1]\n    (Left side)     |      (Right side)\n         \u2198          |          \u2199\n          \u2198 45\u00b0     |      45\u00b0 \u2199\n           \u2198        |        \u2199\n             \u2198   [Box]    \u2199\n               \u2198    |   \u2199\n                 \u2198  \u2193 \u2199\n                   \u2b24 Athlete\n\n    Total separation: 90\u00b0 (optimal for triangulation)\n</code></pre> <p>Why 90\u00b0 separation?</p> <p>Research by Pagnon et al. (2022) and Dill et al. (2024) found 90\u00b0 angle between cameras optimal for stereo 3D reconstruction. This balances:</p> <ul> <li>Triangulation accuracy (wider angles better)</li> <li>Overlapping field of view (cameras must see same landmarks)</li> <li>Practical setup constraints</li> </ul>"},{"location":"guides/camera-setup/#detailed-dual-camera-setup","title":"Detailed Dual Camera Setup","text":""},{"location":"guides/camera-setup/#step-1-position-both-cameras","title":"Step 1: Position Both Cameras","text":"<p>iPhone 1 (Right camera):</p> <ul> <li>Position 45\u00b0 from athlete's right side</li> <li>If athlete faces North, camera is Southeast</li> <li>Distance: 3-5m from athlete</li> <li>Height: Hip level (130-150cm)</li> </ul> <p>iPhone 2 (Left camera):</p> <ul> <li>Position 45\u00b0 from athlete's left side</li> <li>If athlete faces North, camera is Southwest</li> <li>Distance: 3-5m from athlete (same as iPhone 1)</li> <li>Height: Hip level (match iPhone 1 exactly)</li> </ul> <p>Critical alignment:</p> <ul> <li>Both cameras at same height (\u00b12cm tolerance)</li> <li>Both cameras at same distance from athlete (\u00b110cm tolerance)</li> <li>Both cameras level (not tilted)</li> <li>90\u00b0 separation between cameras (\u00b15\u00b0 tolerance)</li> </ul>"},{"location":"guides/camera-setup/#step-2-frame-composition-both-cameras","title":"Step 2: Frame Composition (Both Cameras)","text":"<p>Both iPhones should frame the athlete identically:</p> <pre><code>Each camera view:\n|------------------------|\n|   [margin]             |\n|      \ud83d\udc64 Full body      | \u2190 Same framing\n|       \u2195 Jump height    | \u2190 Both cameras\n|      / \\               |\n|  [Landing area]        |\n|   [margin]             |\n|------------------------|\n</code></pre> <p>Synchronize framing:</p> <ul> <li>Athlete centered in both frames</li> <li>Same margins (10-15% top/bottom)</li> <li>Both see full jump sequence</li> <li>Landing area visible in both</li> </ul>"},{"location":"guides/camera-setup/#step-3-camera-settings-both-iphones","title":"Step 3: Camera Settings (Both iPhones)","text":""},{"location":"guides/camera-setup/#critical-both-cameras-must-have-identical-settings","title":"CRITICAL: Both cameras must have identical settings","text":"Setting Both Cameras Resolution 1080p (1920x1080) - exactly the same Frame Rate 60 fps - exactly the same Orientation Landscape - exactly the same Focus Manual, locked Exposure Manual, locked (same brightness) Format H.264, Most Compatible <p>Why identical settings matter:</p> <ul> <li>Synchronization requires matching frame rates</li> <li>Triangulation assumes same resolution</li> <li>Different exposures affect landmark detection</li> </ul>"},{"location":"guides/camera-setup/#step-4-synchronization","title":"Step 4: Synchronization","text":""},{"location":"guides/camera-setup/#option-a-manual-start-simple","title":"Option A: Manual start (simple)","text":"<ol> <li>Start recording on iPhone 1</li> <li>Start recording on iPhone 2 within 1-2 seconds</li> <li>Synchronization cue: Have athlete clap hands or jump once before actual test</li> <li>Use this event to sync videos in post-processing</li> </ol>"},{"location":"guides/camera-setup/#option-b-audio-sync-better","title":"Option B: Audio sync (better)","text":"<ol> <li>Use external audio cue (clap, beep, voice command)</li> <li>Both iPhones record audio</li> <li>Align videos using audio waveform in post-processing</li> <li>Software like Pose2Sim has built-in sync tools</li> </ol>"},{"location":"guides/camera-setup/#option-c-hardware-sync-best-requires-equipment","title":"Option C: Hardware sync (best, requires equipment)","text":"<ol> <li>Use external trigger device</li> <li>Starts both cameras simultaneously</li> <li>Most accurate synchronization</li> <li>Requires additional hardware</li> </ol> <p>Recommendation: Start with Option A (manual + clap sync), upgrade to Option B if needed.</p>"},{"location":"guides/camera-setup/#step-5-calibration","title":"Step 5: Calibration","text":"<p>Required: One-time calibration before first use or if camera positions change</p> <p>Calibration pattern options:</p> <ol> <li> <p>ChArUco board (recommended - more robust)</p> </li> <li> <p>Print large ChArUco pattern (A3 or larger)</p> </li> <li>Mount on rigid board</li> <li> <p>Grid size: 7x5 or similar</p> </li> <li> <p>Checkerboard (alternative)</p> </li> <li> <p>Print large checkerboard (A3 or larger)</p> </li> <li>8x6 or 9x7 grid</li> <li>Ensure perfectly flat</li> </ol> <p>Calibration procedure:</p> <pre><code># If using Pose2Sim\n1. Record calibration pattern from both cameras\n2. Move pattern through capture volume (10-15 different positions)\n3. Ensure pattern visible in both cameras simultaneously\n4. Run calibration:\n   Pose2Sim.calibration()\n</code></pre> <p>Calibration outputs:</p> <ul> <li>Camera intrinsics (focal length, distortion)</li> <li>Camera extrinsics (relative positions, rotation)</li> <li>Saves to calibration file for reuse</li> </ul> <p>Re-calibrate when:</p> <ul> <li>Camera positions change</li> <li>Different lenses used</li> <li>After several weeks (drift check)</li> </ul>"},{"location":"guides/camera-setup/#processing-dual-camera-videos","title":"Processing Dual Camera Videos","text":"<p>Current kinemotion support: Single camera only</p> <p>To process stereo videos, you'll need:</p>"},{"location":"guides/camera-setup/#option-a-use-pose2sim-recommended","title":"Option A: Use Pose2Sim (recommended)","text":"<pre><code># Install Pose2Sim\npip install pose2sim\n\n# Process stereo videos\nPose2Sim.calibration()      # One-time\nPose2Sim.poseEstimation()   # Run MediaPipe on both videos\nPose2Sim.synchronization()  # Sync videos\nPose2Sim.triangulation()    # 3D reconstruction\nPose2Sim.filtering()        # Smooth trajectories\nPose2Sim.kinematics()       # OpenSim joint angles\n</code></pre>"},{"location":"guides/camera-setup/#option-b-future-kinemotion-stereo-support","title":"Option B: Future kinemotion stereo support","text":"<p>Dual camera support may be added to kinemotion in future versions. Current roadmap:</p> <ul> <li>Stereo triangulation module</li> <li>Automatic synchronization</li> <li>Integrated calibration workflow</li> </ul>"},{"location":"guides/camera-setup/#option-c-manual-triangulation","title":"Option C: Manual triangulation","text":"<p>If you have programming experience, implement stereo triangulation using OpenCV and MediaPipe output from both cameras.</p>"},{"location":"guides/camera-setup/#expected-performance-dual-camera","title":"Expected Performance (Dual Camera)","text":"<p>Accuracy improvements over single camera:</p> Metric Single Camera (45\u00b0) Dual Camera (Stereo) Improvement Position RMSE ~56mm ~30mm 47% better Joint Angle Error ~8-12\u00b0 ~5-7\u00b0 ~30-40% better Depth Accuracy Poor (noisy) Good Eliminates ambiguity Landmark Visibility 40-60% 70-90% Multi-angle coverage <p>Validated research:</p> <ul> <li>Dill et al. (2024): Stereo MediaPipe achieved 30.1mm RMSE vs Qualisys gold standard</li> <li>Pagnon et al. (2022): 90\u00b0 camera separation optimal for triangulation</li> </ul>"},{"location":"guides/camera-setup/#dual-camera-checklist","title":"Dual Camera Checklist","text":"<p>Before recording, verify:</p> <ul> <li>[ ] Both iPhones on stable tripods</li> <li>[ ] Camera 1 at +45\u00b0 from athlete's right side</li> <li>[ ] Camera 2 at -45\u00b0 from athlete's left side</li> <li>[ ] 90\u00b0 total separation between cameras</li> <li>[ ] Same distance (3-5m) from athlete for both cameras</li> <li>[ ] Same height (hip level, 130-150cm) for both cameras</li> <li>[ ] Both level (not tilted up/down)</li> <li>[ ] Identical settings (1080p, 60fps, landscape)</li> <li>[ ] Identical focus and exposure locked</li> <li>[ ] Sync method planned (clap, audio cue, etc.)</li> <li>[ ] Calibration completed (one-time)</li> <li>[ ] Test recording from both cameras simultaneously</li> </ul>"},{"location":"guides/camera-setup/#recording-settings-both-setups","title":"Recording Settings (Both Setups)","text":""},{"location":"guides/camera-setup/#video-specifications","title":"Video Specifications","text":"Setting Requirement Recommendation Reason Resolution 1080p minimum 1080p (1920x1080) Higher resolution improves MediaPipe accuracy Frame Rate 30 fps minimum 60 fps Better for short ground contact times (150-250ms) Orientation Landscape only Landscape Wider field of view for jumping movement Format MP4, MOV, AVI MP4 (H.264) Universal compatibility Bitrate Higher better Auto or 50+ Mbps Preserves detail during motion"},{"location":"guides/camera-setup/#why-60-fps-vs-30-fps","title":"Why 60 fps vs 30 fps?","text":"<p>For drop jumps and CMJ:</p> Metric 30 fps 60 fps Temporal resolution 33.3ms per frame 16.7ms per frame Ground contact sampling 5-8 frames 10-15 frames Time measurement error \u00b133ms \u00b116ms Velocity accuracy Good Better <p>Ground contact times in drop jumps: 150-250ms</p> <ul> <li>At 30 fps: Only 5-8 samples during contact</li> <li>At 60 fps: 10-15 samples during contact (2x better)</li> </ul> <p>Recommendation: Use 60 fps if your iPhone supports it. The accuracy improvement justifies the larger file size.</p>"},{"location":"guides/camera-setup/#iphone-camera-settings","title":"iPhone Camera Settings","text":"<p>How to set up iPhone for optimal recording:</p> <ol> <li>Open Camera app</li> <li>Settings \u2192 Camera \u2192 Record Video</li> <li>Select: 1080p at 60 fps (or 30 fps if 60 not available)</li> <li>Settings \u2192 Camera \u2192 Formats</li> <li>Select: Most Compatible (H.264, not HEVC)</li> <li>Before recording:</li> <li>Lock focus: Tap and hold on athlete until \"AE/AF Lock\" appears</li> <li>Lock exposure: Swipe up/down to adjust brightness, then keep locked</li> <li>Frame composition:</li> <li>Position athlete in center</li> <li>Ensure full body visible with margins</li> <li>Start recording before athlete begins jump sequence</li> </ol> <p>ProTip: Record a test video first and verify:</p> <ul> <li>Athlete stays in frame</li> <li>Focus remains sharp</li> <li>Lighting is adequate</li> <li>No motion blur</li> </ul>"},{"location":"guides/camera-setup/#lighting-guidelines","title":"Lighting Guidelines","text":""},{"location":"guides/camera-setup/#indoor-recording","title":"Indoor Recording","text":"<p>Recommended:</p> <ul> <li>Overhead gym lights (typical 400-800 lux sufficient)</li> <li>Even lighting across jumping area</li> <li>Avoid creating athlete shadow on background</li> </ul> <p>Check:</p> <ul> <li>Athlete's face and joints clearly visible</li> <li>No harsh shadows on body</li> <li>No bright spots (windows, reflective surfaces)</li> </ul>"},{"location":"guides/camera-setup/#outdoor-recording","title":"Outdoor Recording","text":"<p>Best conditions:</p> <ul> <li>Overcast day (soft, even lighting)</li> <li>Avoid midday sun (harsh shadows)</li> <li>Avoid late afternoon (low angle, long shadows)</li> </ul> <p>Positioning:</p> <ul> <li>Sun behind or to side of cameras</li> <li>Athlete not backlit (silhouette)</li> <li>Consider time of day for consistent lighting</li> </ul>"},{"location":"guides/camera-setup/#background-guidelines","title":"Background Guidelines","text":"<p>Optimal background:</p> <ul> <li>Plain wall (neutral color)</li> <li>Contrasting with athlete's clothing</li> <li>No patterns or busy elements</li> <li>Static (no movement)</li> </ul> <p>Color contrast examples:</p> <ul> <li>Athlete in dark clothing \u2192 light background (white/gray wall)</li> <li>Athlete in light clothing \u2192 dark background (blue/gray wall)</li> <li>Avoid: Athlete in white \u2192 white background (low contrast)</li> </ul> <p>Why it matters: MediaPipe separates figure from background. High contrast improves landmark detection accuracy and reduces false positives.</p>"},{"location":"guides/camera-setup/#common-mistakes-to-avoid","title":"Common Mistakes to Avoid","text":""},{"location":"guides/camera-setup/#camera-not-at-45-angle","title":"\u274c Camera Not at 45\u00b0 Angle","text":"<pre><code>\u274c INCORRECT: Pure lateral (90\u00b0)\n         [Athlete]\n             |\n             |\n    [Camera]\u2190\u2518\n\n\u274c INCORRECT: Pure frontal (0\u00b0)\n    [Camera]\n       \u2193\n    [Athlete]\n\n\u2705 CORRECT: 45\u00b0 angle\n         [Athlete]\n             \u2198\n              \u2198 45\u00b0\n            [Camera]\n</code></pre> <p>Problem with lateral: High occlusion, low ankle/knee visibility Problem with frontal: Depth ambiguity, jump height measurement poor Solution: Use 45\u00b0 angle as specified</p>"},{"location":"guides/camera-setup/#camera-too-close-3m","title":"\u274c Camera Too Close (\\&lt;3m)","text":"<p>Problems:</p> <ul> <li>Perspective distortion (wide-angle effect)</li> <li>Risk of athlete moving out of frame</li> <li>Lens distortion at edges (curved lines)</li> </ul> <p>Solution: Maintain 3-5m distance</p>"},{"location":"guides/camera-setup/#camera-too-high-or-too-low","title":"\u274c Camera Too High or Too Low","text":"<pre><code>\u274c Too high (looking down):\n    [Camera]\n       \u2193 \u2198\n         [Athlete]\n\n\u274c Too low (looking up):\n         [Athlete]\n       \u2197 \u2191\n    [Camera]\n\n\u2705 Correct (hip level):\n    [Camera] \u2192 [Athlete]\n</code></pre> <p>Problem: Parallax error, distorted proportions Solution: Camera lens at hip height (130-150cm)</p>"},{"location":"guides/camera-setup/#poor-framing","title":"\u274c Poor Framing","text":"<p>Common mistakes:</p> <ul> <li>Athlete too small in frame (camera too far)</li> <li>Athlete cut off during jump (camera too close or low)</li> <li>Not centered (athlete drifts out of frame)</li> </ul> <p>Solution:</p> <ul> <li>Test recording first</li> <li>Adjust framing to include full jump with margins</li> <li>Mark jumping position to ensure consistency</li> </ul>"},{"location":"guides/camera-setup/#inconsistent-settings-between-dual-cameras","title":"\u274c Inconsistent Settings Between Dual Cameras","text":"<p>For stereo setup only:</p> <p>Problems:</p> <ul> <li>Different frame rates \u2192 sync impossible</li> <li>Different resolutions \u2192 triangulation fails</li> <li>Different exposure \u2192 landmark detection inconsistent</li> </ul> <p>Solution: Configure both iPhones identically (see Dual Camera Checklist)</p>"},{"location":"guides/camera-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/camera-setup/#poor-landmark-visibility-warning","title":"\"Poor landmark visibility\" Warning","text":"<p>Symptoms: Kinemotion reports low visibility scores</p> <p>Causes:</p> <ul> <li>Insufficient lighting</li> <li>Low contrast with background</li> <li>Camera out of focus</li> <li>Motion blur (shutter speed too slow)</li> </ul> <p>Solutions:</p> <ol> <li>Add lighting sources</li> <li>Change background or athlete clothing for contrast</li> <li>Lock focus on athlete (tap and hold)</li> <li>Increase shutter speed (reduce exposure if needed)</li> <li>Ensure 1080p resolution</li> </ol>"},{"location":"guides/camera-setup/#jump-height-seems-incorrect","title":"Jump Height Seems Incorrect","text":"<p>Possible causes:</p> <ol> <li>Camera angle not optimal (measurement error)</li> <li>Athlete moving horizontally (drift during jump)</li> <li>Camera not level (tilted)</li> <li>Poor video quality affecting tracking</li> </ol> <p>Solutions:</p> <ol> <li>Verify camera angle with measuring app or protractor</li> <li>Coach athlete to jump straight up (minimal drift)</li> <li>Use tripod level indicator or phone level app</li> <li>Use <code>--quality accurate</code> for best results with good videos</li> </ol>"},{"location":"guides/camera-setup/#no-drop-jump-detected-error","title":"\"No Drop Jump Detected\" Error","text":"<p>Possible causes:</p> <ol> <li>Video doesn't include complete sequence</li> <li>Athlete cut off in framing</li> <li>Very poor tracking quality</li> </ol> <p>Solutions:</p> <ol> <li>Start recording before athlete steps on box</li> <li>Adjust framing - test with practice jump</li> <li>Improve video quality (lighting, focus, resolution)</li> <li>Use manual <code>--drop-start-frame</code> flag if auto-detection fails</li> </ol>"},{"location":"guides/camera-setup/#dual-camera-videos-not-synchronized","title":"Dual Camera: Videos Not Synchronized","text":"<p>Symptoms: Triangulation fails or produces unrealistic 3D poses</p> <p>Solutions:</p> <ol> <li>Verify both videos have identical frame rates</li> <li>Use audio/visual cue to sync (clap, beep)</li> <li>Use Pose2Sim synchronization module</li> <li>Consider hardware trigger for future recordings</li> </ol>"},{"location":"guides/camera-setup/#equipment-recommendations","title":"Equipment Recommendations","text":""},{"location":"guides/camera-setup/#single-camera-setup","title":"Single Camera Setup","text":"<p>Budget Option ($100-300):</p> <ul> <li>iPhone SE (2020 or later) or Android flagship</li> <li>Basic tripod with smartphone mount ($20-50)</li> <li>Total: ~$150-350</li> </ul> <p>Mid-Range ($500-800):</p> <ul> <li>Recent iPhone (11 or later) with 4K/60fps</li> <li>Quality tripod with fluid head ($100-200)</li> <li>Total: ~$600-1000</li> </ul> <p>What you need:</p> <ul> <li>iPhone capable of 1080p @ 60fps minimum</li> <li>Stable tripod (lightweight OK for indoor use)</li> <li>Level indicator (most tripods have bubble level)</li> </ul>"},{"location":"guides/camera-setup/#dual-camera-setup","title":"Dual Camera Setup","text":"<p>Budget Stereo ($300-600):</p> <ul> <li>2x iPhone SE or similar</li> <li>2x basic tripods</li> <li>Calibration board (print and mount, \\&lt;$20)</li> <li>Total: ~$350-650</li> </ul> <p>Mid-Range Stereo ($1000-1600):</p> <ul> <li>2x Recent iPhone (same model)</li> <li>2x quality tripods</li> <li>Professional calibration board</li> <li>Optional: Hardware sync trigger</li> <li>Total: ~$1200-1800</li> </ul> <p>What you need:</p> <ul> <li>2 iPhones (same model strongly recommended)</li> <li>2 stable tripods (identical height adjustment)</li> <li>Calibration pattern (ChArUco or checkerboard)</li> <li>Processing capability (laptop/desktop for Pose2Sim)</li> </ul> <p>Cost comparison to research-grade systems:</p> <ul> <li>Marker-based MoCap (Vicon, Qualisys): $50,000-$500,000</li> <li>Commercial markerless (Theia3D): $5,000-$20,000</li> <li>Dual iPhone + Pose2Sim: $300-$1,800 (100x cheaper!)</li> </ul>"},{"location":"guides/camera-setup/#validation-and-quality-checks","title":"Validation and Quality Checks","text":""},{"location":"guides/camera-setup/#after-recording","title":"After Recording","text":"<p>For every video, verify:</p> <ol> <li> <p>Playback check:</p> </li> <li> <p>Full jump sequence captured</p> </li> <li>Athlete stays in frame</li> <li>Focus sharp throughout</li> <li> <p>No motion blur</p> </li> <li> <p>Quality metrics:</p> </li> <li> <p>File size appropriate (60fps 1080p \u2248 200MB/min)</p> </li> <li>No dropped frames (smooth playback)</li> <li> <p>Audio clear (if using for sync)</p> </li> <li> <p>Test analysis:</p> </li> <li> <p>Run kinemotion on video</p> </li> <li>Check debug overlay output</li> <li>Verify landmark detection quality</li> </ol>"},{"location":"guides/camera-setup/#quality-indicators","title":"Quality Indicators","text":"<p>Good quality video (ready for analysis):</p> <ul> <li>\u2705 MediaPipe visibility scores &gt;0.5 average</li> <li>\u2705 Smooth landmark tracking (minimal jitter)</li> <li>\u2705 All jump phases detected automatically</li> <li>\u2705 Debug overlay shows consistent tracking</li> </ul> <p>Poor quality video (re-record recommended):</p> <ul> <li>\u274c Visibility scores \\&lt;0.3 average</li> <li>\u274c Jumpy landmark positions (tracking loss)</li> <li>\u274c Failed phase detection</li> <li>\u274c Debug overlay shows gaps or unrealistic poses</li> </ul>"},{"location":"guides/camera-setup/#advanced-tips","title":"Advanced Tips","text":""},{"location":"guides/camera-setup/#for-consistent-multi-session-recording","title":"For Consistent Multi-Session Recording","text":"<p>Create a standardized setup:</p> <ol> <li> <p>Mark camera positions on floor with tape</p> </li> <li> <p>Measure 45\u00b0 angle precisely</p> </li> <li>Mark 4m distance circle</li> <li> <p>Label \"Camera 1\" and \"Camera 2\" positions</p> </li> <li> <p>Document your setup:</p> </li> <li> <p>Take photos of camera positions</p> </li> <li>Note tripod height settings</li> <li> <p>Save camera settings screenshot</p> </li> <li> <p>Use same equipment across sessions</p> </li> <li> <p>Same iPhone(s)</p> </li> <li>Same tripod height</li> <li>Same room/location if possible</li> </ol> <p>Benefits:</p> <ul> <li>Consistent measurements across time</li> <li>Easier to compare athlete progress</li> <li>Simplified setup for each session</li> </ul>"},{"location":"guides/camera-setup/#optimizing-for-different-jump-types","title":"Optimizing for Different Jump Types","text":"<p>Drop Jump specific:</p> <ul> <li>Ensure drop box visible in frame (important for context)</li> <li>Capture pre-drop standing phase</li> <li>Need to see ground contact clearly</li> </ul> <p>CMJ specific:</p> <ul> <li>Start with athlete already in frame (no drop box)</li> <li>Capture countermovement phase (downward motion)</li> <li>Need full range of motion (lowest point to peak)</li> </ul> <p>Both:</p> <ul> <li>60 fps beneficial for fast movements</li> <li>Hip-level camera height optimal</li> <li>45\u00b0 angle works for both jump types</li> </ul>"},{"location":"guides/camera-setup/#research-background","title":"Research Background","text":""},{"location":"guides/camera-setup/#why-these-recommendations","title":"Why These Recommendations?","text":"<p>Camera angle (45\u00b0):</p> <ul> <li>Baldinger et al. (2025) showed camera viewing angle significantly affects joint angle validity</li> <li>45\u00b0 reduces occlusion while maintaining sagittal plane visibility</li> <li>Compromise between frontal (high visibility) and lateral (pure sagittal)</li> </ul> <p>Dual camera 90\u00b0 separation:</p> <ul> <li>Pagnon et al. (2022): Tested multiple angles, found 90\u00b0 optimal for 3D triangulation</li> <li>Dill et al. (2024): Validated stereo MediaPipe at 30.1mm RMSE with 90\u00b0 setup</li> <li>Balance between wide baseline (accuracy) and overlapping views (matching)</li> </ul> <p>1080p @ 60fps:</p> <ul> <li>Higher resolution improves MediaPipe landmark detection</li> <li>60 fps necessary for accurate temporal events (ground contact)</li> <li>Validated in multiple studies as sufficient for biomechanics</li> </ul>"},{"location":"guides/camera-setup/#limitations-of-single-camera","title":"Limitations of Single Camera","text":"<p>What single camera (45\u00b0) CANNOT provide:</p> <ul> <li>Research-grade accuracy (limited to ~8-12\u00b0 joint angle errors)</li> <li>Accurate depth/3D coordinates (z-axis noisy)</li> <li>Biomechanical constraints (no skeletal model)</li> <li>Validation against gold-standard (needs multi-camera)</li> </ul> <p>What single camera (45\u00b0) CAN provide:</p> <ul> <li>Training and assessment quality measurements</li> <li>Relative comparisons (same athlete over time)</li> <li>Drop jump key metrics (contact time, flight time, RSI)</li> <li>CMJ metrics (jump height, countermovement depth)</li> </ul> <p>For research-grade accuracy: Use dual camera stereo setup with Pose2Sim or OpenCap.</p>"},{"location":"guides/camera-setup/#summary","title":"Summary","text":""},{"location":"guides/camera-setup/#single-iphone-at-45-standard-setup","title":"Single iPhone at 45\u00b0 (Standard Setup)","text":"<p>Quick setup:</p> <ol> <li>Position camera 45\u00b0 from athlete's sagittal plane</li> <li>4 meters distance, hip height (130-150cm)</li> <li>1080p @ 60 fps, landscape, locked focus/exposure</li> <li>Frame full body with 10-15% margins</li> <li>Even lighting, plain background</li> <li>Record full jump sequence</li> </ol> <p>Expected accuracy: Good for training/assessment (~8-12\u00b0 joint angles)</p>"},{"location":"guides/camera-setup/#dual-iphone-stereo-advanced-setup","title":"Dual iPhone Stereo (Advanced Setup)","text":"<p>Quick setup:</p> <ol> <li>Position Camera 1 at +45\u00b0 (right), Camera 2 at -45\u00b0 (left)</li> <li>Both 4m distance, both hip height, 90\u00b0 separation</li> <li>Identical settings: 1080p @ 60fps</li> <li>Calibrate with ChArUco/checkerboard pattern</li> <li>Sync with clap or audio cue</li> <li>Process with Pose2Sim for 3D reconstruction</li> </ol> <p>Expected accuracy: Research-grade (~5-7\u00b0 joint angles, 30mm RMSE)</p>"},{"location":"guides/camera-setup/#decision-guide","title":"Decision Guide","text":"<p>Use single camera if:</p> <ul> <li>Training/coaching applications</li> <li>Assessing relative improvements</li> <li>Budget/equipment constraints</li> <li>Simplicity prioritized</li> </ul> <p>Use dual camera if:</p> <ul> <li>Research applications</li> <li>Elite athlete assessment</li> <li>Accurate 3D kinematics needed</li> <li>Publishing or validation required</li> </ul>"},{"location":"guides/camera-setup/#related-documentation","title":"Related Documentation","text":"<ul> <li>Versi\u00f3n en Espa\u00f1ol - Spanish version of this guide</li> <li>Sports Biomechanics Pose Estimation - Comprehensive research on pose systems</li> <li>Pose Systems Quick Reference - System comparison guide</li> <li>CLI Parameters Guide - Analysis parameters</li> <li>CMJ Guide - Counter-movement jump specifics</li> <li>CLAUDE.md - Complete project documentation (GitHub)</li> </ul>"},{"location":"guides/camera-setup/#references","title":"References","text":"<p>Camera angle research:</p> <ul> <li>Baldinger, M., Reimer, L. M., &amp; Senner, V. (2025). Influence of the Camera Viewing Angle on OpenPose Validity in Motion Analysis. Sensors, 25(3), 799. https://doi.org/10.3390/s25030799</li> </ul> <p>Stereo camera validation:</p> <ul> <li>Dill, S., et al. (2024). Accuracy Evaluation of 3D Pose Reconstruction Algorithms Through Stereo Camera Information Fusion for Physical Exercises with MediaPipe Pose. Sensors, 24(23), 7772. https://doi.org/10.3390/s24237772</li> </ul> <p>Optimal camera separation:</p> <ul> <li>Pagnon, D., Domalain, M., &amp; Reveret, L. (2022). Pose2Sim: An End-to-End Workflow for 3D Markerless Sports Kinematics\u2014Part 2: Accuracy. Sensors, 22(7), 2712. https://doi.org/10.3390/s22072712</li> </ul> <p>For complete bibliography, see sports-biomechanics-pose-estimation.md.</p> <p>Last Updated: November 6, 2025</p>"},{"location":"guides/cmj-guide/","title":"Counter Movement Jump (CMJ) Analysis Guide","text":""},{"location":"guides/cmj-guide/#overview","title":"Overview","text":"<p>The CMJ (Counter Movement Jump) analysis module provides comprehensive biomechanical analysis of counter movement jumps performed at floor level. Unlike drop jumps which start from an elevated platform, CMJs begin with the athlete standing on the ground, performing a countermovement (downward squat), and then jumping upward.</p>"},{"location":"guides/cmj-guide/#quick-start","title":"Quick Start","text":""},{"location":"guides/cmj-guide/#basic-analysis","title":"Basic Analysis","text":"<pre><code># Simple analysis (JSON to stdout)\nkinemotion cmj-analyze video.mp4\n\n# With debug video overlay (includes triple extension)\nkinemotion cmj-analyze video.mp4 --output debug.mp4 --json-output results.json\n</code></pre>"},{"location":"guides/cmj-guide/#python-api","title":"Python API","text":"<pre><code>from kinemotion import process_cmj_video\n\n# Analyze CMJ video\nmetrics = process_cmj_video(\n    \"athlete_cmj.mp4\",\n    quality=\"balanced\",\n    output_video=\"debug.mp4\",\n    verbose=True\n)\n\nprint(f\"Jump height: {metrics.jump_height:.3f}m\")\nprint(f\"Countermovement depth: {metrics.countermovement_depth:.3f}m\")\nprint(f\"Eccentric duration: {metrics.eccentric_duration*1000:.0f}ms\")\n</code></pre>"},{"location":"guides/cmj-guide/#why-cmj-is-different-from-drop-jumps","title":"Why CMJ is Different from Drop Jumps","text":"Aspect Drop Jump CMJ Starting Position Elevated box (0.3-0.6m) Floor level Drop Height Required parameter Not applicable Key Phases Drop \u2192 Contact \u2192 Jump Standing \u2192 Eccentric \u2192 Concentric \u2192 Flight Primary Focus Reactive strength index Neuromuscular coordination Key Metric Ground contact time Jump height from flight time Tracking Method Foot tracking Foot tracking Use Case Plyometric performance Power output, jump height"},{"location":"guides/cmj-guide/#cmj-specific-metrics","title":"CMJ-Specific Metrics","text":""},{"location":"guides/cmj-guide/#performance-metrics","title":"Performance Metrics","text":"<ol> <li> <p>Jump Height (m) - Maximum vertical displacement calculated from flight time</p> </li> <li> <p>Formula: h = (g \u00d7 t\u00b2) / 8</p> </li> <li> <p>Typical range: 0.20-0.60m for athletes</p> </li> <li> <p>Flight Time (s) - Time spent airborne</p> </li> <li> <p>Typical range: 300-600ms</p> </li> </ol>"},{"location":"guides/cmj-guide/#movement-characteristics","title":"Movement Characteristics","text":"<ol> <li> <p>Countermovement Depth (m) - Vertical distance during eccentric phase</p> </li> <li> <p>Represents how deep the athlete squats</p> </li> <li>Typical range: 0.20-0.40m</li> <li> <p>Deeper \u2260 always better (optimal depth varies by athlete)</p> </li> <li> <p>Eccentric Duration (s) - Time from countermovement start to lowest point</p> </li> <li> <p>Downward phase duration</p> </li> <li> <p>Typical range: 300-800ms</p> </li> <li> <p>Concentric Duration (s) - Time from lowest point to takeoff</p> </li> <li> <p>Upward phase duration</p> </li> <li> <p>Typical range: 200-500ms</p> </li> <li> <p>Total Movement Time (s) - Full movement from countermovement to takeoff</p> </li> <li> <p>Sum of eccentric + concentric durations</p> </li> <li> <p>Typical range: 500-1200ms</p> </li> <li> <p>Transition Time (s) - Duration at lowest point (amortization phase)</p> </li> <li> <p>Brief pause at countermovement bottom</p> </li> <li>Shorter = better stretch-shortening cycle utilization</li> <li>Typical range: 50-150ms</li> </ol>"},{"location":"guides/cmj-guide/#velocity-profile","title":"Velocity Profile","text":"<ol> <li> <p>Peak Eccentric Velocity (m/s) - Maximum downward speed</p> </li> <li> <p>Indicates countermovement speed</p> </li> <li> <p>Typical range: 0.5-1.5 m/s</p> </li> <li> <p>Peak Concentric Velocity (m/s) - Maximum upward speed</p> </li> <li> <p>Indicates propulsion force</p> </li> <li>Typical range: 1.5-3.0 m/s</li> </ol>"},{"location":"guides/cmj-guide/#triple-extension-in-debug-video","title":"Triple Extension (in debug video)","text":"<ol> <li>Ankle Angle - Dorsiflexion/plantarflexion</li> <li>Knee Angle - Flexion/extension</li> <li>Hip Angle - Flexion/extension</li> <li>Trunk Tilt - Forward/backward lean</li> </ol> <p>Note: Ankle/knee angles have limited visibility in lateral view videos (~20-30% of frames). Trunk angle is available throughout. See docs/TRIPLE_EXTENSION.md for details.</p>"},{"location":"guides/cmj-guide/#cmj-phases-explained","title":"CMJ Phases Explained","text":""},{"location":"guides/cmj-guide/#1-standing-phase-optional","title":"1. Standing Phase (Optional)","text":"<ul> <li>Duration: Variable (1-3 seconds typical)</li> <li>Characteristics: Near-zero velocity, stable position</li> <li>Detection: Velocity \\&lt; 0.01 normalized units</li> <li>Note: May not be present if athlete starts moving immediately</li> </ul>"},{"location":"guides/cmj-guide/#2-eccentric-phase-countermovement","title":"2. Eccentric Phase (Countermovement)","text":"<ul> <li>Duration: 300-800ms typical</li> <li>Characteristics: Downward motion, positive velocity</li> <li>Detection: Velocity crosses countermovement threshold (+0.015 @ 30fps)</li> <li>Purpose: Store elastic energy in muscles and tendons</li> <li>Color in Debug Video: \ud83d\udfe0 Orange</li> </ul>"},{"location":"guides/cmj-guide/#3-transition-phase-amortization","title":"3. Transition Phase (Amortization)","text":"<ul> <li>Duration: 50-150ms typical</li> <li>Characteristics: Velocity near zero at lowest point</li> <li>Detection: Velocity crosses from positive to negative</li> <li>Purpose: Brief coupling between eccentric and concentric phases</li> <li>Importance: Shorter = better stretch-shortening cycle</li> <li>Color in Debug Video: \ud83d\udfe3 Purple</li> </ul>"},{"location":"guides/cmj-guide/#4-concentric-phase-propulsion","title":"4. Concentric Phase (Propulsion)","text":"<ul> <li>Duration: 200-500ms typical</li> <li>Characteristics: Upward motion, negative velocity</li> <li>Detection: From lowest point until takeoff</li> <li>Purpose: Generate upward force for jump</li> <li>Color in Debug Video: \ud83d\udfe2 Green</li> </ul>"},{"location":"guides/cmj-guide/#5-flight-phase","title":"5. Flight Phase","text":"<ul> <li>Duration: 300-600ms typical</li> <li>Characteristics: Airborne, no ground contact</li> <li>Detection: Peak negative velocity + acceleration analysis</li> <li>Color in Debug Video: \ud83d\udd34 Red</li> </ul>"},{"location":"guides/cmj-guide/#6-landing-phase","title":"6. Landing Phase","text":"<ul> <li>Duration: Brief (1-2 frames)</li> <li>Characteristics: Impact deceleration</li> <li>Detection: Acceleration spike detection</li> <li>Color in Debug Video: \u26aa White</li> </ul>"},{"location":"guides/cmj-guide/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/cmj-guide/#intelligent-auto-tuning","title":"Intelligent Auto-Tuning","text":"<p>Parameters automatically adjust based on:</p> <ul> <li>Frame Rate: Higher FPS \u2192 adjusted thresholds</li> <li><code>countermovement_threshold = 0.015 \u00d7 (30 / fps)</code></li> <li><code>min_contact_frames = round(3 \u00d7 (fps / 30))</code></li> <li>Tracking Quality: Lower visibility \u2192 more smoothing</li> <li>Quality Preset: fast/balanced/accurate</li> </ul>"},{"location":"guides/cmj-guide/#quality-presets","title":"Quality Presets","text":"<p>Fast - Quick analysis (~50% faster)</p> <ul> <li>Lower confidence thresholds</li> <li>Less smoothing</li> <li>Good for batch processing</li> </ul> <p>Balanced (Default) - Best for most cases</p> <ul> <li>Optimal accuracy/speed tradeoff</li> <li>Recommended for general use</li> </ul> <p>Accurate - Research-grade</p> <ul> <li>Higher confidence thresholds</li> <li>More aggressive smoothing</li> <li>Best for publication-quality data</li> </ul>"},{"location":"guides/cmj-guide/#sub-frame-precision","title":"Sub-Frame Precision","text":"<ul> <li>Interpolation: Linear interpolation of smooth velocity curves</li> <li>Accuracy: \u00b110ms at 30fps (vs \u00b133ms without interpolation)</li> <li>Method: Savitzky-Golay derivative-based velocity calculation</li> <li>Improvement: 60-70% reduction in timing error</li> </ul>"},{"location":"guides/cmj-guide/#backward-search-algorithm","title":"Backward Search Algorithm","text":"<p>The CMJ algorithm works backward from peak height for robust detection:</p> <ol> <li>Find peak height (global minimum position)</li> <li>Find takeoff (peak upward velocity before peak)</li> <li>Find lowest point (maximum position before takeoff)</li> <li>Find landing (acceleration spike after peak)</li> </ol> <p>Why this is better:</p> <ul> <li>Peak height is unambiguous</li> <li>All events relative to peak (robust to artifacts)</li> <li>Avoids false detections from video start/end</li> <li>Matches physical reality</li> </ul>"},{"location":"guides/cmj-guide/#cli-reference","title":"CLI Reference","text":""},{"location":"guides/cmj-guide/#basic-options","title":"Basic Options","text":"<pre><code>kinemotion cmj-analyze VIDEO_PATH [OPTIONS]\n</code></pre> <p>Required:</p> <ul> <li><code>VIDEO_PATH</code> - Path(s) to video file(s), supports glob patterns</li> </ul> <p>Recommended:</p> <ul> <li><code>--output PATH</code> - Generate debug video with triple extension visualization</li> <li><code>--json-output PATH</code> - Save metrics to JSON file</li> </ul> <p>Quality:</p> <ul> <li><code>--quality [fast|balanced|accurate]</code> - Analysis preset (default: balanced)</li> <li><code>--verbose</code> - Show auto-selected parameters</li> </ul>"},{"location":"guides/cmj-guide/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple videos\nkinemotion cmj-analyze videos/*.mp4 --batch --workers 4\n\n# With output directories\nkinemotion cmj-analyze videos/*.mp4 --batch \\\n  --json-output-dir results/ \\\n  --output-dir debug_videos/ \\\n  --csv-summary summary.csv\n</code></pre>"},{"location":"guides/cmj-guide/#expert-mode","title":"Expert Mode","text":"<p>Override auto-tuned parameters (rarely needed):</p> <pre><code>kinemotion cmj-analyze video.mp4 \\\n  --countermovement-threshold 0.012 \\\n  --velocity-threshold 0.015 \\\n  --smoothing-window 7\n</code></pre> <p>Expert Parameters:</p> <ul> <li><code>--smoothing-window</code> - Savitzky-Golay window size</li> <li><code>--velocity-threshold</code> - Flight detection threshold (unused, kept for API)</li> <li><code>--countermovement-threshold</code> - Eccentric phase threshold (positive value)</li> <li><code>--min-contact-frames</code> - Minimum frames for valid phases</li> <li><code>--visibility-threshold</code> - Landmark confidence threshold</li> <li><code>--detection-confidence</code> - MediaPipe pose detection confidence</li> <li><code>--tracking-confidence</code> - MediaPipe pose tracking confidence</li> </ul>"},{"location":"guides/cmj-guide/#python-api-reference","title":"Python API Reference","text":""},{"location":"guides/cmj-guide/#single-video-processing","title":"Single Video Processing","text":"<pre><code>from kinemotion import process_cmj_video, CMJMetrics\n\nmetrics: CMJMetrics = process_cmj_video(\n    video_path=\"athlete.mp4\",\n    quality=\"balanced\",           # \"fast\", \"balanced\", or \"accurate\"\n    output_video=\"debug.mp4\",     # Optional debug video with triple extension\n    json_output=\"results.json\",   # Optional JSON output\n    smoothing_window=None,        # Expert override\n    velocity_threshold=None,      # Expert override\n    countermovement_threshold=None, # Expert override\n    min_contact_frames=None,      # Expert override\n    visibility_threshold=None,    # Expert override\n    detection_confidence=None,    # Expert override\n    tracking_confidence=None,     # Expert override\n    verbose=False                 # Print progress\n)\n\n# Access metrics\nprint(f\"Jump height: {metrics.jump_height:.3f}m\")\nprint(f\"Countermovement depth: {metrics.countermovement_depth:.3f}m\")\nprint(f\"Eccentric/Concentric ratio: {metrics.eccentric_duration / metrics.concentric_duration:.2f}\")\n</code></pre>"},{"location":"guides/cmj-guide/#bulk-processing","title":"Bulk Processing","text":"<pre><code>from kinemotion import CMJVideoConfig, process_cmj_videos_bulk\n\n# Configure multiple videos\nconfigs = [\n    CMJVideoConfig(\"video1.mp4\"),\n    CMJVideoConfig(\"video2.mp4\", quality=\"accurate\"),\n    CMJVideoConfig(\"video3.mp4\", output_video=\"debug3.mp4\"),\n]\n\n# Process in parallel\nresults = process_cmj_videos_bulk(configs, max_workers=4)\n\n# Handle results\nfor result in results:\n    if result.success:\n        m = result.metrics\n        print(f\"\u2713 {result.video_path}: {m.jump_height:.3f}m\")\n    else:\n        print(f\"\u2717 {result.video_path}: {result.error}\")\n</code></pre>"},{"location":"guides/cmj-guide/#camera-setup-for-cmj","title":"Camera Setup for CMJ","text":""},{"location":"guides/cmj-guide/#required-setup","title":"Required Setup","text":"<ol> <li>Lateral (Side) View - Camera perpendicular to sagittal plane (90\u00b0 angle)</li> <li>Distance - 3-5 meters from athlete (optimal: ~4m)</li> <li>Height - Camera at athlete's hip height (0.8-1.2m)</li> <li>Framing - Full body visible (head to feet) throughout jump</li> <li>Orientation - Landscape preferred (portrait works but less ideal)</li> <li>Stability - Tripod required (no hand-held)</li> <li>Frame Rate - 30+ fps minimum (60fps recommended)</li> <li>Resolution - 1080p or higher</li> </ol>"},{"location":"guides/cmj-guide/#why-lateral-view-is-required","title":"Why Lateral View is Required","text":"<ul> <li>CMJ is a vertical movement in the sagittal plane</li> <li>Direct measurement of vertical displacement</li> <li>Clear visualization of countermovement depth</li> <li>Accurate velocity calculations</li> <li>Validated in biomechanics research</li> </ul>"},{"location":"guides/cmj-guide/#front-view-will-not-work","title":"Front View Will Not Work","text":"<p>\u274c Do not use front view for CMJ:</p> <ul> <li>Cannot measure vertical motion accurately</li> <li>Parallax errors from forward/backward movement</li> <li>Y-coordinate confounded by distance changes</li> <li>Results will be unreliable</li> </ul> <p>See <code>docs/CAMERA_SETUP.md</code> for detailed setup guide.</p>"},{"location":"guides/cmj-guide/#interpreting-results","title":"Interpreting Results","text":""},{"location":"guides/cmj-guide/#jump-height","title":"Jump Height","text":"<ul> <li>Elite athletes: 0.40-0.60m</li> <li>Trained athletes: 0.30-0.45m</li> <li>Recreational: 0.20-0.35m</li> </ul>"},{"location":"guides/cmj-guide/#countermovement-depth","title":"Countermovement Depth","text":"<ul> <li>Too shallow (\\&lt; 0.15m): May not utilize full potential</li> <li>Optimal (0.20-0.35m): Good technique</li> <li>Too deep (&gt; 0.40m): May slow down transition</li> </ul>"},{"location":"guides/cmj-guide/#eccentricconcentric-ratio","title":"Eccentric/Concentric Ratio","text":"<ul> <li>\\&lt; 1.0: Very fast transition (good for power)</li> <li>1.0-2.0: Normal range (good technique)</li> <li>&gt; 2.0: Slow transition (may indicate fatigue)</li> </ul>"},{"location":"guides/cmj-guide/#transition-time","title":"Transition Time","text":"<ul> <li>\\&lt; 50ms: Excellent stretch-shortening cycle</li> <li>50-100ms: Good</li> <li>100-150ms: Average</li> <li>&gt; 150ms: May indicate technique issues</li> </ul>"},{"location":"guides/cmj-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/cmj-guide/#could-not-detect-cmj-phases","title":"\"Could not detect CMJ phases\"","text":"<p>Solutions:</p> <ul> <li>Verify video shows complete jump (from standing through landing)</li> <li>Use <code>--quality accurate</code> for better tracking</li> <li>Adjust <code>--countermovement-threshold</code> (try 0.010 or 0.020)</li> <li>Generate debug video to visually verify detection</li> </ul>"},{"location":"guides/cmj-guide/#unrealistic-jump-heights","title":"Unrealistic Jump Heights","text":"<p>Causes:</p> <ul> <li>Camera not level</li> <li>Athlete moves out of frame</li> <li>Poor tracking quality</li> </ul> <p>Solutions:</p> <ul> <li>Ensure camera is level and stable</li> <li>Use <code>--quality accurate</code></li> <li>Check that full body is visible in all frames</li> </ul>"},{"location":"guides/cmj-guide/#front-view-video-gives-wrong-results","title":"Front View Video Gives Wrong Results","text":"<p>Explanation: Front view cannot measure vertical displacement accurately due to parallax and foreshortening.</p> <p>Solution: Always use lateral (side) view as documented.</p>"},{"location":"guides/cmj-guide/#triple-extension-shows-na","title":"Triple Extension Shows \"N/A\"","text":"<p>Explanation: MediaPipe has difficulty detecting ankle/knee in pure lateral view.</p> <p>Solutions:</p> <ul> <li>Use slightly oblique camera angle (80-85\u00b0 instead of 90\u00b0)</li> <li>Higher resolution (1080p+)</li> <li>Trunk angle always available (100% visibility)</li> </ul> <p>See <code>docs/TRIPLE_EXTENSION.md</code> for detailed information on joint angle tracking.</p>"},{"location":"guides/cmj-guide/#validation","title":"Validation","text":"<p>The CMJ module has been validated with:</p> <p>\u2705 Real video testing: samples/cmjs/cmj.mp4 \u2705 Jump height: 50.6cm (\u00b11 frame = 33ms precision) \u2705 Frame detection: Takeoff 154 (known: 153), Landing 173 (known: 172) \u2705 Test coverage: 9 CMJ-specific tests, all passing \u2705 Integration: 70 total tests passing</p>"},{"location":"guides/cmj-guide/#references-further-reading","title":"References &amp; Further Reading","text":"<ol> <li> <p>Countermovement Jump Biomechanics</p> </li> <li> <p>Linthorne, N. P. (2001). Analysis of standing vertical jumps using a force platform</p> </li> <li> <p>Stretch-Shortening Cycle</p> </li> <li> <p>Komi, P. V. (2000). Stretch-shortening cycle: a powerful model to study muscle function</p> </li> <li> <p>Video-Based Motion Analysis</p> </li> <li> <p>Validated approach using flight time method (force plate standard)</p> </li> </ol> <p>Kinemotion CMJ Analysis Module - Version 0.1.0 Floor-level counter movement jump analysis with intelligent auto-tuning</p>"},{"location":"reference/parameters/","title":"Configuration Parameters Guide","text":"<p>\u26a0\ufe0f NOTICE: This document is mostly DEPRECATED as of November 2025.</p> <p>Kinemotion now features intelligent auto-tuning that automatically optimizes all parameters based on video characteristics (FPS, tracking quality). Most users no longer need to manually adjust parameters.</p> <p>For current usage, see:</p> <ul> <li>README.md - Simplified interface with auto-tuning</li> <li>CLAUDE.md - Auto-tuning system documentation</li> </ul> <p>This document is preserved for:</p> <ul> <li>Expert users who need to override auto-tuned values (use <code>--expert</code> mode)</li> <li>Understanding what each parameter does internally</li> <li>Debugging when auto-tuning doesn't work as expected</li> </ul>"},{"location":"reference/parameters/#quick-reference-auto-tuned-values","title":"Quick Reference (Auto-Tuned Values)","text":"<p>You don't need to set these manually! The tool auto-detects:</p> Parameter 30fps Auto 60fps Auto 120fps Auto Formula <code>velocity-threshold</code> 0.020 0.010 0.005 0.02 \u00d7 (30/fps) <code>min-contact-frames</code> 3 6 12 round(3 \u00d7 (fps/30)) <code>smoothing-window</code> 5 3 3 5 if fps\u226430 else 3 <code>outlier-rejection</code> \u2705 \u2705 \u2705 Always enabled <code>use-curvature</code> \u2705 \u2705 \u2705 Always enabled <code>polyorder</code> 2 2 2 Always 2 (optimal for jumps) <p>Quality adjustments (based on MediaPipe visibility):</p> <ul> <li>High quality (visibility &gt; 0.7): Minimal smoothing, no bilateral filter</li> <li>Medium quality (0.4-0.7): +1 smoothing adjustment, enable bilateral</li> <li>Low quality (\\&lt; 0.4): +2 smoothing adjustment, enable bilateral, lower confidence</li> </ul> <p>Use <code>--verbose</code> to see what was auto-selected for your video!</p>"},{"location":"reference/parameters/#batch-processing-parameters","title":"Batch Processing Parameters","text":"<p>These parameters control batch processing mode when analyzing multiple videos:</p> Parameter Type Default Description <code>--batch</code> flag auto Explicitly enable batch mode (auto-enabled with multiple files) <code>--workers</code> int 4 Number of parallel workers for ProcessPoolExecutor <code>--output-dir</code> path - Directory for debug videos (auto-named: <code>{video_name}_debug.mp4</code>) <code>--json-output-dir</code> path - Directory for JSON metrics (auto-named: <code>{video_name}.json</code>) <code>--csv-summary</code> path - Export aggregated results to CSV file <p>Usage:</p> <pre><code># Batch process with all outputs\nkinemotion dropjump-analyze videos/*.mp4 --batch \\\n  --workers 4 \\\n  --json-output-dir results/ \\\n  --output-dir debug_videos/ \\\n  --csv-summary summary.csv\n</code></pre> <p>Notes:</p> <ul> <li>Batch mode is automatically enabled when multiple video paths are provided</li> <li>All analysis parameters (quality, smoothing-window, etc.) apply to all videos in batch</li> <li>Progress is shown in real-time: <code>[3/10] \u2713 athlete3.mp4 (2.1s)</code></li> <li>Summary statistics are calculated across successful videos</li> <li>CSV includes all videos (successful and failed)</li> </ul> <p>Python API Alternative:</p> <p>For more control, use the Python API:</p> <pre><code>from kinemotion import DropJumpVideoConfig, process_dropjump_videos_bulk\n\nconfigs = [\n    DropJumpVideoConfig(\"video1.mp4\", quality=\"fast\"),\n    DropJumpVideoConfig(\"video2.mp4\", quality=\"accurate\"),  # Different settings per video\n]\n\nresults = process_dropjump_videos_bulk(configs, max_workers=4)\n</code></pre> <p>See <code>examples/bulk/README.md</code> for complete API documentation.</p>"},{"location":"reference/parameters/#legacy-manual-parameter-reference","title":"Legacy Manual Parameter Reference","text":"<p>\u26a0\ufe0f Important: kinemotion accuracy is currently unvalidated. These parameter recommendations are based on theoretical considerations and industry best practices, not empirically verified performance.</p> <p>This section explains each parameter for expert users who need manual control.</p> <p>Note: Drop jump analysis uses foot-based tracking. The <code>--use-com</code> and <code>--adaptive-threshold</code> features (available in <code>core/</code> modules) require longer videos (~5+ seconds) with a 3-second standing baseline, making them unsuitable for typical drop jump videos (~3 seconds total).</p> <p>Accuracy Disclaimer: All parameter tuning recommendations assume kinemotion provides accurate measurements. Actual accuracy performance is currently unknown and requires validation against gold standards (force plates, 3D motion capture).</p>"},{"location":"reference/parameters/#smoothing-parameters","title":"Smoothing Parameters","text":""},{"location":"reference/parameters/#-smoothing-window-default-5","title":"<code>--smoothing-window</code> (default: 5)","text":"<p>What it does: Controls the window size for the Savitzky-Golay filter that smooths landmark trajectories over time.</p> <p>How it works:</p> <ul> <li>Applies a polynomial smoothing filter across N consecutive frames</li> <li>Must be an odd number (3, 5, 7, 9, etc.)</li> <li>Larger window = smoother trajectories but less responsive to quick movements</li> <li>Smaller window = more responsive but potentially noisier</li> </ul> <p>Technical details:</p> <ul> <li>Uses polynomial order specified by <code>--polyorder</code> (default: 2, quadratic fit)</li> <li>Applied to x and y coordinates of all foot landmarks</li> <li>Smoothing happens AFTER all frames are tracked, not in real-time</li> </ul> <p>When to increase (7, 9, 11):</p> <ul> <li>Video has significant camera shake</li> <li>Tracking is jittery/noisy</li> <li>Athlete moves slowly (long ground contact times)</li> <li>Low-quality video or poor lighting</li> <li>False contact detections due to landmark jitter</li> </ul> <p>When to decrease (3):</p> <ul> <li>High-quality, stable video</li> <li>Very fast movements (reactive jumps)</li> <li>Need to capture brief contact phases</li> <li>High frame rate video (60+ fps)</li> </ul> <p>Example:</p> <pre><code># Noisy video with camera shake\nkinemotion dropjump-analyze video.mp4 --smoothing-window 9\n\n# High-quality 60fps video\nkinemotion dropjump-analyze video.mp4 --smoothing-window 3\n```text\n\n**Visual effect:**\n\n- Before smoothing: Foot position jumps between frames\n- After smoothing: Smooth trajectory curve through the jump\n\n---\n\n### `--polyorder` (default: 2)\n\n**What it does:**\nControls the polynomial order used in the Savitzky-Golay filter for smoothing and derivative calculations.\n\n**How it works:**\n\n- Fits a polynomial of order N to data points in the smoothing window\n- Order 2 (quadratic): y = a + bx + cx\u00b2 - fits parabolas\n- Order 3 (cubic): y = a + bx + cx\u00b2 + dx\u00b3 - fits S-curves\n- Order 4+ (quartic, quintic): captures more complex patterns\n- Higher order polynomials can fit more complex motion but are more sensitive to noise\n- Must satisfy: polyorder &lt; smoothing-window (e.g., polyorder=3 requires window\u22655)\n\n**Technical details:**\n\n- Applied to landmark smoothing, velocity calculation, and acceleration calculation\n- Affects all three: position smoothing, first derivative (velocity), second derivative (acceleration)\n- Jump motion is fundamentally parabolic (constant acceleration), so polyorder=2 is mathematically ideal\n- Higher orders useful when motion deviates from ideal parabola (e.g., athlete adjusting mid-air)\n- Same polyorder used throughout entire analysis pipeline for consistency\n\n**When to use polyorder=2 (quadratic, default):**\n\n- Most jump scenarios (motion follows gravity's parabola)\n- Noisy videos (lower orders more robust to noise)\n- Standard drop jumps and reactive jumps\n- When in doubt - this is the safest choice\n- **Recommended for 95% of use cases**\n\n**When to use polyorder=3 (cubic):**\n\n- High-quality studio videos with stable tracking\n- Complex motion patterns (athlete adjusting posture mid-flight)\n- Very smooth, low-noise tracking data\n- Research scenarios requiring maximum precision\n- When motion appears to deviate from simple parabola\n- Requires larger smoothing window (7+ recommended)\n\n**When to use polyorder=4+ (advanced):**\n\n- Rarely needed in practice\n- May overfit to noise rather than capture real motion\n- Only for special research cases with very high-quality data\n- Requires smoothing-window \u2265 polyorder + 2\n\n**Performance comparison:**\n\n```text\npolyorder=2 (typical):\n- Baseline performance for jump motion\n- Robust to noise and tracking errors\n- Ideal for parabolic trajectories\n\npolyorder=3 (advanced):\n- Theoretically better for complex motion (\u26a0\ufe0f unvalidated)\n- Better captures non-parabolic adjustments\n- More sensitive to noise\n- Requires high-quality video\n\npolyorder=4+ (expert):\n- Minimal practical benefit\n- Risk of overfitting to noise\n- Not recommended for general use\n```text\n\n**Examples:**\n\n```bash\n# Default: polyorder=2 (recommended for most cases)\nkinemotion dropjump-analyze video.mp4\n\n# High-quality video with complex motion\nkinemotion dropjump-analyze studio.mp4 \\\n  --polyorder 3 \\\n  --smoothing-window 7\n\n# Maximum accuracy setup\nkinemotion dropjump-analyze video.mp4 \\\n  --polyorder 3 \\\n  --smoothing-window 9\n```\n\n**Validation rules:**\n\n```bash\n# Valid combinations\n--smoothing-window 5 --polyorder 2  \u2713 (2 &lt; 5)\n--smoothing-window 7 --polyorder 3  \u2713 (3 &lt; 7)\n--smoothing-window 9 --polyorder 4  \u2713 (4 &lt; 9)\n\n# Invalid combinations\n--smoothing-window 5 --polyorder 5  \u2717 (5 \u226e 5)\n--smoothing-window 3 --polyorder 3  \u2717 (3 \u226e 3)\n```text\n\n**Physical interpretation:**\n\n```text\nGravity causes constant downward acceleration\n\u2192 Velocity changes linearly with time\n\u2192 Position follows quadratic (parabolic) path\n\u2192 polyorder=2 is theoretically optimal\n\nNon-ideal factors:\n\u2192 Air resistance (higher order needed)\n\u2192 Athlete adjustments mid-flight (higher order needed)\n\u2192 But these effects are usually small vs measurement noise\n\u2192 So polyorder=2 works best in practice\n```text\n\n**Troubleshooting:**\n\n- If smoothing seems too aggressive with polyorder=3:\n  - Reduce to polyorder=2\n  - Or increase smoothing-window\n- If validation error \"polyorder must be &lt; smoothing-window\":\n  - Increase smoothing-window (e.g., from 5 to 7)\n  - Or decrease polyorder\n- If results look noisier with polyorder=3:\n  - Video quality may not support higher order\n  - Revert to polyorder=2\n  - Or increase smoothing-window to compensate\n\n**Performance impact:**\n\n- Negligible computational difference between polyorder values\n- Same post-processing time regardless of order\n- No runtime performance reason to prefer lower orders\n- Choose based on accuracy/noise tradeoff only\n\n---\n\n## Advanced Filtering Parameters\n\n### `--outlier-rejection` / `--no-outlier-rejection` (default: --outlier-rejection)\n\n**What it does:**\nDetects and removes MediaPipe tracking glitches (outliers) before smoothing, replacing them with interpolated values.\n\n**How it works:**\n\n- Applies two complementary outlier detection methods:\n  1. **RANSAC-based polynomial fitting**: Fits a polynomial to sliding windows of data and identifies points that deviate significantly from the fit\n  2. **Median filtering**: Detects points that differ significantly from the local median\n- Outliers are replaced with linear interpolation from neighboring valid points\n- Applied to each landmark coordinate (x, y) independently\n- Runs BEFORE Savitzky-Golay smoothing in the processing pipeline\n\n**Technical details:**\n\n- RANSAC parameters:\n  - Window size: 15 frames\n  - Threshold: 0.02 (normalized coordinates)\n  - Min inliers: 70% of window must fit the model\n- Median filter parameters:\n  - Window size: 5 frames\n  - Threshold: 0.03 (normalized coordinates)\n- Combines both methods (marks as outlier if either detects it)\n- Interpolation method: Linear between nearest valid neighbors\n\n**When to use --outlier-rejection (default, recommended):**\n\n- All typical use cases (enabled by default for good reason)\n- Videos with occasional tracking glitches or jumps\n- Medium to low quality video\n- Camera shake or motion blur\n- Partially occluded landmarks\n- Athlete wearing loose clothing\n- Improves robustness across varying video quality\n\n**When to use --no-outlier-rejection:**\n\n- Debugging or testing raw MediaPipe output\n- Perfect tracking quality (rare in real-world videos)\n- Academic comparison studies\n- Performance-critical applications (saves ~5-10% processing time)\n- When you want to see unfiltered tracking errors\n\n**Example:**\n\n```bash\n# Standard usage (outlier rejection enabled by default)\nkinemotion dropjump-analyze video.mp4\n\n# Explicitly enable outlier rejection\nkinemotion dropjump-analyze video.mp4 --outlier-rejection\n\n# Disable for debugging\nkinemotion dropjump-analyze video.mp4 --no-outlier-rejection --output debug.mp4\n```text\n\n**Visual effect:**\n\n- Without outlier rejection: Occasional position \"jumps\" in debug video (landmark suddenly shifts 5-10cm then returns)\n- With outlier rejection: Smooth trajectory throughout the jump, glitches removed\n\n**Effect:**\n\n- Removes tracking glitches for more consistent measurements\n- Most beneficial for videos with tracking issues\n- Minimal effect on perfect tracking (no glitches to remove)\n\n**Common scenarios:**\n\n- **Scenario 1: Loose clothing**\n  - Problem: Ankle landmark occasionally jumps to clothing edge\n  - Solution: RANSAC detects deviation from smooth trajectory, replaces with interpolation\n- **Scenario 2: Motion blur**\n  - Problem: Landing causes blur, landmark position uncertainty\n  - Solution: Median filter catches brief spikes, smooths transition\n- **Scenario 3: Occlusion**\n  - Problem: One foot temporarily hidden behind other\n  - Solution: Both methods detect inconsistent position, use valid frames for interpolation\n\n**Troubleshooting:**\n\n- If trajectories look \"too smooth\" (missing real motion):\n  - Outlier rejection is not the cause (operates at 0.02-0.03 threshold, small deviations)\n  - Check --smoothing-window instead (probably too large)\n- If still seeing tracking glitches in output:\n  - Outlier rejection may be too conservative for your video\n  - This is rare; glitches might be in velocity calculation instead\n  - Try increasing --smoothing-window to further reduce noise\n- If metrics seem unrealistic:\n  - Outlier rejection is likely helping, not hurting\n  - Check other parameters (velocity-threshold, min-contact-frames)\n\n**Performance impact:**\n\n- Adds ~5-10% to processing time\n- O(n \u00d7 window_size) complexity per landmark\n- Negligible impact on overall analysis time (most time spent in MediaPipe tracking)\n\n---\n\n### `--bilateral-filter` / `--no-bilateral-filter` (default: --no-bilateral-filter)\n\n**What it does:**\nUses bilateral temporal filtering instead of Savitzky-Golay smoothing to preserve sharp transitions (landing/takeoff) while smoothing noise.\n\n**How it works:**\n\n- **Standard Savitzky-Golay** (default): Uniform smoothing across all frames\n  - Smooths based only on temporal distance\n  - Treats all frames equally regardless of motion\n  - May blur sharp transitions like landing impact\n\n- **Bilateral filtering** (--bilateral-filter): Edge-preserving smoothing\n  - Weights each neighbor by TWO factors:\n    1. **Spatial weight**: Temporal distance (like standard smoothing)\n    2. **Intensity weight**: Position similarity (preserves edges)\n  - Frames with similar positions get high weight (smooth together)\n  - Frames with different positions get low weight (preserve transition)\n  - Landing/takeoff edges remain sharp, noise in smooth regions is reduced\n\n**Technical details:**\n\n- Bilateral filter parameters:\n  - Window size: 9 frames (automatically adjusted to odd)\n  - Sigma spatial: 3.0 (controls temporal weighting)\n  - Sigma intensity: 0.02 (controls position difference weighting)\n- Replaces Savitzky-Golay smoothing when enabled (not additive)\n- Applied to landmark positions before velocity/acceleration calculation\n- More computationally expensive than Savitzky-Golay (~2x time)\n\n**Mathematical formulation:**\n\n```text\nFor each frame i:\n  For each neighbor j in window:\n    spatial_weight[j] = exp(-(i-j)\u00b2 / (2 \u00d7 sigma_spatial\u00b2))\n    intensity_weight[j] = exp(-(pos[j]-pos[i])\u00b2 / (2 \u00d7 sigma_intensity\u00b2))\n    combined_weight[j] = spatial_weight[j] \u00d7 intensity_weight[j]\n\n  smoothed_pos[i] = \u03a3(combined_weight[j] \u00d7 pos[j]) / \u03a3(combined_weight[j])\n```text\n\n**When to use --bilateral-filter:**\n\n- Videos with rapid state transitions (landing, takeoff)\n- High-quality video where preserving timing precision is critical\n- Research scenarios requiring maximum event timing accuracy\n- When Savitzky-Golay smoothing blurs important transitions\n- Drop jumps with very brief ground contact times\n- Reactive jumps with explosive movements\n\n**When to use --no-bilateral-filter (default):**\n\n- Most typical use cases\n- Standard video quality\n- When processing speed matters\n- Proven baseline method (Savitzky-Golay)\n- When results are already good with default settings\n- Lower-quality videos (bilateral may amplify noise)\n\n**Example:**\n\n```bash\n# Use bilateral filter for high-quality video\nkinemotion dropjump-analyze studio_video.mp4 \\\n  --bilateral-filter \\\n  --outlier-rejection \\\n  --output debug.mp4\n\n# Compare bilateral vs standard smoothing\nkinemotion dropjump-analyze video.mp4 --output standard.mp4\nkinemotion dropjump-analyze video.mp4 --bilateral-filter --output bilateral.mp4\n```text\n\n**Visual effect:**\n\n- Standard smoothing: Landing transition spread over 2-3 frames, smooth curve\n- Bilateral filtering: Landing transition sharp at 1-2 frames, preserves impact timing\n\n**Effect:**\n\n- Preserves timing precision for rapid transitions\n- Most beneficial for high-quality videos with sharp state changes\n- May amplify noise in low-quality videos\n\n**Trade-offs:**\n\n- **Advantages:**\n  - Preserves sharp transitions (landing, takeoff)\n  - More accurate event timing\n  - Better for rapid movements\n  - Physics-aware (respects motion discontinuities)\n\n- **Disadvantages:**\n  - Experimental feature (less tested than Savitzky-Golay)\n  - ~2x slower processing\n  - May preserve noise in low-quality videos\n  - More parameters to tune (sigma_spatial, sigma_intensity)\n  - Less predictable behavior across varying video quality\n\n**Interaction with other parameters:**\n\n- **Compatible with:**\n  - --outlier-rejection: Apply together for best results (outlier removal \u2192 bilateral smoothing)\n  - --use-curvature: Bilateral preserves transitions for accurate timing refinement\n\n- **Replaces:**\n  - --smoothing-window and --polyorder have no effect when bilateral filter is enabled\n  - Bilateral uses its own window size (9 frames) and weighting scheme\n\n**Common scenarios:**\n\n- **Scenario 1: Explosive reactive jump**\n  - Problem: Takeoff happens in &lt;2 frames, Savitzky-Golay smooths it to 4 frames\n  - Solution: Bilateral preserves sharp takeoff, accurate flight time measurement\n\n- **Scenario 2: Hard landing impact**\n  - Problem: Landing deceleration spread over 3 frames, timing imprecise\n  - Solution: Bilateral maintains sharp landing transition, better contact time\n\n- **Scenario 3: Noisy low-quality video**\n  - Problem: Bilateral amplifies frame-to-frame noise\n  - Solution: Use standard Savitzky-Golay instead (more robust to noise)\n\n**Troubleshooting:**\n\n- If results are noisier with --bilateral-filter:\n  - Video quality may not be high enough\n  - Revert to standard smoothing (--no-bilateral-filter)\n  - Or use --outlier-rejection to clean data first\n- If transitions still seem blurred:\n  - Bilateral sigma_intensity may be too large (currently 0.02)\n  - This is a fixed parameter; file an issue for configurability\n- If processing is too slow:\n  - Bilateral adds ~2x time to smoothing step\n  - Use standard Savitzky-Golay for faster processing\n\n**Performance impact:**\n\n- Adds ~50-100% to smoothing step time\n- Smoothing is ~10-20% of total pipeline\n- Overall impact: ~10-20% slower total processing\n- O(n \u00d7 window_size) complexity (same as Savitzky-Golay)\n- Slower per-frame due to exponential calculations\n\n**Recommendation:**\n\n- Start with default (--no-bilateral-filter)\n- If timing precision seems off, try --bilateral-filter\n- Always use with --outlier-rejection for best results\n- Consider experimental feature; may become default in future versions\n\n---\n\n## Contact Detection Parameters\n\n### `--velocity-threshold` (default: 0.02)\n\n**What it does:**\nThe vertical velocity threshold (in normalized coordinates) below which feet are considered stationary/on ground.\n\n**How it works:**\n\n- Velocity is calculated as change in y-position per frame\n- Units are in normalized coordinates (0-1, where 1 = full frame height)\n- Velocity &lt; threshold = potentially on ground\n- Velocity &gt; threshold = in motion (flight)\n\n**Technical details:**\n\n- Calculated: `velocity = abs(y_position[frame] - y_position[frame-1])`\n- Applied to average foot position (mean of all visible foot landmarks)\n- Works in combination with `min-contact-frames`\n\n**When to decrease (0.01, 0.005):**\n\n- Missing flight phases (everything detected as ground contact)\n- Very reactive jumps with minimal ground time\n- High frame rate video (motion per frame is smaller)\n- Athlete has minimal vertical movement during contact\n\n**When to increase (0.03, 0.05):**\n\n- Detecting false contacts during flight\n- Video has significant jitter/noise\n- Low frame rate video (larger motion per frame)\n- Athlete bounces during ground contact\n\n**Math example:**\n\n```text\nVideo: 1080p (height = 1 in normalized coords)\nFrame rate: 30 fps\nThreshold: 0.02\n\n0.02 * 1080 pixels = 21.6 pixels per frame\n21.6 pixels * 30 fps = 648 pixels/second\n\nSo feet moving &lt; 648 pixels/sec vertically = on ground\n```text\n\n**Example:**\n\n```bash\n# Missing short flight phases\nkinemotion dropjump-analyze video.mp4 --velocity-threshold 0.01\n\n# Too many false contacts detected\nkinemotion dropjump-analyze video.mp4 --velocity-threshold 0.03\n```text\n\n---\n\n### `--min-contact-frames` (default: 3)\n\n**What it does:**\nMinimum number of consecutive frames with low velocity required to confirm ground contact.\n\n**How it works:**\n\n- Acts as a temporal filter to remove spurious detections\n- If feet are stationary for &lt; N frames, contact is ignored\n- Prevents single-frame tracking glitches from being labeled as contact\n\n**Technical details:**\n\n- Applied after velocity thresholding\n- Works on consecutive frames only (not total count)\n- Example: [1, 1, 0, 1, 1] with min=3 \u2192 no valid contact (broken sequence)\n\n**When to increase (5, 7, 10):**\n\n- Video has significant tracking noise/jitter\n- Many false brief contacts detected\n- Athlete has long ground contact times (&gt;200ms)\n- Low confidence in tracking quality\n\n**When to decrease (1, 2):**\n\n- Missing very brief ground contacts\n- High-quality tracking with minimal noise\n- Very reactive/plyometric jumps\n- High frame rate video (60+ fps)\n\n**Frame rate consideration:**\n\n```text\n30 fps video:\n  3 frames = 100ms minimum contact time\n  5 frames = 167ms minimum contact time\n  10 frames = 333ms minimum contact time\n\n60 fps video:\n  3 frames = 50ms minimum contact time\n  6 frames = 100ms minimum contact time\n```text\n\n**Example:**\n\n```bash\n# Noisy tracking with false contacts\nkinemotion dropjump-analyze video.mp4 --min-contact-frames 5\n\n# Missing brief contacts in 60fps video\nkinemotion dropjump-analyze video.mp4 --min-contact-frames 2\n```text\n\n---\n\n### `--visibility-threshold` (default: 0.5)\n\n**What it does:**\nMinimum MediaPipe visibility score (0-1) required to trust a landmark for contact detection.\n\n**How it works:**\n\n- MediaPipe assigns each landmark a \"visibility\" score (0 = not visible, 1 = clearly visible)\n- Landmarks below threshold are ignored in contact detection\n- Average visibility of foot landmarks determines if frame is valid\n\n**Technical details:**\n\n- Applied to: left/right ankle, left/right heel, left/right foot index\n- If average foot visibility &lt; threshold \u2192 frame marked as UNKNOWN contact state\n- Does NOT affect pose tracking itself, only contact detection logic\n\n**When to decrease (0.3, 0.4):**\n\n- Feet frequently occluded (e.g., long grass, obstacles)\n- Side view not perfectly aligned\n- Baggy clothing covering feet/ankles\n- Many frames marked as UNKNOWN in debug video\n\n**When to increase (0.6, 0.7):**\n\n- Require high confidence in tracking\n- Front/back view where feet visibility varies greatly\n- Multiple people in frame (need clear foot separation)\n- Suspicious tracking results\n\n**MediaPipe visibility score meaning:**\n\n- 0.0-0.3: Landmark likely occluded or outside frame\n- 0.3-0.5: Low confidence, possibly visible\n- 0.5-0.7: Moderate confidence, probably visible\n- 0.7-1.0: High confidence, clearly visible\n\n**Example:**\n\n```bash\n# Feet often occluded by equipment\nkinemotion dropjump-analyze video.mp4 --visibility-threshold 0.3\n\n# Need high confidence tracking only\nkinemotion dropjump-analyze video.mp4 --visibility-threshold 0.7\n```text\n\n---\n\n## Pose Tracking Parameters (MediaPipe)\n\n### `--detection-confidence` (default: 0.5)\n\n**What it does:**\nMinimum confidence score (0-1) for MediaPipe to detect a pose in a frame.\n\n**How it works:**\n\n- First stage of MediaPipe Pose: \"Is there a person in this frame?\"\n- If confidence &lt; threshold \u2192 no pose detected for that frame\n- Higher threshold = fewer false detections but may miss valid poses\n- Only applied when MediaPipe needs to detect a NEW pose\n\n**Technical details:**\n\n- Used during initial detection and when tracking is lost\n- Once tracking starts, `tracking-confidence` takes over\n- Trade-off between false positives (detecting non-humans) and false negatives (missing real poses)\n\n**When to increase (0.6, 0.7, 0.8):**\n\n- Multiple people in frame\n- Background objects look like people\n- Getting false pose detections\n- Need very reliable pose initialization\n\n**When to decrease (0.3, 0.4):**\n\n- Person is far from camera\n- Poor lighting conditions\n- Unusual camera angle\n- Athlete wearing bulky equipment\n- Getting \"no pose detected\" errors\n\n**Example:**\n\n```bash\n# Multiple athletes in frame\nkinemotion dropjump-analyze video.mp4 --detection-confidence 0.7\n\n# Poor lighting, distant athlete\nkinemotion dropjump-analyze video.mp4 --detection-confidence 0.3\n```text\n\n---\n\n### `--tracking-confidence` (default: 0.5)\n\n**What it does:**\nMinimum confidence score (0-1) for MediaPipe to continue tracking an existing pose across frames.\n\n**How it works:**\n\n- Second stage of MediaPipe Pose: \"Is this still the same person as last frame?\"\n- If confidence &lt; threshold \u2192 tracking is lost, must re-detect pose\n- Higher threshold = more likely to re-detect if person moves quickly\n- Lower threshold = more persistent tracking even with occlusions\n\n**Technical details:**\n\n- Only used after initial pose detection succeeds\n- If tracking fails, falls back to detection stage (using `detection-confidence`)\n- Balance between tracking stability and false tracking\n\n**When to increase (0.6, 0.7, 0.8):**\n\n- Tracking jumps between different people/objects\n- Tracking continues when person leaves frame\n- Need to force re-detection frequently\n- Multiple moving objects in scene\n\n**When to decrease (0.3, 0.4):**\n\n- Tracking frequently lost during movement\n- Athlete moves very quickly\n- Temporary occlusions (e.g., arm passes in front of body)\n- Need more stable, persistent tracking\n\n**Relationship with detection-confidence:**\n\n```text\nHigh detection + High tracking = Very conservative, frequent re-detection\nHigh detection + Low tracking = Strict initialization, persistent tracking\nLow detection + High tracking = Easy initialization, frequent re-detection\nLow detection + Low tracking = Lenient overall, stable but risky\n```text\n\n**Example:**\n\n```bash\n# Tracking jumps to wrong person\nkinemotion dropjump-analyze video.mp4 --tracking-confidence 0.7\n\n# Tracking frequently lost during jump\nkinemotion dropjump-analyze video.mp4 --tracking-confidence 0.3\n```text\n\n---\n\n## Auto-Tuning System\n\nKinemotion uses an intelligent auto-tuning system that automatically optimizes analysis parameters based on video characteristics. This eliminates the need for manual calibration and makes the tool accessible to users without technical expertise.\n\n### How Auto-Tuning Works\n\nThe system analyzes:\n\n- **Video frame rate** - Adjusts smoothing windows and thresholds\n- **Tracking quality** - Adapts confidence levels and filtering\n- **Landmark visibility** - Determines outlier rejection needs\n- **Quality preset** - Balances speed vs accuracy based on user selection\n\n### Quality Presets\n\nAll analysis functions accept a `quality` parameter:\n\n- **`\"fast\"`** - Quick processing, good for batch operations (50% faster)\n- **`\"balanced\"`** - Default, optimal for most use cases\n- **`\"accurate\"`** - Research-grade, maximum precision (slower)\n\n**Example:**\n\n```bash\n# Fast processing for batch\nkinemotion dropjump-analyze videos/*.mp4 --batch --quality fast\n\n# Accurate for research\nkinemotion dropjump-analyze video.mp4 --quality accurate --verbose\n```\n\n**Python API:**\n\n```python\nfrom kinemotion import process_dropjump_video\n\nmetrics = process_dropjump_video(\n    \"video.mp4\",\n    quality=\"accurate\",\n    verbose=True  # Shows selected parameters\n)\n```text\n\n**Troubleshooting:**\n\n- If jump height still seems wrong:\n  1. Verify box height measurement is accurate\n  2. Check that entire drop is visible in video\n  3. Ensure camera is stationary (not panning/zooming)\n  4. Generate debug video to verify drop phase detection\n- If automatic drop jump detection fails:\n  - First ground phase must be &gt;5% higher than second ground phase\n  - Try adjusting contact detection parameters\n  - Check that athlete starts clearly on the box\n\n---\n\n## Trajectory Analysis Parameters\n\n### `--use-curvature / --no-curvature` (default: --use-curvature)\n\n**What it does:**\nEnables or disables trajectory curvature analysis for refining phase transition timing.\n\n**How it works:**\n\n- **With curvature** (`--use-curvature`, default): Uses acceleration patterns to refine event timing\n  - Step 1: Velocity-based detection finds approximate transitions (sub-frame interpolation)\n  - Step 2: Acceleration analysis searches \u00b13 frames for characteristic patterns\n  - Step 3: Blends curvature-based refinement (70%) with velocity estimate (30%)\n  - Landing detection: Finds maximum acceleration spike (impact deceleration)\n  - Takeoff detection: Finds maximum acceleration change (static \u2192 upward motion)\n\n- **Without curvature** (`--no-curvature`): Pure velocity-based detection\n  - Uses only velocity threshold crossings with sub-frame interpolation\n  - Simpler, faster algorithm\n  - Still highly accurate with smooth Savitzky-Golay velocity curves\n\n**Technical details:**\n\n- Acceleration computed using Savitzky-Golay second derivative (deriv=2)\n- Search window: \u00b13 frames around velocity-based estimate\n- Blending factor: 70% curvature + 30% velocity\n- No performance penalty (reuses smoothed trajectory from velocity calculation)\n- Independent validation based on physics (Newton's laws)\n\n**When to keep enabled (`--use-curvature`, default):**\n\n- Maximum accuracy desired\n- Rapid transitions (reactive jumps, short contact times)\n- Noisy velocity estimates need refinement\n- When combined with other accuracy features (CoM, adaptive threshold, calibration)\n- General use cases (recommended default)\n\n**When to disable (`--no-curvature`):**\n\n- Debugging: isolate velocity-based detection\n- Comparison with simpler algorithms\n- Extremely smooth, high-quality videos where velocity alone is sufficient\n- Research on pure velocity-based methods\n- Troubleshooting unexpected transition timing\n\n**Timing precision comparison:**\n\n```text\nWithout curvature (velocity only):\n- Uses smooth Savitzky-Golay velocity with sub-frame interpolation\n- Effective for most use cases\n- Theoretical timing precision: \u00b110ms at 30fps (\u26a0\ufe0f unvalidated)\n\nWith curvature (velocity + acceleration):\n- Refines timing using physics-based acceleration patterns\n- Theoretically more precise for rapid transitions\n- Theoretical timing precision: \u00b15-8ms at 30fps (\u26a0\ufe0f unvalidated)\n- Especially effective for landing detection (impact spike)\n```text\n\n**Physical basis:**\n\n```text\nLanding impact:\n- Large acceleration spike as feet decelerate body on contact\n- Peak acceleration marks exact landing moment\n- More precise than velocity threshold crossing\n\nTakeoff event:\n- Acceleration changes from ~0 (static) to positive (upward)\n- Maximum acceleration change marks exact takeoff\n- Validates velocity-based estimate\n\nDuring flight:\n- Constant acceleration (gravity \u2248 -9.81 m/s\u00b2)\n- Smooth trajectory, no spikes\n\nOn ground (static):\n- Near-zero acceleration\n- Stationary position\n```text\n\n**Example:**\n\n```bash\n# Default: curvature enabled\nkinemotion dropjump-analyze video.mp4\n\n# Explicitly enable curvature\nkinemotion dropjump-analyze video.mp4 --use-curvature\n\n# Disable for comparison\nkinemotion dropjump-analyze video.mp4 --no-curvature --json-output no_curve.json\n\n# Maximum accuracy: all features enabled\nkinemotion dropjump-analyze video.mp4 \\\n  --use-curvature \\\n  --adaptive-threshold \\\n  --use-com \\\n  --output debug_max.mp4 \\\n  --json-output metrics.json\n```\n\n**Effect on timing:**\n\n```text\nExample landing detection at 30fps:\n\nVelocity-based estimate: frame 49.0\n  \u2192 Velocity drops below threshold at this point\n\nCurvature refinement: frame 46.9\n  \u2192 Acceleration spike occurs earlier (impact moment)\n\nBlended result: 0.7 \u00d7 46.9 + 0.3 \u00d7 49.0 = 47.43\n  \u2192 2.1 frames (70ms) more accurate timing\n```text\n\n**Troubleshooting:**\n\n- If curvature refinement gives unexpected results:\n  1. Disable with `--no-curvature` to see velocity-only timing\n  2. Generate debug video to verify transition points\n  3. Check if acceleration patterns are unusual (e.g., soft landing, gradual takeoff)\n  4. Try adjusting `--smoothing-window` (affects derivative quality)\n- If timing seems off:\n  - Curvature only refines by \u00b13 frames maximum\n  - Blending prevents large deviations from velocity estimate\n  - Core velocity detection may need parameter tuning\n\n---\n\n## Common Scenarios and Recommended Settings\n\n### Scenario 1: High-Quality Studio Video\n\n- 60fps, stable camera, good lighting, clear side view\n\n```bash\nkinemotion dropjump-analyze video.mp4 \\\n  --smoothing-window 3 \\\n  --velocity-threshold 0.015 \\\n  --min-contact-frames 2 \\\n  --visibility-threshold 0.6 \\\n  --detection-confidence 0.5 \\\n  --tracking-confidence 0.5\n```text\n\n### Scenario 2: Outdoor Handheld Video\n\n- 30fps, camera shake, variable lighting, somewhat noisy\n\n```bash\nkinemotion dropjump-analyze video.mp4 \\\n  --smoothing-window 7 \\\n  --velocity-threshold 0.02 \\\n  --min-contact-frames 4 \\\n  --visibility-threshold 0.4 \\\n  --detection-confidence 0.4 \\\n  --tracking-confidence 0.4\n```text\n\n**Note:** Higher smoothing compensates for camera shake.\n\n### Scenario 3: Low-Quality Smartphone Video\n\n- 30fps, distant view, poor lighting, compression artifacts\n\n```bash\nkinemotion dropjump-analyze video.mp4 \\\n  --smoothing-window 9 \\\n  --velocity-threshold 0.025 \\\n  --min-contact-frames 5 \\\n  --visibility-threshold 0.3 \\\n  --detection-confidence 0.3 \\\n  --tracking-confidence 0.3\n```text\n\n**Note:** High smoothing filters out jitter from compression artifacts.\n\n### Scenario 4: Very Reactive/Fast Jumps\n\n- Need to capture brief flight times and contacts\n\n```bash\nkinemotion dropjump-analyze video.mp4 \\\n  --smoothing-window 3 \\\n  --velocity-threshold 0.01 \\\n  --min-contact-frames 2 \\\n  --visibility-threshold 0.5 \\\n  --detection-confidence 0.5 \\\n  --tracking-confidence 0.5\n```text\n\n### Scenario 5: Multiple People in Frame\n\n- Need to avoid tracking wrong person\n\n```bash\nkinemotion dropjump-analyze video.mp4 \\\n  --smoothing-window 5 \\\n  --velocity-threshold 0.02 \\\n  --min-contact-frames 3 \\\n  --visibility-threshold 0.6 \\\n  --detection-confidence 0.7 \\\n  --tracking-confidence 0.7\n```text\n\n### Scenario 6: Drop Jump with Expert Parameter Tuning\n\n- Drop jump analysis with manually tuned parameters for specific conditions\n\n```bash\nkinemotion dropjump-analyze video.mp4 \\\n  --quality accurate \\\n  --smoothing-window 5 \\\n  --velocity-threshold 0.02 \\\n  --min-contact-frames 3 \\\n  --visibility-threshold 0.5 \\\n  --detection-confidence 0.5 \\\n  --tracking-confidence 0.5 \\\n  --output debug.mp4 \\\n  --json-output metrics.json\n```\n\n**Note:** Expert parameters should only be adjusted when the automatic tuning doesn't work for your specific video conditions. The `--verbose` flag shows auto-selected parameters for comparison.\n\n### Scenario 7: High-Performance Drop Jump Analysis (Maximum Accuracy)\n\n- Research-grade analysis with all accuracy features enabled\n\n```bash\nkinemotion dropjump-analyze video.mp4 \\\n  --quality accurate \\\n  --use-curvature \\\n  --outlier-rejection \\\n  --output debug_max.mp4 \\\n  --json-output metrics.json \\\n  --smoothing-window 5 \\\n  --velocity-threshold 0.02 \\\n  --min-contact-frames 3 \\\n  --visibility-threshold 0.6 \\\n  --detection-confidence 0.5 \\\n  --tracking-confidence 0.5\n```\n\n**Note:** This uses maximum accuracy settings with advanced filtering:\n\n- Curvature analysis: Enhanced timing precision\n- Outlier rejection: Removes tracking glitches\n- Fine-tuned expert parameters: Optimized for clean, high-quality videos\n\n---\n\n## Debugging Workflow\n\n### Step 1: Generate Debug Video\n\nAlways start with a debug video to visualize what's happening:\n\n```bash\nkinemotion dropjump-analyze video.mp4 --output debug.mp4\n```text\n\n### Step 2: Identify the Problem\n\nWatch `debug.mp4` and look for:\n\n| Problem | Visual Indication | Parameter to Adjust |\n|---------|------------------|---------------------|\n| Foot position jumps around | Circle/landmarks jittery | \u2191 smoothing-window |\n| False flight phases | Red circle during ground contact | \u2191 velocity-threshold or \u2191 min-contact-frames |\n| Missing flight phases | Green circle during jump | \u2193 velocity-threshold |\n| \"UNKNOWN\" states everywhere | Frequent state changes | \u2193 visibility-threshold |\n| No pose detected | No landmarks visible | \u2193 detection-confidence |\n| Tracking wrong person | Landmarks jump to other person | \u2191 tracking-confidence |\n\n### Step 3: Adjust One Parameter at a Time\n\n```bash\n# Test hypothesis: missing contacts due to high velocity threshold\nkinemotion dropjump-analyze video.mp4 --output debug2.mp4 --velocity-threshold 0.01\n\n# Compare debug.mp4 vs debug2.mp4\n```text\n\n### Step 4: Verify with JSON Output\n\n```bash\nkinemotion dropjump-analyze video.mp4 \\\n  --json-output results.json \\\n  --smoothing-window 7 \\\n  --velocity-threshold 0.015\n\n# Check metrics make sense\ncat results.json\n```text\n\n---\n\n## Parameter Interactions\n\n### Smoothing affects velocity calculation\n\n```text\nHigh smoothing \u2192 smoother velocity \u2192 may need lower velocity-threshold\nLow smoothing \u2192 noisier velocity \u2192 may need higher velocity-threshold\n```text\n\n### Velocity + min-contact-frames work together\n\n```text\nStrict velocity (low) + lenient frames (low) = sensitive to brief contacts\nLenient velocity (high) + strict frames (high) = only long, clear contacts\n```text\n\n### Detection + tracking confidence relationship\n\n```text\nIf detection-confidence &gt; tracking-confidence:\n  \u2192 Will re-detect frequently (less stable tracking)\n\nIf tracking-confidence &gt; detection-confidence:\n  \u2192 Will maintain tracking longer (more stable)\n```text\n\n### Frame rate affects velocity threshold\n\n```text\n30 fps:\n\u2192 More motion per frame\n\u2192 May need higher velocity-threshold (e.g., 0.02)\n\u2192 Adjust min-contact-frames based on expected contact duration\n\n60 fps:\n\u2192 Less motion per frame\n\u2192 May need lower velocity-threshold (e.g., 0.01)\n\u2192 Can use smaller min-contact-frames for brief contacts\n```text\n\n### Curvature + sub-frame interpolation work together\n\n```text\nBoth enabled (default):\n\u2192 Velocity interpolation gives sub-frame precision\n\u2192 Curvature refines based on acceleration patterns\n\u2192 Blended result combines both methods\n\u2192 Best timing accuracy\n\nCurvature disabled:\n\u2192 Pure velocity-based interpolation\n\u2192 Still highly accurate with smooth derivatives\n\u2192 Useful for debugging or comparison\n```text\n\n### Outlier rejection + bilateral filter pipeline\n\n```text\nOutlier rejection first (when enabled):\n\u2192 Removes tracking glitches (jumps, spikes)\n\u2192 Replaces with interpolated values\n\u2192 Cleans data for subsequent smoothing\n\nThen bilateral filter (if enabled) OR Savitzky-Golay (default):\n\u2192 Bilateral: edge-preserving, replaces Savitzky-Golay\n\u2192 Savitzky-Golay: uniform smoothing (default)\n\nBest practice:\n\u2192 Keep outlier-rejection enabled (default)\n\u2192 Use bilateral for high-quality videos with rapid transitions\n\u2192 Use Savitzky-Golay (default) for most cases\n```text\n\n### Bilateral filter replaces smoothing-window/polyorder\n\n```text\nWhen bilateral-filter enabled:\n\u2192 Ignores --smoothing-window parameter\n\u2192 Ignores --polyorder parameter\n\u2192 Uses its own window size (9 frames) and weighting\n\nWhen bilateral-filter disabled (default):\n\u2192 Uses --smoothing-window and --polyorder\n\u2192 Standard Savitzky-Golay smoothing\n```text\n\n---\n\n## Performance Impact\n\n| Parameter | Performance Impact |\n|-----------|-------------------|\n| smoothing-window | Negligible (post-processing) |\n| polyorder | Negligible (same algorithm complexity) |\n| outlier-rejection | Low (~5-10% total time increase) |\n| bilateral-filter | Medium (~10-20% total time increase when enabled) |\n| velocity-threshold | None (simple comparison) |\n| min-contact-frames | None (simple counting) |\n| visibility-threshold | None (simple comparison) |\n| detection-confidence | Medium (affects MediaPipe workload) |\n| tracking-confidence | Medium (affects MediaPipe workload) |\n| use-curvature | Negligible (reuses smoothed trajectory) |\n\n**Notes:**\n\n- Higher confidence thresholds can actually improve performance by reducing unnecessary pose detection/tracking attempts\n- Curvature analysis reuses existing derivatives, effectively free\n- Polyorder has no performance impact (polynomial fit complexity is O(window_size), independent of order)\n\n---\n\n## Advanced Tips\n\n### 1. Frame Rate Matters\n\nScale velocity-threshold and min-contact-frames based on FPS:\n\n```text\n30 fps: velocity-threshold = 0.02, min-contact-frames = 3\n60 fps: velocity-threshold = 0.01, min-contact-frames = 6\n```text\n\n### 2. Aspect Ratio Considerations\n\nVelocity threshold is in normalized coordinates:\n\n- Tall videos (9:16 portrait): threshold has more \"room\" vertically\n- Wide videos (16:9 landscape): threshold has less relative space\n- Generally doesn't require adjustment, but good to be aware\n\n### 3. Use Debug Video's Frame Numbers\n\nThe debug video shows frame numbers. Use these with JSON output:\n\n```json\n{\n  \"contact_start_frame\": 10,\n  \"contact_end_frame\": 35,\n  \"flight_start_frame\": 36,\n  \"flight_end_frame\": 45\n}\n```text\n\nJump to these frames in debug video to verify detection accuracy.\n\n### 4. Iterate Systematically\n\n```bash\n# Baseline\nkinemotion dropjump-analyze video.mp4 --output v1.mp4 --json-output v1.json\n\n# Test smoothing\nkinemotion dropjump-analyze video.mp4 --output v2.mp4 --json-output v2.json --smoothing-window 7\n\n# Test velocity\nkinemotion dropjump-analyze video.mp4 --output v3.mp4 --json-output v3.json --smoothing-window 7 --velocity-threshold 0.015\n\n# Compare v1, v2, v3 side-by-side\n```text\n\n---\n\n## Summary Table\n\n| Parameter | Default | Range | Primary Effect | Adjust When |\n|-----------|---------|-------|----------------|-------------|\n| `smoothing-window` | 5 | 3-11 (odd) | Trajectory smoothness | Video is jittery or too smooth |\n| `polyorder` | 2 | 1-4 | Polynomial fit complexity | High-quality video with complex motion (+1-2%) |\n| `outlier-rejection` | enabled | enabled/disabled | Removes tracking glitches | Keep enabled unless debugging (+1-2%) |\n| `bilateral-filter` | disabled | enabled/disabled | Edge-preserving smoothing | High-quality video with rapid transitions (+1-2%) |\n| `velocity-threshold` | 0.02 | 0.005-0.05 | Contact sensitivity | Missing contacts or false detections |\n| `min-contact-frames` | 3 | 1-10 | Contact duration filter | Brief false contacts or missing short contacts |\n| `visibility-threshold` | 0.5 | 0.3-0.8 | Landmark trust level | Occlusions or need high confidence |\n| `detection-confidence` | 0.5 | 0.1-0.9 | Initial pose detection | Multiple people or poor visibility |\n| `tracking-confidence` | 0.5 | 0.1-0.9 | Tracking persistence | Tracking lost or wrong person tracked |\n| `use-curvature` | enabled | enabled/disabled | Timing refinement | Default: keep enabled for best accuracy |\n| `quality` | balanced | fast/balanced/accurate | Analysis speed vs accuracy | Use fast for batch, accurate for research |\n```\n</code></pre>"},{"location":"reference/pose-systems/","title":"Pose Estimation Systems - Quick Reference","text":"<p>Last Updated: November 2025 (Sources through July 2025)</p>"},{"location":"reference/pose-systems/#tldr-best-choice-by-use-case","title":"TL;DR - Best Choice by Use Case","text":"Use Case Recommended System Accuracy Setup Effort Cost Research Lab Pose2Sim \u2605\u2605\u2605\u2605\u2605 (3-4\u00b0) Medium Free Field Testing OpenCap \u2605\u2605\u2605\u2605\u2606 Low Free Clinical/Rehab Stereo MediaPipe \u2605\u2605\u2605\u2606\u2606 Low Free Elite Sports Pose2Sim + Sport Data \u2605\u2605\u2605\u2605\u2605 Medium-High Free-$$$ Commercial Theia3D \u2605\u2605\u2605\u2605\u2606 Low $$$"},{"location":"reference/pose-systems/#system-comparison-table","title":"System Comparison Table","text":"System Joint Angle Error Position RMSE Validation Cameras Open Source Pose2Sim 3-4\u00b0 30-40mm Qualisys \u2713 4-8 Yes OpenCap TBD TBD Emerging 2+ phones Yes Stereo MediaPipe ~5-7\u00b0 30mm Qualisys \u2713 2 Yes Theia3D 2.6-13.2\u00b0 - Published Multi No Current MediaPipe ~10-15\u00b0 56mm Limited 1 Yes Marker-Based \\&lt;2\u00b0 1-15mm Gold Std 8+ No"},{"location":"reference/pose-systems/#quick-decision-tree","title":"Quick Decision Tree","text":"<pre><code>Need sports biomechanics accuracy?\n\u251c\u2500 Yes\n\u2502  \u251c\u2500 Have budget/time for multi-camera setup?\n\u2502  \u2502  \u251c\u2500 Yes \u2192 Use Pose2Sim\n\u2502  \u2502  \u2514\u2500 No \u2192 Use OpenCap (smartphones)\n\u2502  \u2514\u2500 Just need exercise assessment?\n\u2502     \u2514\u2500 Use Stereo MediaPipe OR OpenCap\n\u2514\u2500 No (general pose detection)\n   \u2514\u2500 Stay with current MediaPipe/OpenPose\n</code></pre>"},{"location":"reference/pose-systems/#pose2sim-quick-setup","title":"Pose2Sim Quick Setup","text":"<pre><code># 1. Install\npip install pose2sim\n\n# 2. Setup project\npose2sim-project setup\n\n# 3. Run pipeline\npose2sim calibration      # Camera calibration\npose2sim poseEstimation   # 2D pose detection\npose2sim triangulation    # 3D reconstruction\npose2sim filtering        # Smooth trajectories\npose2sim kinematics       # OpenSim IK \u2192 joint angles\n</code></pre> <p>Hardware Needed:</p> <ul> <li>4-8 RGB cameras (1080p+, 60 Hz+)</li> <li>Calibration board (ChArUco)</li> <li>Sync (hardware or software)</li> </ul> <p>Output:</p> <ul> <li>3D joint coordinates (.trc)</li> <li>Joint angles (.mot)</li> <li>Velocities, accelerations</li> <li>OpenSim model</li> </ul>"},{"location":"reference/pose-systems/#opencap-quick-setup","title":"OpenCap Quick Setup","text":"<pre><code># 1. Record with 2+ smartphones\n# 2. Upload to opencap.ai\n# 3. Download results\n# 4. Parse OpenSim output\n</code></pre> <p>Hardware Needed:</p> <ul> <li>2+ smartphones</li> <li>Internet connection</li> </ul> <p>Output:</p> <ul> <li>OpenSim results</li> <li>Joint angles</li> <li>Video with overlay</li> </ul>"},{"location":"reference/pose-systems/#stereo-mediapipe-quick-setup","title":"Stereo MediaPipe Quick Setup","text":"<pre><code>import mediapipe as mp\nimport cv2\n\n# Process two camera views\npose = mp.solutions.pose.Pose()\nresults1 = pose.process(frame1)  # Camera 1\nresults2 = pose.process(frame2)  # Camera 2\n\n# Triangulate (need calibration)\npoint_3d = triangulate(results1, results2, calib)\n</code></pre> <p>Hardware Needed:</p> <ul> <li>2 cameras at 90\u00b0 angle</li> <li>Camera calibration</li> </ul> <p>Output:</p> <ul> <li>3D joint coordinates</li> <li>MediaPipe landmarks (33 points)</li> </ul>"},{"location":"reference/pose-systems/#kinemotion-upgrade-options","title":"Kinemotion Upgrade Options","text":""},{"location":"reference/pose-systems/#option-1-pose2sim-best-accuracy","title":"Option 1: Pose2Sim (Best Accuracy)","text":"<ul> <li>Effort: Medium (2-4 weeks)</li> <li>Benefit: Research-grade (3-4\u00b0 errors)</li> <li>Changes: Add 3-4 cameras, implement full pipeline</li> </ul>"},{"location":"reference/pose-systems/#option-2-stereo-mediapipe-simple","title":"Option 2: Stereo MediaPipe (Simple)","text":"<ul> <li>Effort: Low (1-2 weeks)</li> <li>Benefit: ~50% error reduction</li> <li>Changes: Add 1 camera, implement triangulation</li> </ul>"},{"location":"reference/pose-systems/#option-3-opencap-easiest","title":"Option 3: OpenCap (Easiest)","text":"<ul> <li>Effort: Very Low (days)</li> <li>Benefit: Biomechanical constraints</li> <li>Changes: Record with phones, parse output</li> </ul>"},{"location":"reference/pose-systems/#key-metrics-explained","title":"Key Metrics Explained","text":"<p>CMC (Coefficient of Multiple Correlation):</p> <ul> <li> <p>0.95: Excellent</p> </li> <li>0.85-0.94: Very good</li> <li>0.75-0.84: Good</li> <li>\\&lt;0.75: Poor</li> </ul> <p>Joint Angle Error:</p> <ul> <li>\\&lt;2\u00b0: Gold standard (marker-based)</li> <li>3-5\u00b0: Excellent (research-grade markerless)</li> <li>5-10\u00b0: Good (clinical use)</li> <li> <p>10\u00b0: Limited use</p> </li> </ul> <p>Position RMSE:</p> <ul> <li>\\&lt;15mm: Gold standard</li> <li>30-40mm: Good markerless</li> <li>50-60mm: Acceptable for exercises</li> <li> <p>100mm: Poor</p> </li> </ul>"},{"location":"reference/pose-systems/#critical-success-factors","title":"Critical Success Factors","text":"<p>For sports biomechanics markerless systems:</p> <ol> <li>\u2705 Multi-camera (\u22652, ideally 4-8)</li> <li>\u2705 90\u00b0 camera separation (optimal triangulation)</li> <li>\u2705 Biomechanical constraints (OpenSim skeletal model)</li> <li>\u2705 Proper calibration (intrinsic + extrinsic)</li> <li>\u2705 Temporal filtering (Butterworth 4th order, 6 Hz)</li> <li>\u2705 Sport-specific training (fine-tune on athletic data)</li> <li>\u2705 Gold standard validation (compare to Vicon/Qualisys)</li> </ol>"},{"location":"reference/pose-systems/#common-pitfalls","title":"Common Pitfalls","text":"<p>\u274c Using monocular for biomechanics \u2192 depth ambiguity, noisy \u274c No biomechanical constraints \u2192 physically inconsistent \u274c Lateral view only \u2192 occlusion (kinemotion's issue) \u274c Poor calibration \u2192 3D reconstruction errors \u274c Generic training data for sports \u2192 69% worse accuracy \u274c No validation \u2192 unknown accuracy \u274c Low frame rate for fast movements \u2192 temporal aliasing</p>"},{"location":"reference/pose-systems/#resources","title":"Resources","text":"<p>Documentation:</p> <ul> <li>Pose2Sim: https://pose2sim.readthedocs.io/</li> <li>OpenCap: https://www.opencap.ai/</li> <li>OpenSim: https://opensim.stanford.edu/</li> </ul> <p>Datasets:</p> <ul> <li>AthletePose3D: https://github.com/calvinyeungck/athletepose3d</li> <li>BioCV: https://doi.org/10.1038/s41597-024-04077-3</li> </ul> <p>Key Papers (Full citations in main documentation):</p> <ul> <li>[1] Pagnon et al. (2022) - Pose2Sim validation: https://doi.org/10.3390/s22072712</li> <li>[2] Dill et al. (2024) - Stereo MediaPipe validation: https://doi.org/10.3390/s24237772</li> <li>[3] Yeung et al. (2025) - AthletePose3D dataset: https://arxiv.org/abs/2503.07499</li> <li>[4] Bazarevsky et al. (2020) - MediaPipe Pose: https://arxiv.org/abs/2006.10204</li> <li>[5] Cao et al. (2019) - OpenPose: https://doi.org/10.1109/TPAMI.2019.2929257</li> <li>[11] Delp et al. (2007) - OpenSim: https://doi.org/10.1109/TBME.2007.901024</li> </ul>"},{"location":"reference/pose-systems/#faq","title":"FAQ","text":"<p>Q: Can I use OpenPose/MediaPipe alone for biomechanics? A: No. They provide 2D/noisy 3D keypoints but lack biomechanical constraints and multi-view accuracy [1]. Use Pose2Sim or OpenCap.</p> <p>Q: How many cameras do I need? A: Minimum 2 (stereo), recommended 4-8 (full 3D).</p> <p>Q: Will DWpose or newer models help? A: Not significantly. The bottleneck is multi-view triangulation and biomechanical modeling, not the 2D detector.</p> <p>Q: What about IMUs (Xsens)? A: Comparable accuracy (2-5\u00b0 errors) but requires wearable sensors [1]. Video-based is more natural.</p> <p>Q: Can I process in real-time? A: Most systems are batch processing. Real-time requires optimization and GPU acceleration.</p> <p>Q: How do I validate my setup? A: Compare against marker-based system (Vicon/Qualisys). Calculate CMC, RMSE, ROM errors, Bland-Altman plots [1,2].</p>"},{"location":"reference/pose-systems/#contact-updates","title":"Contact &amp; Updates","text":"<p>This is a living document based on research as of November 2025 (sources through July 2025). The field is rapidly advancing.</p> <p>For kinemotion project-specific questions, see:</p> <ul> <li>Main documentation: <code>SPORTS_BIOMECHANICS_POSE_ESTIMATION.md</code></li> <li>Project: https://github.com/feniix/kinemotion (or your repo)</li> </ul> <p>Check for updates: Research landscape changes quickly. New validations and systems emerge regularly.</p>"},{"location":"research/sports-biomechanics-pose-estimation/","title":"Best Pose Detection &amp; Tracking for Sports Biomechanics","text":""},{"location":"research/sports-biomechanics-pose-estimation/#research-summary-november-2025","title":"Research Summary - November 2025","text":"<p>This document summarizes current state-of-the-art markerless pose estimation and tracking systems for sports biomechanics applications, based on validation studies and research literature published through mid-2025 (most recent: July 2025).</p>"},{"location":"research/sports-biomechanics-pose-estimation/#executive-summary","title":"Executive Summary","text":"<p>Key Finding: For sports biomechanics research, Pose2Sim is currently the best-validated open-source solution, achieving 3-4\u00b0 joint angle accuracy when validated against gold-standard motion capture systems [1].</p> <p>Critical Insight: The 2D pose estimator (OpenPose, MediaPipe, etc.) is only the first step. Sports biomechanics requires [2]:</p> <ol> <li>Multi-camera 3D triangulation</li> <li>Biomechanical skeletal modeling (OpenSim) [11]</li> <li>Inverse kinematics for joint angles</li> <li>Validation against gold-standard motion capture</li> </ol>"},{"location":"research/sports-biomechanics-pose-estimation/#1-sports-biomechanics-requirements","title":"1. Sports Biomechanics Requirements","text":""},{"location":"research/sports-biomechanics-pose-estimation/#why-general-pose-estimation-is-insufficient","title":"Why General Pose Estimation is Insufficient","text":"<p>Standard pose estimation libraries (OpenPose [5], MediaPipe [4], MoveNet [9]) are designed for general-purpose human pose detection but fall short for biomechanics because:</p> Requirement General HPE Sports Biomechanics Need Output 2D or noisy 3D keypoints Accurate 3D joint coordinates Joint Angles Not provided \\&lt;5\u00b0 error required Constraints None Anatomically plausible poses Frame Rate 30 fps typical 120-240 Hz for fast movements Validation General datasets Gold standard MoCap (Vicon, Qualisys) Kinematics Position only Position, velocity, acceleration Depth Ambiguous (monocular) Accurate 3D depth required"},{"location":"research/sports-biomechanics-pose-estimation/#biomechanics-specific-challenges","title":"Biomechanics-Specific Challenges","text":"<ol> <li>Joint Center Bias: ML-based detectors have systematic 10-50mm offsets from true anatomical joint centers [1]</li> <li>Depth Ambiguity: Monocular 3D estimation is ill-posed (multiple 3D poses \u2192 same 2D projection) [2]</li> <li>Occlusion: Self-occlusion in lateral views (e.g., 18-27% ankle/knee visibility in lateral camera angles)</li> <li>Physical Plausibility: Need to enforce joint angle limits and kinematic constraints [11]</li> <li>Temporal Consistency: Smooth trajectories required for velocity/acceleration calculations [1]</li> </ol>"},{"location":"research/sports-biomechanics-pose-estimation/#2-state-of-the-art-systems","title":"2. State-of-the-Art Systems","text":""},{"location":"research/sports-biomechanics-pose-estimation/#21-pose2sim-recommended-for-research","title":"2.1 Pose2Sim (Recommended for Research)","text":""},{"location":"research/sports-biomechanics-pose-estimation/#pose2sim-overview","title":"Pose2Sim Overview","text":"<ul> <li>Full pipeline for sports biomechanics: 2D detection \u2192 triangulation \u2192 OpenSim modeling</li> <li>Validated against Qualisys 31-marker system</li> <li>Open-source, actively maintained</li> <li>Citation: Pagnon, D., Domalain, M., &amp; Reveret, L. (2022). Pose2Sim: An End-to-End Workflow for 3D Markerless Sports Kinematics\u2014Part 2: Accuracy. Sensors, 22(7), 2712. https://doi.org/10.3390/s22072712 [1]</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#validated-accuracy-vs-qualisys-gold-standard","title":"Validated Accuracy (vs Qualisys Gold Standard)","text":"Task Mean Joint Angle Error ROM Error CMC Correlation Walking 3.0\u00b0 2.7\u00b0 0.90-0.98 Running 4.1\u00b0 2.3\u00b0 0.65-1.00* Cycling 4.0\u00b0 4.3\u00b0 0.75-1.00* <p>*Hip flexion/extension in running had 15\u00b0 offset (CMC=0.65) due to systematic bias *Ankle in cycling partially occluded (CMC=0.75)</p> <p>95% Limits of Agreement: \u00b115\u00b0 across all movements (Bland-Altman analysis)</p>"},{"location":"research/sports-biomechanics-pose-estimation/#pipeline-architecture","title":"Pipeline Architecture","text":"<pre><code>1. Multi-camera video capture (4-8 cameras recommended)\n2. 2D pose estimation (OpenPose, MediaPipe, or AlphaPose)\n3. Person tracking across views\n4. Robust 3D triangulation (RANSAC filtering)\n5. Butterworth filtering (4th order, 6 Hz)\n6. OpenSim skeletal model scaling\n7. Inverse kinematics optimization\n8. Export: joint angles, velocities, accelerations\n</code></pre>"},{"location":"research/sports-biomechanics-pose-estimation/#key-features","title":"Key Features","text":"<ul> <li>Flexible 2D backend (works with OpenPose, MediaPipe BlazePose, AlphaPose)</li> <li>Physically consistent poses via OpenSim constraints</li> <li>Camera calibration without prior calibration (optional)</li> <li>Robust to dark/blurry images, calibration errors</li> <li>Works with as few as 4 cameras</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#setup-requirements","title":"Setup Requirements","text":"<ul> <li>4-8 RGB cameras (90\u00b0 separation optimal)</li> <li>Hardware or software synchronization</li> <li>Python environment</li> <li>OpenSim for biomechanical modeling</li> </ul> <p>Repository: https://github.com/perfanalytics/pose2sim</p>"},{"location":"research/sports-biomechanics-pose-estimation/#validation-studies","title":"Validation Studies","text":"<ul> <li>Pagnon et al. (2022): Walking, running, cycling validation</li> <li>Compared favorably to Theia3D commercial system</li> <li>Mean errors within marker-based system tolerances</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#22-opencap-accessible-alternative","title":"2.2 OpenCap (Accessible Alternative)","text":""},{"location":"research/sports-biomechanics-pose-estimation/#opencap-overview","title":"OpenCap Overview","text":"<ul> <li>Stanford-developed, web-based motion capture system [13]</li> <li>Uses smartphones (2+ required)</li> <li>Free, open-source</li> <li>Designed specifically for biomechanics research</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#features","title":"Features","text":"<ul> <li>No specialized hardware required</li> <li>Automatic OpenSim integration</li> <li>Web-based processing (no local compute needed)</li> <li>Growing validation literature</li> <li>Accessible to non-experts</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#validation-status","title":"Validation Status","text":"<ul> <li>Multiple ongoing validation studies (2024)</li> <li>Designed for clinical and sports applications</li> <li>Accuracy metrics still emerging in literature</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#use-cases","title":"Use Cases","text":"<ul> <li>Field testing and training environments</li> <li>Clinical assessments</li> <li>Educational applications</li> <li>Low-resource settings</li> </ul> <p>Access: https://www.opencap.ai/</p>"},{"location":"research/sports-biomechanics-pose-estimation/#23-stereo-mediapipe-triangulation","title":"2.3 Stereo MediaPipe + Triangulation","text":""},{"location":"research/sports-biomechanics-pose-estimation/#stereo-mediapipe-overview","title":"Stereo MediaPipe Overview","text":"<ul> <li>Two-camera setup using MediaPipe Pose [4]</li> <li>2D detection + stereo triangulation</li> <li>No biomechanical constraints (unless added)</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#validated-accuracy","title":"Validated Accuracy","text":"<ul> <li>Median RMSE: 30.1mm (vs Qualisys)</li> <li>Monocular MediaPipe: 56.3mm RMSE</li> <li>Improvement: 47% reduction in error with stereo</li> <li>Statistical significance: p \\&lt; 10\u207b\u2076</li> </ul> <p>Validation Study: Dill, S., Ahmadi, A., Grimmer, M., Haufe, D., Rohr, M., Zhao, Y., Sharbafi, M., &amp; Hoog Antink, C. (2024). Accuracy Evaluation of 3D Pose Reconstruction Algorithms Through Stereo Camera Information Fusion for Physical Exercises with MediaPipe Pose. Sensors, 24(23), 7772. https://doi.org/10.3390/s24237772 [2]</p>"},{"location":"research/sports-biomechanics-pose-estimation/#key-findings-2","title":"Key Findings [2]","text":"<ul> <li>9 subjects performing squats (correct and incorrect)</li> <li>Validated against Qualisys 11-camera system</li> <li>Sufficient accuracy for exercise error detection</li> <li>Not recommended for precise kinematic analysis</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#optimal-setup","title":"Optimal Setup","text":"<ul> <li>90\u00b0 angle between cameras (from Pagnon et al.)</li> <li>MediaPipe Pose (BlazePose model)</li> <li>Triangulation with epipolar geometry</li> <li>6 Hz Butterworth filtering</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#stereo-mediapipe-limitations","title":"Stereo MediaPipe Limitations","text":"<ul> <li>No biomechanical constraints \u2192 anatomically inconsistent poses possible</li> <li>Systematic biases from MediaPipe remain</li> <li>Lower accuracy than Pose2Sim</li> <li>Better for exercise assessment than research</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#24-athletepose3d-sport-specific-training","title":"2.4 AthletePose3D (Sport-Specific Training)","text":""},{"location":"research/sports-biomechanics-pose-estimation/#athletepose3d-overview","title":"AthletePose3D Overview","text":"<ul> <li>New benchmark dataset for athletic movements (CVSports at CVPR 2025)</li> <li>Addresses failure of general models on high-speed sports</li> <li>Fine-tuning dataset for sport-specific applications</li> </ul> <p>Citation: Yeung, C., Suzuki, T., Tanaka, R., Yin, Z., &amp; Fujii, K. (2025). AthletePose3D: A Benchmark Dataset for 3D Human Pose Estimation and Kinematic Validation in Athletic Movements. arXiv preprint arXiv:2503.07499. [3]</p>"},{"location":"research/sports-biomechanics-pose-estimation/#dataset-characteristics-3","title":"Dataset Characteristics [3]","text":"<ul> <li>12 sports movements</li> <li>1.3M frames, 165K postures</li> <li>High-speed, high-acceleration focus</li> <li>Validated against gold-standard MoCap</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#performance-impact-3","title":"Performance Impact [3]","text":"<ul> <li>Generic model MPJPE: 214mm</li> <li>After fine-tuning on AthletePose3D: 65mm</li> <li>Improvement: 69% (3.3x reduction in error)</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#applications","title":"Applications","text":"<ul> <li>Fine-tuning existing models for specific sports</li> <li>Training sport-specific pose estimators</li> <li>Validating markerless systems on athletic movements</li> </ul> <p>Availability: Dataset downloadable for research (non-commercial license)</p> <p>Repository: https://github.com/calvinyeungck/athletepose3d</p>"},{"location":"research/sports-biomechanics-pose-estimation/#25-commercial-systems","title":"2.5 Commercial Systems","text":""},{"location":"research/sports-biomechanics-pose-estimation/#theia3d","title":"Theia3D","text":"<ul> <li>Type: Commercial markerless solution</li> <li>Accuracy: 2.6\u00b0-13.2\u00b0 RMSE (walking validation) [14]</li> <li>Features: Proprietary 2D detector, triangulation, skeletal modeling</li> <li>Cost: Commercial licensing</li> <li>Validation: Published validation studies available [14]</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#valdsimicontemplas","title":"Vald/Simi/Contemplas","text":"<ul> <li>Industry-specific commercial solutions</li> <li>Turnkey systems with support</li> <li>Higher cost, easier deployment</li> <li>Limited academic validation literature</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#3-validation-studies-summary","title":"3. Validation Studies Summary","text":""},{"location":"research/sports-biomechanics-pose-estimation/#31-pose2sim-validation-pagnon-et-al-2022-1","title":"3.1 Pose2Sim Validation (Pagnon et al., 2022) [1]","text":""},{"location":"research/sports-biomechanics-pose-estimation/#pose2sim-study-design","title":"Pose2Sim Study Design","text":"<ul> <li>1 participant, 83 reflective markers (CAST marker set)</li> <li>Reference: 20 Qualisys opto-electronic cameras</li> <li>Tasks: Walking, running, cycling (8-13 cycles each)</li> <li>Comparison: Pose2Sim vs marker-based with same OpenSim model</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#results-lower-limb-sagittal-plane","title":"Results - Lower Limb (Sagittal Plane)","text":"Joint Task CMC Pearson r ROM Error Mean Error Ankle Walk 0.90 0.89 -1.2\u00b0 -2.8\u00b0 Knee Walk 0.98 0.96 -0.3\u00b0 -1.9\u00b0 Hip Walk 0.96 0.96 -1.7\u00b0 -3.0\u00b0 Ankle Run 0.99 0.99 -2.9\u00b0 -0.7\u00b0 Knee Run 1.00 1.00 0.0\u00b0 -0.7\u00b0 Hip Run 0.65 0.95 4.0\u00b0 15.2\u00b0* Ankle Cycle 0.75 0.85 1.9\u00b0 -6.7\u00b0* Knee Cycle 1.00 1.00 -2.9\u00b0 2.1\u00b0 Hip Cycle 0.92 0.97 -5.9\u00b0 6.1\u00b0 <p>*Systematic offsets due to occlusion or movement patterns</p>"},{"location":"research/sports-biomechanics-pose-estimation/#interpretation-of-cmc-coefficient-of-multiple-correlation-1","title":"Interpretation of CMC (Coefficient of Multiple Correlation) [1]","text":"<ul> <li>CMC &gt; 0.95: Excellent</li> <li>CMC 0.85-0.94: Very good</li> <li>CMC 0.75-0.84: Good</li> <li>CMC \\&lt; 0.75: Poor</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#key-findings-1","title":"Key Findings [1]","text":"<ol> <li>Sagittal plane (flexion/extension): Excellent agreement (CMC &gt; 0.9)</li> <li>Non-sagittal planes: Good to poor (CMC 0.3-0.9)</li> <li>Systematic offsets can occur (hip running, ankle cycling)</li> <li>Overall accuracy comparable to marker-based for primary movements</li> </ol>"},{"location":"research/sports-biomechanics-pose-estimation/#32-stereo-mediapipe-validation-dill-et-al-2024-2","title":"3.2 Stereo MediaPipe Validation (Dill et al., 2024) [2]","text":""},{"location":"research/sports-biomechanics-pose-estimation/#stereo-mediapipe-study-design","title":"Stereo MediaPipe Study Design","text":"<ul> <li>9 subjects performing squats (correct and incorrect forms)</li> <li>Reference: Qualisys 11-camera system, 31 markers</li> <li>Setup: 2 smartphones (frontal and lateral views)</li> <li>Comparison: Stereo MediaPipe vs monocular MediaPipe 3D</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#results","title":"Results","text":"Method Median RMSE Best Use Case Monocular MediaPipe 3D 56.3mm General pose estimation Stereo MediaPipe 30.1mm Exercise assessment Pose2Sim ~30-40mm Sports biomechanics Marker-based 1-15mm Research gold standard"},{"location":"research/sports-biomechanics-pose-estimation/#conclusions-2","title":"Conclusions [2]","text":"<ul> <li>Stereo significantly better than monocular (p \\&lt; 10\u207b\u2076)</li> <li>Sufficient for exercise error recognition</li> <li>Not recommended for precise biomechanical research</li> <li>90\u00b0 camera angle optimal (confirmed from Pagnon et al. [1])</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#33-athletepose3d-validation-yeung-et-al-2025-3","title":"3.3 AthletePose3D Validation (Yeung et al., 2025) [3]","text":""},{"location":"research/sports-biomechanics-pose-estimation/#key-findings-3","title":"Key Findings [3]","text":"<ul> <li>Problem: SOTA models trained on daily activities fail on athletic movements</li> <li>MPJPE on athletic motions: 214mm (generic models)</li> <li>After fine-tuning: 65mm MPJPE</li> <li>Improvement: 69% error reduction</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#kinematic-validation-3","title":"Kinematic Validation [3]","text":"<ul> <li>Strong joint angle correlation</li> <li>Limitations in velocity estimation</li> <li>Highlights need for sport-specific training data</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#impact-3","title":"Impact [3]","text":"<ul> <li>Proves generic pose datasets insufficient for sports</li> <li>Provides benchmark for athletic pose estimation</li> <li>Enables fine-tuning for sport-specific applications</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#4-comparison-pose2sim-vs-alternatives","title":"4. Comparison: Pose2Sim vs Alternatives","text":""},{"location":"research/sports-biomechanics-pose-estimation/#41-accuracy-comparison","title":"4.1 Accuracy Comparison","text":"System Joint Angle Error Position RMSE CMC Validation Cost Pose2Sim 3-4\u00b0 30-40mm &gt;0.9 Qualisys Free OpenCap TBD TBD TBD Emerging Free Stereo MediaPipe 5-7\u00b0* 30mm 0.75-0.85 Qualisys Free Theia3D 2.6-13.2\u00b0 - - Published $$$ Xsens (IMU) 2-5\u00b0 N/A - Published $$$$ Marker-Based \\&lt;2\u00b0 1-15mm 1.0 Gold Standard $$$$$ <p>*Estimated from RMSE and validation studies</p>"},{"location":"research/sports-biomechanics-pose-estimation/#42-pose2sim-vs-theia3d-vs-xsens-walking","title":"4.2 Pose2Sim vs Theia3D vs Xsens (Walking)","text":"<p>From Pagnon et al. (2022) comparison [1]:</p> Metric Pose2Sim Theia3D Xsens (IMU) Ankle RMSE 4.0\u00b0 - - Knee RMSE 5.1\u00b0 3.3\u00b0 - Hip RMSE 5.6\u00b0 11\u00b0 - Ankle Mean Error 2.8\u00b0 - 2.2\u00b0 Hip Mean Error 3.0\u00b0 - 2.5\u00b0 Ankle ROM Error -1.2\u00b0 \u2248-10\u00b0 0.4\u00b0 Hip ROM Error -1.7\u00b0 \u2248-10\u00b0 2.4\u00b0 <p>Note: Different studies, different setups. Direct comparison limited but indicates comparable performance.</p>"},{"location":"research/sports-biomechanics-pose-estimation/#5-system-recommendations-by-use-case","title":"5. System Recommendations by Use Case","text":""},{"location":"research/sports-biomechanics-pose-estimation/#51-research-labs-highest-accuracy","title":"5.1 Research Labs (Highest Accuracy)","text":""},{"location":"research/sports-biomechanics-pose-estimation/#recommended-pose2sim","title":"Recommended: Pose2Sim","text":""},{"location":"research/sports-biomechanics-pose-estimation/#why-pose2sim-for-research","title":"Why Pose2Sim for Research","text":"<ul> <li>Research-grade accuracy (3-4\u00b0 joint angles)</li> <li>Validated against gold standard</li> <li>Full biomechanics pipeline</li> <li>Open-source and customizable</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#pose2sim-setup","title":"Pose2Sim Setup","text":"<ul> <li>4-8 RGB cameras (1080p, 60-120 Hz)</li> <li>Hardware sync (recommended) or software sync</li> <li>Camera calibration (ChArUco or checkerboard)</li> <li>Compute: GPU recommended but not required</li> </ul> <p>Cost: Low (~$2,000-5,000 for cameras + compute)</p>"},{"location":"research/sports-biomechanics-pose-estimation/#workflow","title":"Workflow","text":"<pre><code>from Pose2Sim import Pose2Sim\n\n# Configure cameras and calibration\nPose2Sim.calibration()\n\n# Process videos\nPose2Sim.poseEstimation()  # 2D pose from each camera\nPose2Sim.synchronization()  # Sync multi-camera\nPose2Sim.triangulation()    # 3D reconstruction\nPose2Sim.filtering()        # Smooth trajectories\nPose2Sim.kinematics()       # OpenSim IK\n</code></pre>"},{"location":"research/sports-biomechanics-pose-estimation/#52-fieldtraining-environments","title":"5.2 Field/Training Environments","text":""},{"location":"research/sports-biomechanics-pose-estimation/#recommended-opencap","title":"Recommended: OpenCap","text":""},{"location":"research/sports-biomechanics-pose-estimation/#why-opencap-for-field-use","title":"Why OpenCap for Field Use","text":"<ul> <li>Minimal equipment (smartphones only)</li> <li>Web-based processing</li> <li>Designed for biomechanics</li> <li>No specialized setup</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#opencap-setup","title":"OpenCap Setup","text":"<ul> <li>2+ smartphones with cameras</li> <li>Internet connection</li> <li>OpenCap account (free)</li> </ul> <p>Cost: Minimal (use existing smartphones)</p>"},{"location":"research/sports-biomechanics-pose-estimation/#opencap-limitations","title":"OpenCap Limitations","text":"<ul> <li>Requires internet</li> <li>Less control over processing</li> <li>Validation still emerging</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#53-clinicalrehabilitation","title":"5.3 Clinical/Rehabilitation","text":""},{"location":"research/sports-biomechanics-pose-estimation/#recommended-stereo-mediapipe-or-opencap","title":"Recommended: Stereo MediaPipe OR OpenCap","text":""},{"location":"research/sports-biomechanics-pose-estimation/#why-for-clinical-use","title":"Why for Clinical Use","text":"<ul> <li>Sufficient accuracy for error detection</li> <li>Low setup complexity</li> <li>Cost-effective</li> <li>Validated for exercise assessment</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#setup-stereo-mediapipe","title":"Setup (Stereo MediaPipe)","text":"<ul> <li>2 cameras at 90\u00b0 angle</li> <li>MediaPipe Pose for 2D detection</li> <li>Triangulation with epipolar geometry</li> <li>Optional: Add OpenSim constraints</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#setup-opencap","title":"Setup (OpenCap)","text":"<ul> <li>2 smartphones</li> <li>Web-based workflow</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#54-elite-sports-performance","title":"5.4 Elite Sports Performance","text":""},{"location":"research/sports-biomechanics-pose-estimation/#recommended-pose2sim-athletepose3d-or-commercial","title":"Recommended: Pose2Sim + AthletePose3D OR Commercial","text":""},{"location":"research/sports-biomechanics-pose-estimation/#why-for-elite-sports","title":"Why for Elite Sports","text":"<ul> <li>Research-grade accuracy critical for competitive advantage</li> <li>Sport-specific fine-tuning available</li> <li>OR commercial support for deployment</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#considerations","title":"Considerations","text":"<ul> <li>Fine-tune on AthletePose3D for sport-specific movements</li> <li>May need &gt;120 Hz for fast movements (pitching, jumping)</li> <li>Consider commercial (Theia3D, Vald) for support</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#6-implementation-guide-pose2sim","title":"6. Implementation Guide: Pose2Sim","text":""},{"location":"research/sports-biomechanics-pose-estimation/#61-hardware-requirements","title":"6.1 Hardware Requirements","text":""},{"location":"research/sports-biomechanics-pose-estimation/#cameras","title":"Cameras","text":"<ul> <li>Minimum: 4 cameras (more is better)</li> <li>Resolution: 1080p minimum, 4K better</li> <li>Frame rate: 60 Hz minimum, 120-240 Hz for fast movements</li> <li>Placement: 90\u00b0 separation optimal, capture volume from multiple angles</li> <li>Sync: Hardware sync ideal, software sync acceptable</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#compute","title":"Compute","text":"<ul> <li>CPU: Multi-core for parallel processing</li> <li>GPU: Recommended for 2D pose estimation (not required)</li> <li>RAM: 16GB+ recommended</li> <li>Storage: Large for raw videos</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#hardware-calibration","title":"Hardware Calibration","text":"<ul> <li>ChArUco board or checkerboard</li> <li>Large enough to be visible from all cameras</li> <li>Multiple calibration poses/positions</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#62-software-setup","title":"6.2 Software Setup","text":"<pre><code># Install Pose2Sim\npip install pose2sim\n\n# Install 2D pose estimator (choose one)\n# OpenPose (most validated)\ngit clone https://github.com/CMU-Perceptual-Computing-Lab/openpose\n# OR MediaPipe\npip install mediapipe\n# OR AlphaPose\ngit clone https://github.com/MVIG-SJTU/AlphaPose\n</code></pre>"},{"location":"research/sports-biomechanics-pose-estimation/#63-workflow-steps","title":"6.3 Workflow Steps","text":""},{"location":"research/sports-biomechanics-pose-estimation/#1-camera-calibration","title":"1. Camera Calibration","text":"<pre><code>from Pose2Sim import Pose2Sim\nPose2Sim.calibration()\n</code></pre> <ul> <li>Capture calibration board from all cameras</li> <li>Estimates intrinsic and extrinsic parameters</li> <li>Outputs calibration file</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#2-video-capture","title":"2. Video Capture","text":"<ul> <li>Record synchronized videos from all cameras</li> <li>Subject performs movement in capture volume</li> <li>Ensure subject visible from all cameras</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#3-2d-pose-estimation","title":"3. 2D Pose Estimation","text":"<pre><code>Pose2Sim.poseEstimation()\n</code></pre> <ul> <li>Runs OpenPose/MediaPipe on each camera</li> <li>Outputs 2D joint coordinates per frame per camera</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#4-synchronization-if-needed","title":"4. Synchronization (if needed)","text":"<pre><code>Pose2Sim.synchronization()\n</code></pre> <ul> <li>Aligns camera timestamps</li> <li>Handles software sync if no hardware sync</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#5-triangulation","title":"5. Triangulation","text":"<pre><code>Pose2Sim.triangulation()\n</code></pre> <ul> <li>Reconstructs 3D coordinates from 2D views</li> <li>Uses RANSAC for robust outlier rejection</li> <li>Outputs 3D joint coordinates</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#6-filtering","title":"6. Filtering","text":"<pre><code>Pose2Sim.filtering()\n</code></pre> <ul> <li>Butterworth filter (4th order, 6 Hz default)</li> <li>Smooths trajectories for velocity/acceleration</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#7-opensim-kinematics","title":"7. OpenSim Kinematics","text":"<pre><code>Pose2Sim.kinematics()\n</code></pre> <ul> <li>Scales OpenSim model to subject</li> <li>Runs inverse kinematics</li> <li>Outputs joint angles, velocities, accelerations</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#64-output-data","title":"6.4 Output Data","text":"<p>Pose2Sim provides:</p> <ul> <li>3D joint coordinates (.trc files)</li> <li>Joint angles (.mot files)</li> <li>Joint velocities and accelerations</li> <li>OpenSim model files (.osim)</li> <li>Visualization files</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#7-kinemotion-specific-recommendations","title":"7. Kinemotion-Specific Recommendations","text":""},{"location":"research/sports-biomechanics-pose-estimation/#current-limitations","title":"Current Limitations","text":""},{"location":"research/sports-biomechanics-pose-estimation/#kinemotions-setup","title":"Kinemotion's Setup","text":"<ul> <li>MediaPipe monocular (single camera)</li> <li>Lateral view (side view)</li> <li>Good temporal processing (smoothing, signed velocity)</li> <li>Backward search algorithm for CMJ</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#identified-issues","title":"Identified Issues","text":"<ul> <li>18-27% ankle/knee visibility in lateral view</li> <li>Depth ambiguity (noisy Z-axis from monocular)</li> <li>No biomechanical constraints</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#recommended-upgrade-path","title":"Recommended Upgrade Path","text":""},{"location":"research/sports-biomechanics-pose-estimation/#option-1-implement-pose2sim-best-accuracy","title":"Option 1: Implement Pose2Sim (Best Accuracy)","text":""},{"location":"research/sports-biomechanics-pose-estimation/#option-1-changes-needed","title":"Option 1: Changes Needed","text":"<ol> <li>Add 3-4 cameras (one frontal, one lateral, 1-2 at 45\u00b0 angles)</li> <li>Implement camera synchronization</li> <li>Replace single MediaPipe call with Pose2Sim pipeline</li> <li>Add OpenSim skeletal model</li> <li>Keep existing: smoothing, signed velocity, backward search</li> </ol>"},{"location":"research/sports-biomechanics-pose-estimation/#option-1-benefits","title":"Option 1: Benefits","text":"<ul> <li>Eliminates occlusion problem</li> <li>Research-grade accuracy</li> <li>Physically consistent poses</li> <li>Validated for jumping movements</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#integration-with-existing-code","title":"Integration with Existing Code","text":"<pre><code># kinemotion/core/pose_estimation.py (new module)\nfrom Pose2Sim import Pose2Sim\n\ndef process_multi_camera_video(video_paths, drop_height):\n    \"\"\"Process multi-camera video with Pose2Sim\"\"\"\n    # 1. Run Pose2Sim pipeline\n    Pose2Sim.poseEstimation()\n    Pose2Sim.triangulation()\n    Pose2Sim.filtering()\n    Pose2Sim.kinematics()\n\n    # 2. Extract ankle/hip trajectories from OpenSim output\n    ankle_pos = extract_joint_trajectory('ankle')\n    hip_pos = extract_joint_trajectory('hip')\n\n    # 3. Use existing kinemotion algorithms\n    from kinemotion.dropjump.analysis import detect_ground_contact\n    ground_contact = detect_ground_contact(ankle_pos, ...)\n\n    # 4. Calculate metrics\n    metrics = calculate_metrics(...)\n    return metrics\n</code></pre> <p>Effort: Medium (2-4 weeks integration)</p>"},{"location":"research/sports-biomechanics-pose-estimation/#option-2-add-second-camera-simpler","title":"Option 2: Add Second Camera (Simpler)","text":""},{"location":"research/sports-biomechanics-pose-estimation/#option-2-changes-needed","title":"Option 2: Changes Needed","text":"<ol> <li>Add 1 camera at 90\u00b0 from current lateral view</li> <li>Implement stereo triangulation</li> <li>Keep MediaPipe as 2D detector</li> <li>Optionally add OpenSim constraints</li> </ol>"},{"location":"research/sports-biomechanics-pose-estimation/#option-2-benefits","title":"Option 2: Benefits","text":"<ul> <li>Lower complexity than Pose2Sim</li> <li>Solves depth ambiguity</li> <li>Can reuse existing MediaPipe code</li> <li>~50% error reduction</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#integration","title":"Integration","text":"<pre><code># kinemotion/core/stereo.py (new module)\nimport mediapipe as mp\nimport cv2\n\ndef triangulate_stereo(video1_path, video2_path, calibration):\n    \"\"\"Triangulate 3D pose from two camera views\"\"\"\n    # Run MediaPipe on both cameras\n    pose1 = run_mediapipe(video1_path)\n    pose2 = run_mediapipe(video2_path)\n\n    # Triangulate matching keypoints\n    pose3d = triangulate(pose1, pose2, calibration)\n\n    # Apply to existing pipeline\n    return pose3d\n</code></pre> <p>Effort: Low (1-2 weeks integration)</p>"},{"location":"research/sports-biomechanics-pose-estimation/#option-3-try-opencap-easiest","title":"Option 3: Try OpenCap (Easiest)","text":""},{"location":"research/sports-biomechanics-pose-estimation/#option-3-changes-needed","title":"Option 3: Changes Needed","text":"<ol> <li>Record videos with 2 smartphones</li> <li>Upload to OpenCap web interface</li> <li>Download OpenSim results</li> <li>Parse output for existing metrics</li> </ol>"},{"location":"research/sports-biomechanics-pose-estimation/#option-3-benefits","title":"Option 3: Benefits","text":"<ul> <li>Minimal code changes</li> <li>No camera setup</li> <li>Automatic biomechanical constraints</li> <li>Free</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#option-3-limitations","title":"Option 3: Limitations","text":"<ul> <li>Less control over processing</li> <li>Requires internet</li> <li>May not match existing output format exactly</li> </ul> <p>Effort: Very Low (days to test)</p>"},{"location":"research/sports-biomechanics-pose-estimation/#performance-comparison-estimate","title":"Performance Comparison Estimate","text":"Setup Expected Accuracy Ankle/Knee Visibility Depth Quality Effort Current (MediaPipe mono) ~10-15\u00b0 angles 18-27% Noisy - Stereo MediaPipe ~5-7\u00b0 angles 50-80% Good Low Pose2Sim ~3-4\u00b0 angles 80-95% Excellent Medium OpenCap ~4-6\u00b0 angles (est) 80-95% Excellent Very Low"},{"location":"research/sports-biomechanics-pose-estimation/#8-best-practices-for-sports-biomechanics","title":"8. Best Practices for Sports Biomechanics","text":""},{"location":"research/sports-biomechanics-pose-estimation/#81-camera-setup","title":"8.1 Camera Setup","text":""},{"location":"research/sports-biomechanics-pose-estimation/#placement","title":"Placement","text":"<ul> <li>Minimum 4 cameras for full 3D reconstruction</li> <li>90\u00b0 separation between adjacent cameras optimal</li> <li>Cover full movement volume from all angles</li> <li>Avoid backlighting (subject silhouettes reduce accuracy)</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#configuration","title":"Configuration","text":"<ul> <li>Resolution: 1080p minimum, higher better</li> <li>Frame rate: Match movement speed</li> <li>Walking/slow: 60 Hz sufficient</li> <li>Running/jumping: 120 Hz</li> <li>Baseball pitch/golf swing: 240 Hz</li> <li>Shutter speed: Fast enough to avoid motion blur</li> <li>Sync: Hardware trigger preferred, software sync acceptable</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#camera-calibration-best-practices","title":"Camera Calibration Best Practices","text":"<ul> <li>Use ChArUco boards (more robust than checkerboard)</li> <li>Calibrate at beginning of each session</li> <li>Move calibration board through capture volume</li> <li>Verify reprojection errors \\&lt; 1 pixel</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#82-data-processing","title":"8.2 Data Processing","text":""},{"location":"research/sports-biomechanics-pose-estimation/#2d-pose-estimation","title":"2D Pose Estimation","text":"<ul> <li>Use highest accuracy settings (slower but better)</li> <li>MediaPipe: <code>model_complexity=2</code>, <code>min_detection_confidence=0.5</code></li> <li>OpenPose: <code>--net_resolution 656x368</code>, <code>--scale_number 4</code></li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#filtering","title":"Filtering","text":"<ul> <li>Low-pass filter: 4th-order Butterworth, 6 Hz cutoff (validated in Pose2Sim)</li> <li>Adjust cutoff based on movement frequency</li> <li>Balance noise removal vs signal preservation</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#biomechanical-modeling","title":"Biomechanical Modeling","text":"<ul> <li>Use OpenSim skeletal model</li> <li>Scale model to subject anthropometry</li> <li>Define joint constraints (angle limits)</li> <li>Verify inverse kinematics residuals</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#83-validation","title":"8.3 Validation","text":""},{"location":"research/sports-biomechanics-pose-estimation/#ground-truth-comparison","title":"Ground Truth Comparison","text":"<ul> <li>Validate against marker-based system (Vicon, Qualisys)</li> <li>Calculate CMC, RMSE, ROM errors</li> <li>Bland-Altman analysis for systematic bias</li> <li>Report 95% limits of agreement</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#quality-metrics","title":"Quality Metrics","text":"<ul> <li>OpenSim IK RMS error \\&lt; 2-4 cm</li> <li>Temporal consistency (smooth trajectories)</li> <li>Physical plausibility (no joint angle violations)</li> <li>Reprojection error \\&lt; 10 pixels</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#9-future-directions","title":"9. Future Directions","text":""},{"location":"research/sports-biomechanics-pose-estimation/#emerging-technologies","title":"Emerging Technologies","text":""},{"location":"research/sports-biomechanics-pose-estimation/#1-foundation-models-for-pose-estimation","title":"1. Foundation Models for Pose Estimation","text":"<ul> <li>Large-scale models trained on diverse data</li> <li>Transfer learning for sport-specific applications</li> <li>Examples: ViTPose, HRNet, TokenPose</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#2-diffusion-models","title":"2. Diffusion Models","text":"<ul> <li>Generative models for pose refinement</li> <li>Can handle severe occlusion</li> <li>Still experimental for biomechanics</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#3-neural-radiance-fields-nerf","title":"3. Neural Radiance Fields (NeRF)","text":"<ul> <li>3D scene reconstruction from images</li> <li>Potential for markerless capture</li> <li>Computationally intensive</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#4-event-cameras","title":"4. Event Cameras","text":"<ul> <li>High temporal resolution (microseconds)</li> <li>Low latency</li> <li>Emerging for sports analysis</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#research-gaps","title":"Research Gaps","text":""},{"location":"research/sports-biomechanics-pose-estimation/#1-sport-specific-validation","title":"1. Sport-Specific Validation","text":"<ul> <li>More validation studies needed per sport</li> <li>High-speed movements understudied</li> <li>Contact sports pose unique challenges</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#2-real-time-processing","title":"2. Real-Time Processing","text":"<ul> <li>Most systems batch processing only</li> <li>Need low-latency for feedback</li> <li>Edge deployment for field use</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#3-outdooruncontrolled-environments","title":"3. Outdoor/Uncontrolled Environments","text":"<ul> <li>Most validation in controlled labs</li> <li>Outdoor lighting challenges</li> <li>Long-distance capture</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#4-multi-person-sports","title":"4. Multi-Person Sports","text":"<ul> <li>Team sports require multi-person tracking</li> <li>Person identity maintenance</li> <li>Interaction analysis</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#10-references","title":"10. References","text":""},{"location":"research/sports-biomechanics-pose-estimation/#key-publications","title":"Key Publications","text":"<p>[1] Pagnon, D., Domalain, M., &amp; Reveret, L. (2022). Pose2Sim: An End-to-End Workflow for 3D Markerless Sports Kinematics\u2014Part 2: Accuracy. Sensors, 22(7), 2712. https://doi.org/10.3390/s22072712</p> <ul> <li>Validation: Qualisys, walking/running/cycling, 3-4\u00b0 joint angle errors</li> <li>Key contribution: Full validated pipeline for sports biomechanics</li> </ul> <p>[2] Dill, S., Ahmadi, A., Grimmer, M., Haufe, D., Rohr, M., Zhao, Y., Sharbafi, M., &amp; Hoog Antink, C. (2024). Accuracy Evaluation of 3D Pose Reconstruction Algorithms Through Stereo Camera Information Fusion for Physical Exercises with MediaPipe Pose. Sensors, 24(23), 7772. https://doi.org/10.3390/s24237772</p> <ul> <li>Validation: Qualisys, squat exercises, 30.1mm RMSE</li> <li>Key contribution: Stereo MediaPipe validation for exercises</li> </ul> <p>[3] Yeung, C., Suzuki, T., Tanaka, R., Yin, Z., &amp; Fujii, K. (2025). AthletePose3D: A Benchmark Dataset for 3D Human Pose Estimation and Kinematic Validation in Athletic Movements. arXiv preprint arXiv:2503.07499. https://arxiv.org/abs/2503.07499</p> <ul> <li>Finding: 69% improvement with sport-specific fine-tuning (214mm \u2192 65mm MPJPE)</li> <li>Key contribution: First large-scale athletic movement dataset</li> </ul> <p>[4] Bazarevsky, V., Grishchenko, I., Raveendran, K., Zhu, T., Zhang, F., &amp; Grundmann, M. (2020). BlazePose: On-device Real-time Body Pose Tracking. arXiv preprint arXiv:2006.10204. https://arxiv.org/abs/2006.10204</p> <ul> <li>Key contribution: MediaPipe Pose architecture</li> </ul> <p>[5] Cao, Z., Hidalgo Martinez, G., Simon, T., Wei, S., &amp; Sheikh, Y. A. (2019). OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(1), 172-186. https://doi.org/10.1109/TPAMI.2019.2929257</p> <ul> <li>Key contribution: Part Affinity Fields for multi-person pose estimation</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#validation-studies-2024-2025","title":"Validation Studies (2024-2025)","text":"<p>[6] Truijen, S., Abdullahi, A., Bijsterbosch, D., van Zoest, E., Conijn, M., Wang, Y., Winds, K., Stoel, R., Struyf, N., Saeys, W., &amp; Jansen, B. (2024). Accuracy, Validity, and Reliability of Markerless Camera-Based 3D Motion Capture Systems versus Marker-Based 3D Motion Capture Systems in Gait Analysis: A Systematic Review and Meta-Analysis. Sensors, 24(11), 3686. https://doi.org/10.3390/s24113686</p> <ul> <li>Key contribution: Systematic review of markerless systems</li> </ul> <p>[7] Needham, L., Evans, M., Cosker, D. P., Wade, L., McGuigan, P. M., Bilzon, J. L., &amp; Colyer, S. L. (2024). Synchronised Video, Motion Capture and Force Plate Dataset for Validating Markerless Human Movement Analysis. Scientific Data, 11, 1281. https://doi.org/10.1038/s41597-024-04077-3</p> <ul> <li>Key contribution: BioCV benchmark dataset for validation</li> </ul> <p>[8] Adlou, B., Wilburn, C., &amp; Weimar, W. (2025). Motion Capture Technologies for Athletic Performance Enhancement and Injury Risk Assessment: A Review for Multi-Sport Organizations. Sensors, 25(14), 4384. https://doi.org/10.3390/s25144384</p> <ul> <li>Key contribution: Comprehensive review for multi-sport organizations</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#comparative-studies","title":"Comparative Studies","text":"<p>[9] Chung, J.-L., Ong, L.-Y., &amp; Leow, M.-C. (2022). Comparative Analysis of Skeleton-Based Human Pose Estimation. Future Internet, 14(12), 380. https://doi.org/10.3390/fi14120380</p> <ul> <li>Comparison: OpenPose, PoseNet, MoveNet, MediaPipe Pose</li> <li>Key contribution: Performance comparison across systems</li> </ul> <p>[10] Needham, L., Evans, M., Cosker, D. P., Wade, L., McGuigan, P. M., Bilzon, J. L., &amp; Colyer, S. L. (2021). The accuracy of several pose estimation methods for 3D joint centre localisation. Scientific Reports, 11, 20673. https://doi.org/10.1038/s41598-021-00212-x</p> <ul> <li>Finding: Task-specific accuracy varies significantly</li> <li>Key contribution: Highlighted importance of task-specific validation</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#opensim-resources","title":"OpenSim Resources","text":"<p>[11] Delp, S. L., Anderson, F. C., Arnold, A. S., Loan, P., Habib, A., John, C. T., Guendelman, E., &amp; Thelen, D. G. (2007). OpenSim: Open-Source Software to Create and Analyze Dynamic Simulations of Movement. IEEE Transactions on Biomedical Engineering, 54(11), 1940-1950. https://doi.org/10.1109/TBME.2007.901024</p> <ul> <li>Key contribution: OpenSim biomechanical modeling framework</li> </ul> <p>[12] Rajagopal, A., Dembia, C. L., DeMers, M. S., Delp, D. D., Hicks, J. L., &amp; Delp, S. L. (2016). Full-Body Musculoskeletal Model for Muscle-Driven Simulation of Human Gait. IEEE Transactions on Biomedical Engineering, 63(10), 2068-2079. https://doi.org/10.1109/TBME.2016.2586891</p> <ul> <li>Key contribution: Full-body gait model used in Pose2Sim</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#opencap-resources","title":"OpenCap Resources","text":"<p>[13] Uhlrich, S. D., Falisse, A., Kidzi\u0144ski, \u0141., Muccini, J., Ko, M., Chaudhari, A. S., Hicks, J. L., &amp; Delp, S. L. (2023). OpenCap: Human movement dynamics from smartphone videos. PLOS Computational Biology, 19(10), e1011462. https://doi.org/10.1371/journal.pcbi.1011462</p> <ul> <li>Key contribution: Smartphone-based accessible biomechanics</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#commercial-system-validation","title":"Commercial System Validation","text":"<p>[14] Kanko, R. M., Laende, E. K., Davis, E. M., Selbie, W. S., &amp; Deluzio, K. J. (2021). Concurrent assessment of gait kinematics using marker-based and markerless motion capture. Journal of Biomechanics, 127, 110665. https://doi.org/10.1016/j.jbiomech.2021.110665</p> <ul> <li>System: Theia3D validation</li> <li>Key contribution: Commercial markerless validation</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#11-software-resources","title":"11. Software &amp; Resources","text":""},{"location":"research/sports-biomechanics-pose-estimation/#open-source-tools","title":"Open-Source Tools","text":""},{"location":"research/sports-biomechanics-pose-estimation/#pose2sim","title":"Pose2Sim","text":"<ul> <li>GitHub: https://github.com/perfanalytics/pose2sim</li> <li>Documentation: https://pose2sim.readthedocs.io/</li> <li>License: BSD 3-Clause</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#opencap","title":"OpenCap","text":"<ul> <li>Website: https://www.opencap.ai/</li> <li>GitHub: https://github.com/stanfordnmbl/opencap-core</li> <li>License: Apache 2.0</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#athletepose3d","title":"AthletePose3D","text":"<ul> <li>GitHub: https://github.com/calvinyeungck/athletepose3d</li> <li>Paper: https://arxiv.org/abs/2503.07499</li> <li>License: Non-commercial research only</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#mediapipe","title":"MediaPipe","text":"<ul> <li>GitHub: https://github.com/google/mediapipe</li> <li>Documentation: https://developers.google.com/mediapipe</li> <li>License: Apache 2.0</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#openpose","title":"OpenPose","text":"<ul> <li>GitHub: https://github.com/CMU-Perceptual-Computing-Lab/openpose</li> <li>License: Academic/commercial licensing</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#opensim","title":"OpenSim","text":"<ul> <li>Website: https://opensim.stanford.edu/</li> <li>GitHub: https://github.com/opensim-org/opensim-core</li> <li>License: Apache 2.0</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#commercial-solutions","title":"Commercial Solutions","text":"<ul> <li>Theia3D: https://www.theiamarkerless.ca/</li> <li>Vald Performance: https://www.valdperformance.com/</li> <li>Simi Motion: https://www.simi.com/</li> <li>Contemplas: https://www.contemplas.com/</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#12-glossary","title":"12. Glossary","text":"<p>CMC (Coefficient of Multiple Correlation): Statistical measure of waveform similarity that jointly evaluates correlation, gain, and offset. Values &gt;0.95 indicate excellent agreement.</p> <p>MPJPE (Mean Per Joint Position Error): Average Euclidean distance between estimated and ground truth 3D joint positions.</p> <p>ROM (Range of Motion): Angular excursion of a joint during movement.</p> <p>RMSE (Root Mean Square Error): Square root of the mean squared differences between estimated and ground truth values.</p> <p>Inverse Kinematics (IK): Computational method to determine joint angles given endpoint positions, enforcing skeletal constraints.</p> <p>Triangulation: Computing 3D coordinates from 2D projections in multiple camera views using epipolar geometry.</p> <p>Gold Standard: Marker-based motion capture systems (Vicon, Qualisys, OptiTrack) with sub-millimeter accuracy.</p> <p>Sagittal Plane: Anatomical plane dividing body into left and right halves (flexion/extension movements).</p> <p>Frontal Plane: Anatomical plane dividing body into front and back (abduction/adduction).</p> <p>Transverse Plane: Anatomical plane dividing body into upper and lower (rotation).</p>"},{"location":"research/sports-biomechanics-pose-estimation/#document-history","title":"Document History","text":"<ul> <li>Version 1.0 - November 2025: Initial documentation based on 2022-2025 research</li> <li>Research Date: November 2025 (sources through July 2025)</li> <li>Author: Research synthesis for kinemotion project</li> <li>Last Updated: November 6, 2025</li> </ul>"},{"location":"research/sports-biomechanics-pose-estimation/#conclusion","title":"Conclusion","text":"<p>For sports biomechanics applications requiring accurate joint angle measurements:</p> <ol> <li>Use Pose2Sim [1] for research-grade accuracy (3-4\u00b0 errors)</li> <li>Multi-camera setup is essential (minimum 4 cameras, 90\u00b0 separation optimal [1])</li> <li>Biomechanical constraints required (OpenSim skeletal modeling [11])</li> <li>Validate against gold standard (marker-based motion capture [7])</li> <li>Sport-specific training helps (AthletePose3D fine-tuning [3])</li> </ol> <p>The field is rapidly advancing, but Pose2Sim [1] currently represents the best-validated open-source solution for sports biomechanics research, with published validation against gold-standard motion capture systems. Recent developments in sport-specific datasets [3] and stereo camera validation [2] continue to improve accessibility and accuracy of markerless systems for athletic applications [8].</p>"},{"location":"technical/framerate/","title":"Frame Rate Guide for Drop Jump Analysis","text":"<p>\u26a0\ufe0f Important: This document discusses frame rate effects on video-based jump analysis in general. kinemotion's actual accuracy is currently unvalidated - accuracy claims are theoretical until empirical testing is completed.</p> <p>This document explains how video frame rate affects accuracy in drop jump analysis and provides recommendations for different use cases.</p>"},{"location":"technical/framerate/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Executive Summary</li> <li>Frame Rate Impact on Accuracy</li> <li>Temporal Resolution Analysis</li> <li>Practical Recommendations</li> <li>Parameter Adjustments by Frame Rate</li> <li>Accuracy Bottlenecks Beyond Frame Rate</li> <li>Cost-Benefit Analysis</li> <li>Limitations &amp; Research Gaps</li> <li>Evidence-Based Summary &amp; Conclusions</li> <li>kinemotion-Specific Recommendations</li> </ul>"},{"location":"technical/framerate/#executive-summary","title":"Executive Summary","text":"<p>\u26a0\ufe0f Critical Notice: kinemotion accuracy is currently unvalidated. The following represents theoretical considerations, not empirically verified performance.</p> <p>TL;DR:</p> <ul> <li>kinemotion accuracy unknown - requires validation studies</li> <li>60 fps appears adequate for most applications based on industry data</li> <li>30 fps may be sufficient with proper uncertainty documentation</li> <li>120+ fps benefits unclear without kinemotion-specific validation</li> <li>Validation priority over theoretical improvements</li> </ul> <p>Key Finding: Industry research shows diminishing returns from higher frame rates, but kinemotion's actual performance characteristics are unknown until empirical validation is completed.</p>"},{"location":"technical/framerate/#frame-rate-impact-on-accuracy","title":"Frame Rate Impact on Accuracy","text":""},{"location":"technical/framerate/#timing-precision-by-frame-rate","title":"Timing Precision by Frame Rate","text":"<p>Sub-frame interpolation (enabled by default) provides sub-millisecond timing precision:</p> Frame Rate Time Between Frames Precision with Interpolation Precision without Interpolation 30 fps 33.3 ms \u00b110 ms \u00b133 ms 60 fps 16.7 ms \u00b15 ms \u00b117 ms 120 fps 8.3 ms \u00b12.5 ms \u00b18 ms 240 fps 4.2 ms \u00b11.25 ms \u00b14 ms 480 fps 2.1 ms \u00b10.6 ms \u00b12 ms <p>How sub-frame interpolation works:</p> <ul> <li>Calculates smooth velocity curve using Savitzky-Golay derivative</li> <li>Finds exact threshold crossing between frames using linear interpolation</li> <li>Returns fractional frame indices (e.g., 48.73 instead of 49)</li> <li>Reduces timing error by 60-70% compared to integer frame boundaries</li> </ul> <p>Math example at 30fps:</p> <pre><code>Without interpolation:\n- Event occurs at frame 48.7\n- Detected at frame 49 (integer boundary)\n- Error: 0.3 frames \u00d7 33.3ms/frame = 10ms error\n- Worst case: \u00b133ms (full frame)\n\nWith interpolation:\n- Event detected at frame 48.73 (fractional)\n- Error: 0.03 frames \u00d7 33.3ms/frame = 1ms error\n- Typical error: \u00b110ms (residual from velocity smoothness)\n```text\n\n### Current Kinemotion Accuracy Status\n\n**\u26a0\ufe0f Important:** **Kinemotion accuracy is currently unvalidated**. No peer-reviewed studies exist comparing kinemotion outputs to gold standards.\n\n### What We Don't Know\n\n**Actual kinemotion accuracy metrics:**\n\n- **30fps accuracy**: Unknown (requires validation)\n- **60fps accuracy**: Unknown (requires validation)\n- **Systematic errors**: Unknown (requires validation)\n- **Precision**: Unknown (requires validation)\n\n**Previous Document Claims (Unverified):**\n\n- ~~88% accuracy at 30fps~~ - *No validation data*\n- ~~90-91% accuracy at 60fps~~ - *No validation data*\n- ~~Specific error percentages~~ - *Theoretical estimates only*\n\n### Industry Reference Data (Not kinemotion-specific)\n\nFrame rate studies from other systems (not kinemotion):\n\n| Frame Rate | Reference System Error* | Evidence Source |\n|------------|------------------------|------------------|\n| **120 fps** | 1.4% vs 1000Hz reference | PMC10108745 (n=10) |\n| **240 fps** | 0.7% vs 1000Hz reference | PMC10108745 (n=10) |\n\n*These are from other video analysis systems, not kinemotion validation\n\n**Conclusion:** Actual kinemotion accuracy can only be determined through empirical validation studies.\n\n### MediaPipe Pose Estimation Validation\n\n**Detection Accuracy:**\n\n- **95.24-99.02%** for event detection (heel strike/toe-off) vs Vicon gold standard\n- **Temporal errors: 20-50ms** for gait parameters\n- **Strong correlation:** r = 0.992 vs force plates\n- **High reliability:** ICC &gt; 0.9 for temporal variables\n\n**Jump Height Measurement:**\n\n- **4.8-6.2% systematic overestimation** vs 3D marker-based systems\n- **Correlation:** r &gt; 0.98 for most kinematic variables\n\n**Note:** These validation studies used controlled laboratory conditions and may not translate directly to field applications.\n\n---\n\n## Temporal Resolution Analysis\n\n### Contact Detection Reliability\n\nBrief ground contacts benefit most from high frame rates. Minimum contact frames filter (`--min-contact-frames`) must capture enough samples to confirm contact:\n\n#### Example: 100ms ground contact (brief reactive jump)\n\n| Frame Rate | Frames Captured | Detection Reliability | Recommended `--min-contact-frames` |\n|------------|----------------|----------------------|-----------------------------------|\n| **30 fps** | ~3 frames | \u26a0\ufe0f Marginal (requires `--min-contact-frames 2-3`) | 2-3 |\n| **60 fps** | ~6 frames | \u2705 Reliable (default `--min-contact-frames 3` \u00d7 2) | 4-6 |\n| **120 fps** | ~12 frames | \u2705 Very robust | 8-12 |\n| **240 fps** | ~24 frames | \u2705 Excessive (overkill) | 16-24 |\n\n#### Example: 250ms ground contact (typical drop jump)\n\n| Frame Rate | Frames Captured | Detection Reliability | Recommended `--min-contact-frames` |\n|------------|----------------|----------------------|-----------------------------------|\n| **30 fps** | ~7-8 frames | \u2705 Reliable | 3-5 |\n| **60 fps** | ~15 frames | \u2705 Very robust | 6-10 |\n| **120 fps** | ~30 frames | \u2705 Excessive | 12-20 |\n| **240 fps** | ~60 frames | \u2705 Excessive (overkill) | 24-40 |\n\n**Rule of thumb:** Frame rate becomes less critical as contact time increases.\n\n### Velocity Calculation Smoothness\n\nMore temporal samples = smoother velocity derivative:\n\n**30 fps:**\n\n- Adequate for Savitzky-Golay smoothing (5-frame window = 167ms)\n- Occasional noise spikes in velocity\n- Sub-frame interpolation compensates effectively\n- \u2705 Acceptable for most applications\n\n**60 fps:**\n\n- Noticeably smoother velocity curves (5-frame window = 83ms)\n- Fewer false threshold crossings\n- Better acceleration pattern detection for curvature analysis\n- \u2705 **Recommended** for consistent results\n\n**120 fps:**\n\n- Very smooth velocity curves (5-frame window = 42ms)\n- Minimal noise in derivatives\n- Marginal improvement over 60fps in practice\n- \u26a0\ufe0f Diminishing returns begin\n\n**240+ fps:**\n\n- Extremely smooth curves but no practical benefit\n- Accuracy limited by tracking quality, not sampling rate\n- \u274c Processing overhead outweighs gains\n\n### MediaPipe Tracking Quality\n\n**Frame rate affects pose tracking in two ways:**\n\n1. **Inter-frame motion** (lower is better):\n   - 30 fps: Larger motion between frames, harder to track\n   - 60 fps: Smaller motion between frames, more stable tracking\n   - 120+ fps: Minimal inter-frame motion, excellent stability\n\n2. **Processing overhead** (higher is worse):\n   - 30 fps: Baseline processing time\n   - 60 fps: 2\u00d7 processing time (linear scaling)\n   - 120 fps: 4\u00d7 processing time\n   - 240 fps: 8\u00d7 processing time\n\n**Tracking quality improvement is logarithmic:**\n\n- 30\u219260 fps: Noticeable improvement in landmark stability\n- 60\u2192120 fps: Small improvement in stability\n- 120\u2192240 fps: Negligible improvement\n\n---\n\n## Practical Recommendations\n\n### 30 fps - Minimum Acceptable \u2699\ufe0f\n\n**Best for:**\n\n- Quick exploratory analysis\n- Storage/bandwidth constraints\n- Longer drop jump contacts (&gt;200ms)\n- Non-critical measurements\n\n**Advantages:**\n\n- \u2705 Small file sizes (~500MB for 5min at 1080p)\n- \u2705 Fast processing (baseline)\n- \u2705 Sub-frame interpolation provides \u00b110ms precision\n- \u2705 Adequate for most drop jump scenarios\n\n**Limitations:**\n\n- \u26a0\ufe0f May struggle with very reactive jumps (&lt;150ms contact)\n- \u26a0\ufe0f Parameter tuning critical (`--min-contact-frames`, `--velocity-threshold`)\n- \u26a0\ufe0f More susceptible to tracking glitches\n- \u26a0\ufe0f Less robust velocity calculations\n\n**Recommended parameters:**\n\n```bash\nkinemotion dropjump-analyze video_30fps.mp4 \\\n  --smoothing-window 5 \\\n  --velocity-threshold 0.02 \\\n  --min-contact-frames 3\n```\n\n**Note:** Auto-tuning handles these parameters automatically. Manual overrides shown for illustration.\n\n---\n\n### 60 fps - Sweet Spot \u2b50 (Recommended)\n\n**Best for:**\n\n- Performance analysis and athlete monitoring\n- Research requiring good accuracy\n- Most drop jump scenarios\n- Balance of quality and practicality\n\n**Advantages:**\n\n- \u2705 \u00b15ms timing precision (excellent)\n- \u2705 Robust detection of brief contacts\n- \u2705 Smooth velocity curves\n- \u2705 Better tracking stability\n- \u2705 Reasonable file sizes (~1GB for 5min at 1080p)\n- \u2705 Best accuracy/cost trade-off\n\n**Limitations:**\n\n- \u26a0\ufe0f 2\u00d7 processing time vs 30fps\n- \u26a0\ufe0f 2\u00d7 storage requirements\n- \u26a0\ufe0f Still some noise in high-speed movements\n\n**Recommended parameters:**\n\n```bash\nkinemotion dropjump-analyze video_60fps.mp4 \\\n  --smoothing-window 5 \\\n  --velocity-threshold 0.01 \\      # halve (less motion per frame)\n  --min-contact-frames 6          # double (same time duration)\n```\n\n**Expected accuracy:** ~90-91% with calibration (+2-3% over 30fps)\n\n**\ud83d\udca1 Recommendation:** If you can only choose one upgrade, **use 60fps video** - it provides the best return on investment for accuracy.\n\n---\n\n### 120 fps - Diminishing Returns \ud83d\udd2c\n\n**Best for:**\n\n- Research requiring &lt;5ms timing precision\n- Analysis of explosive/reactive movements\n- High-speed biomechanics research\n- Validation studies against force plates\n\n**Advantages:**\n\n- \u2705 \u00b12.5ms timing precision\n- \u2705 Very robust brief contact detection\n- \u2705 Excellent velocity curve smoothness\n- \u2705 Captures rapid transitions accurately\n\n**Limitations:**\n\n- \u26a0\ufe0f 4\u00d7 processing time vs 30fps (2\u00d7 vs 60fps)\n- \u26a0\ufe0f Large file sizes (~2GB for 5min at 1080p)\n- \u26a0\ufe0f Only +1% accuracy over 60fps (marginal gain)\n- \u26a0\ufe0f Requires proportional parameter adjustments\n- \u26a0\ufe0f Other factors become limiting (tracking, calibration)\n\n**Recommended parameters:**\n\n```bash\nkinemotion dropjump-analyze video_120fps.mp4 \\\n  --smoothing-window 5 \\\n  --velocity-threshold 0.005 \\     # quarter (4\u00d7 more frames)\n  --min-contact-frames 12         # quadruple\n```\n\n**Expected accuracy:** ~91-92% with calibration (+3-4% over 30fps, +1% over 60fps)\n\n**\ud83d\udca1 Note:** Only pursue 120fps if 60fps accuracy is insufficient for your application and you've already maximized calibration quality.\n\n---\n\n### 240 fps - Overkill for Drop Jumps \u274c\n\n**Best for:**\n\n- Ultra-high-speed research (e.g., ballistic movements)\n- Special applications requiring sub-2ms precision\n- Validation of measurement systems\n\n**Advantages:**\n\n- \u2705 \u00b11.25ms timing precision (theoretical)\n- \u2705 Maximum temporal resolution\n- \u2705 Captures finest motion details\n\n**Limitations:**\n\n- \u274c 8\u00d7 processing time vs 30fps (4\u00d7 vs 60fps)\n- \u274c Massive file sizes (~4GB for 5min at 1080p)\n- \u274c Only +0.5% accuracy over 120fps (imperceptible)\n- \u274c Accuracy limited by other factors:\n  - MediaPipe tracking precision (~5-10ms equivalent)\n  - Motion blur (limits effective temporal resolution)\n  - Calibration precision (drop height measurement)\n  - 2D vs 3D motion assumptions\n- \u274c Requires specialized high-speed cameras\n- \u274c Difficult to achieve good lighting at 240fps\n\n**Recommended parameters:**\n\n```bash\nkinemotion dropjump-analyze video_240fps.mp4 \\\n  --smoothing-window 5 \\\n  --velocity-threshold 0.0025 \\    # 1/8\u00d7 (8\u00d7 more frames)\n  --min-contact-frames 24         # 8\u00d7\n```\n\n**Expected accuracy:** ~92-93% with calibration (+4-5% over 30fps, +1-2% over 60fps, +0.5% over 120fps)\n\n**\ud83d\udca1 Verdict:** Not recommended for drop jumps - other factors become limiting before frame rate at this level.\n\n---\n\n## Parameter Adjustments by Frame Rate\n\n### Proportional Scaling Rules\n\nWhen changing frame rate, adjust parameters proportionally to maintain equivalent behavior:\n\n#### Rule 1: Velocity threshold scales inversely with FPS\n\n```text\nthreshold_new = threshold_30fps \u00d7 (30 / fps_new)\n\nExamples:\n30 fps \u2192 60 fps: 0.02 \u2192 0.01 (halve)\n30 fps \u2192 120 fps: 0.02 \u2192 0.005 (quarter)\n30 fps \u2192 240 fps: 0.02 \u2192 0.0025 (1/8\u00d7)\n```text\n\n**Explanation:** Higher FPS = less motion per frame, so lower threshold needed to detect same velocity.\n\n#### Rule 2: Minimum contact frames scales linearly with FPS\n\n```text\nmin_frames_new = min_frames_30fps \u00d7 (fps_new / 30)\n\nExamples:\n30 fps \u2192 60 fps: 3 \u2192 6 (double)\n30 fps \u2192 120 fps: 3 \u2192 12 (quadruple)\n30 fps \u2192 240 fps: 3 \u2192 24 (8\u00d7)\n```text\n\n**Explanation:** To capture the same minimum contact time duration, need proportionally more frames.\n\n#### Rule 3: Smoothing window can stay constant (or adjust slightly)\n\n```text\n# Keep temporal duration constant\nsmoothing_window_new = smoothing_window_30fps \u00d7 (fps_new / 30)\n\n# OR keep frame count constant for similar smoothing\nsmoothing_window_new = smoothing_window_30fps\n\nExamples (constant duration):\n30 fps with window=5 \u2192 167ms temporal window\n60 fps with window=10 \u2192 167ms temporal window (same duration)\n120 fps with window=20 \u2192 167ms temporal window (same duration)\n\nExamples (constant frames, recommended):\n30 fps with window=5 \u2192 5 frames\n60 fps with window=5 \u2192 5 frames (less temporal duration, more samples)\n120 fps with window=5 \u2192 5 frames\n```text\n\n**Recommendation:** Keep smoothing window at 5-7 frames regardless of FPS for best results.\n\n### Complete Parameter Sets by Frame Rate\n\n#### 30 fps baseline\n\n```bash\nkinemotion dropjump-analyze video_30fps.mp4 \\\n  --smoothing-window 5 \\\n  --polyorder 2 \\\n  --velocity-threshold 0.02 \\\n  --min-contact-frames 3 \\\n  --visibility-threshold 0.5\n```\n\n#### 60 fps (2\u00d7 frames)\n\n```bash\nkinemotion dropjump-analyze video_60fps.mp4 \\\n  --smoothing-window 5 \\          # same (or 10 for constant duration)\n  --polyorder 2 \\                 # same\n  --velocity-threshold 0.01 \\     # halve (2\u00d7 more frames)\n  --min-contact-frames 6 \\        # double (2\u00d7 more frames)\n  --visibility-threshold 0.5     # same\n```\n\n#### 120 fps (4\u00d7 frames)\n\n```bash\nkinemotion dropjump-analyze video_120fps.mp4 \\\n  --smoothing-window 5 \\          # same (or 20 for constant duration)\n  --polyorder 2 \\                 # same\n  --velocity-threshold 0.005 \\    # quarter (4\u00d7 more frames)\n  --min-contact-frames 12 \\       # quadruple (4\u00d7 more frames)\n  --visibility-threshold 0.5     # same\n```\n\n#### 240 fps (8\u00d7 frames)\n\n```bash\nkinemotion dropjump-analyze video_240fps.mp4 \\\n  --smoothing-window 5 \\          # same (or 40 for constant duration)\n  --polyorder 2 \\                 # same\n  --velocity-threshold 0.0025 \\   # 1/8\u00d7 (8\u00d7 more frames)\n  --min-contact-frames 24 \\       # 8\u00d7 (8\u00d7 more frames)\n  --visibility-threshold 0.5     # same\n```\n\n### Auto-Detecting Frame Rate (Future Enhancement)\n\nCurrently, you must manually specify parameters based on FPS. A future enhancement could auto-detect FPS and adjust parameters:\n\n```python\n# Pseudo-code for future feature\nfps = video.get(cv2.CAP_PROP_FPS)\nscaling_factor = fps / 30.0\n\nvelocity_threshold = 0.02 / scaling_factor\nmin_contact_frames = int(3 * scaling_factor)\n```text\n\n---\n\n## Accuracy Bottlenecks Beyond Frame Rate\n\nAt high frame rates (120+ fps), other factors become limiting:\n\n### 1. MediaPipe Tracking Precision\n\n**Tracking resolution:** ~1-2 pixels per landmark in 1080p video\n\n```text\nExample: 1 pixel error in 1080p frame\n\u2192 1/1080 = 0.0009 normalized units\n\u2192 ~1mm real-world error with good calibration\n\u2192 Equivalent to ~5-10ms timing error at typical jump velocities\n```text\n\n**Impact:** Even with 240fps, tracking precision limits effective accuracy to ~5-10ms, making frame rate improvements beyond 60fps marginal.\n\n### 2. Camera Motion Blur\n\n**Exposure time creates motion blur:**\n\n```text\n30 fps \u2192 typical exposure: 1/60s (16.7ms)\n60 fps \u2192 typical exposure: 1/120s (8.3ms)\n120 fps \u2192 typical exposure: 1/240s (4.2ms)\n240 fps \u2192 typical exposure: 1/480s (2.1ms)\n```text\n\n**Motion blur limits effective temporal resolution:**\n\n- At 30fps with 1/60s exposure: landmarks \"smeared\" over ~1.5 frames\n- At 240fps with 1/480s exposure: landmarks sharp, minimal blur\n- **But:** Pose tracking already introduces ~1-2 pixel uncertainty (5-10ms)\n\n**Conclusion:** Beyond 60fps, reduced motion blur provides minimal practical benefit given tracking limitations.\n\n### 3. Calibration Accuracy\n\n**Drop height measurement precision:**\n\n```text\n\u00b11cm error in 40cm drop height measurement\n\u2192 \u00b12.5% calibration error\n\u2192 \u00b12.2% jump height error (propagates)\n\u2192 Equivalent to \u00b18mm error on 35cm jump\n\nFrame rate improvement from 60\u2192240fps:\n\u2192 ~\u00b11% timing improvement\n\u2192 \u00b13.5mm jump height improvement\n\nConclusion: Calibration accuracy dominates over frame rate\n```text\n\n**Impact:** Improving drop height measurement from \u00b11cm to \u00b12mm has greater effect than upgrading from 60fps to 240fps.\n\n### 4. Out-of-Plane Motion\n\n**2D video captures only one plane:**\n\n```text\nAthlete moves forward/backward during jump:\n\u2192 Foot appears higher/lower than actual\n\u2192 Creates systematic measurement error\n\u2192 Not improved by higher frame rate\n```text\n\n**Typical error:** \u00b15-10mm from out-of-plane motion\n**Impact:** Comparable to timing errors at 60fps; frame rate doesn't address this\n\n### 5. Athlete Movement Variability\n\n**Human movement is inherently variable:**\n\n```text\nTypical jump-to-jump variability:\n- Ground contact time: \u00b110-20ms\n- Flight time: \u00b15-15ms\n- Jump height: \u00b12-5cm\n\nMeasurement precision required:\n\u2192 ~5-10ms timing precision (met by 60fps)\n\u2192 Sub-millisecond precision unnecessary\n```text\n\n**Conclusion:** Beyond 60fps, measurement precision exceeds athlete repeatability.\n\n---\n\n## Cost-Benefit Analysis\n\n### Kinemotion Development Priorities\n\n**Based on current validation gaps:**\n\n1. **Validation Planning** (High Priority)\n   - Design validation study against force plates\n   - Develop uncertainty quantification methods\n   - Create testing protocols for different conditions\n\n2. **Quality Assurance** (Medium Priority)\n   - Unit testing of accuracy-critical components\n   - Error analysis for systematic biases\n   - Performance benchmarking\n\n3. **User Documentation** (Medium Priority)\n   - Clearly communicate accuracy limitations\n   - Provide uncertainty guidelines\n   - Document appropriate use cases\n\n4. **Future Research** (Low Priority)\n   - Frame rate optimization after validation\n   - Algorithm improvements based on measured performance\n   - Feature expansion based on user needs\n\n**Key Finding:** Priorities should focus on **validation and testing** rather than theoretical accuracy improvements.\n\n### Processing Time vs Frame Rate\n\n**Relative processing time** (assuming 5-minute video, 1080p):\n\n| Frame Rate | Frames to Process | Relative Time | Absolute Time* |\n|------------|------------------|--------------|----------------|\n| **30 fps** | 9,000 | 1\u00d7 | ~2 minutes |\n| **60 fps** | 18,000 | 2\u00d7 | ~4 minutes |\n| **120 fps** | 36,000 | 4\u00d7 | ~8 minutes |\n| **240 fps** | 72,000 | 8\u00d7 | ~16 minutes |\n\n*Approximate times on M1 MacBook Pro (actual times vary by hardware)\n\n**Storage requirements** (5-minute video, 1080p, H.264):\n\n| Frame Rate | File Size | Storage for 100 videos |\n|------------|-----------|----------------------|\n| **30 fps** | ~500 MB | ~50 GB |\n| **60 fps** | ~1 GB | ~100 GB |\n| **120 fps** | ~2 GB | ~200 GB |\n| **240 fps** | ~4-6 GB | ~400-600 GB |\n\n### Return on Investment\n\n**30 fps \u2192 60 fps:**\n\n- **Cost:** 2\u00d7 storage, 2\u00d7 processing time, may need better camera ($0-500)\n- **Benefit:** +2-3% accuracy, more robust detection, better tracking\n- **ROI:** \u2b50\u2b50\u2b50\u2b50\u2b50 **Excellent** - recommended upgrade\n\n**60 fps \u2192 120 fps:**\n\n- **Cost:** 2\u00d7 storage, 2\u00d7 processing time, high-speed camera ($500-2000)\n- **Benefit:** +1% accuracy, marginal robustness improvement\n- **ROI:** \u2b50\u2b50 **Marginal** - only for research applications\n\n**120 fps \u2192 240 fps:**\n\n- **Cost:** 2\u00d7 storage, 2\u00d7 processing time, pro high-speed camera ($2000-10000)\n- **Benefit:** +0.5% accuracy, no practical improvement\n- **ROI:** \u2b50 **Poor** - not recommended for drop jumps\n\n---\n\n## Limitations &amp; Research Gaps\n\n### Evidence Limitations\n\n**Sample Size Issues:**\n\n- Most validation studies use small samples (n=10-12)\n- Limited demographic diversity (young adults, athletes)\n- Short-term controlled environments\n\n**Methodological Gaps:**\n\n- **Drop jump specific validation lacking** - most research on CMJ or gait\n- **Field vs laboratory conditions** - limited real-world validation\n- **Camera variety** - most studies use specific camera setups\n- **Standardized protocols** - no consensus on best practices\n\n### Recommended Research Priorities\n\n1. **Comprehensive drop jump validation** across frame rates\n2. **Field testing** with various camera setups and conditions\n3. **Standardized accuracy metrics** for video-based jump analysis\n4. **Cross-validation studies** between different pose estimation systems\n5. **Cost-benefit analysis** with real-world performance data\n\n### Practical Recommendations for kinemotion\n\n**For Current Users:**\n\n- **30fps/60fps** currently adequate for most applications\n- **Document uncertainty** in all measurements\n- **Consider systematic errors** potentially present in pose estimation\n- **Report limitations** when sharing results\n\n**For Development:**\n\n- **Focus on validation** before optimization\n- **Test against gold standards** (force plates, 3D systems)\n- **Quantify uncertainty** of all measurements\n- **Conduct field testing** in real conditions\n\n## Evidence-Based Summary &amp; Conclusions\n\n**\u26a0\ufe0f Critical Limitations:** Many accuracy claims in this field lack comprehensive peer-reviewed validation. The following conclusions are based on limited available evidence:\n\n### Current Limitations\n\n**Kinemotion-specific unknowns:**\n\n- **No validation studies** exist comparing kinemotion to force plates\n- **Actual accuracy metrics** unknown (not theoretically estimated)\n- **Real-world performance** untested\n\n**MediaPipe limitations (potential kinemotion limitations):**\n\n- **Temporal errors:** 20-50ms in pose estimation systems\n- **Systematic bias:** 4.8-6.2% overestimation vs 3D systems\n- **Detection accuracy:** 95.24-99.02% for timing events\n- **Strong correlation:** r = 0.992 vs force plates\n\n**Key Knowledge Gaps:**\n\n- kinemotion's **actual accuracy** vs gold standards\n- **Frame rate impact** on kinemotion specifically\n- **Calibration effectiveness** for kinemotion algorithms\n- **Field performance** vs laboratory conditions\n\n### Required Research\n\n1. **Kinemotion validation study** against force plates\n2. **Frame rate testing** with kinemotion specifically\n3. **Field validation** of kinemotion in real conditions\n4. **Error analysis** of kinemotion's systematic biases\n5. **Uncertainty quantification** of kinemotion measurements\n\n**Bottom Line:** kinemotion accuracy claims are **currently theoretical** - empirical validation is required to determine real performance.\n\n---\n\n## kinemotion-Specific Recommendations\n\n**Current Status:**\n\n- **No validation studies** exist for kinemotion specifically\n- **Accuracy unknown** - requires empirical testing\n- **Focus on reliability** rather than theoretical improvements\n\n**Immediate Actions:**\n\n1. **Plan validation study** against force plates or 3D motion capture\n2. **Document current limitations** in user-facing materials\n3. **Implement uncertainty quantification** for all measurements\n4. **Test systematic biases** across different conditions\n\n**Decision Framework:**\n\n```text\nNeed precise measurements for critical applications?\n\u251c\u2500 Yes \u2192 Conduct validation study first\n\u2514\u2500 No \u2192 Use with caution, document uncertainty\n```text\n\n**Bottom Line:** kinemotion is currently **unvalidated software** - accuracy claims are theoretical until empirical validation is completed.\n</code></pre>"},{"location":"technical/imu-metadata/","title":"IMU Data and Video Editing Metadata Preservation","text":""},{"location":"technical/imu-metadata/#what-is-imu-data","title":"What is IMU Data?","text":"<p>IMU (Inertial Measurement Unit) sensors in smartphones include:</p>"},{"location":"technical/imu-metadata/#accelerometer","title":"Accelerometer","text":"<ul> <li>Measures linear acceleration in 3 axes (x, y, z)</li> <li>Captures device movement forces including gravity</li> <li>Provides direct measurement of acceleration during motion</li> </ul>"},{"location":"technical/imu-metadata/#gyroscope","title":"Gyroscope","text":"<ul> <li>Measures angular velocity (rotation speed) around 3 axes</li> <li>Captures device orientation changes</li> <li>Helps distinguish gravity vector from actual acceleration</li> </ul>"},{"location":"technical/imu-metadata/#how-iphones-store-imu-data","title":"How iPhones Store IMU Data","text":"<p>iPhone videos embed IMU sensor data in extended metadata tracks within the MOV container:</p> <ul> <li>Location: Stored as additional data streams alongside video/audio</li> <li>Format: Proprietary Apple metadata atoms</li> <li>Timestamps: Synchronized with video frames for precise alignment</li> <li>Access: Extractable via <code>ffprobe</code>, MediaInfo, or specialized tools</li> </ul>"},{"location":"technical/imu-metadata/#video-editing-impact-on-imu-data","title":"Video Editing Impact on IMU Data","text":""},{"location":"technical/imu-metadata/#operations-that-destroy-imu-data","title":"\u274c Operations That Destroy IMU Data","text":"<p>Re-encoding processes:</p> <ul> <li>Photos app trimming/splitting</li> <li>Any video compression or format conversion that re-compresses video</li> <li>Online video converters</li> <li>Social media uploads (compress video)</li> </ul> <p>Why lost: Re-encoding creates a new video file, only preserving standard metadata (creation date, GPS, camera settings). Extended sensor data tracks are stripped.</p>"},{"location":"technical/imu-metadata/#operations-that-preserve-imu-data","title":"\u2705 Operations That Preserve IMU Data","text":"<p>Container remuxing (stream copying):</p> <ul> <li>No re-compression of video/audio streams</li> <li>Metadata streams copied unchanged</li> <li>File container changes without touching contents</li> </ul>"},{"location":"technical/imu-metadata/#safe-video-processing-methods","title":"Safe Video Processing Methods","text":""},{"location":"technical/imu-metadata/#container-format-conversion-mov-mp4","title":"Container Format Conversion (MOV \u2192 MP4)","text":"<pre><code># Preserves all metadata including IMU\nffmpeg -i input.mov -c copy -map 0 output.mp4\n</code></pre>"},{"location":"technical/imu-metadata/#video-trimming-without-re-encoding","title":"Video Trimming Without Re-encoding","text":"<pre><code># Trim while preserving all metadata\nffmpeg -i input.mov -ss 00:00:05 -t 00:00:10 -c copy segment.mov\n</code></pre>"},{"location":"technical/imu-metadata/#professional-video-editors","title":"Professional Video Editors","text":"<p>Tools that preserve extended metadata:</p> <ul> <li>LumaFusion (iOS)</li> <li>Adobe Premiere Rush</li> <li>DaVinci Resolve (desktop)</li> <li>Final Cut Pro</li> </ul>"},{"location":"technical/imu-metadata/#testing-imu-data-preservation","title":"Testing IMU Data Preservation","text":""},{"location":"technical/imu-metadata/#check-original-metadata","title":"Check Original Metadata","text":"<pre><code>ffprobe -v quiet -print_format json -show_streams -show_format input.mov\n</code></pre>"},{"location":"technical/imu-metadata/#safe-conversion","title":"Safe Conversion","text":"<pre><code>ffmpeg -i input.mov -c copy -map 0 output.mp4\n</code></pre>"},{"location":"technical/imu-metadata/#verify-preservation","title":"Verify Preservation","text":"<pre><code>ffprobe -v quiet -print_format json -show_streams -show_format output.mp4\n</code></pre>"},{"location":"technical/imu-metadata/#can-we-use-imu-data-for-better-jump-analysis","title":"Can We Use IMU Data for Better Jump Analysis?","text":""},{"location":"technical/imu-metadata/#investigation-summary","title":"\u2753 Investigation Summary","text":"<p>Question: Could we extract and use IMU data from iPhone videos to improve drop jump analysis accuracy?</p> <p>Current Status (as of November 2025): Vision-based tracking achieves:</p> <ul> <li>Contact timing: \u00b12-4 frames (33-67ms @ 60fps)</li> <li>Drop start detection: \u00b15 frames (83ms @ 60fps)</li> <li>Jump height: Calibrated from trajectory + known drop height</li> <li>Tested on iPhone 16 Pro 60fps videos - accuracy validated against manual frame-by-frame observations</li> </ul> <p>Potential IMU Benefits:</p> <ul> <li>Higher sampling rate: 100-200Hz (vs 30-60fps video)</li> <li>Direct acceleration measurement during flight (should be -9.81 m/s\u00b2)</li> <li>More precise landing/takeoff detection (acceleration spikes)</li> <li>Independent validation of vision-based measurements</li> </ul>"},{"location":"technical/imu-metadata/#research-findings","title":"\ud83d\udd2c Research Findings","text":"<p>Available Libraries:</p> <ul> <li>telemetry-parser (Rust-based, Python bindings):</li> <li>\u2705 Supports: GoPro GPMF, DJI, Sony, Insta360</li> <li>\u2705 Supports: iOS third-party apps (Sensor Logger, G-Field Recorder)</li> <li>\u274c Does NOT support: iPhone native Camera app videos</li> </ul> <p>Apple's Format:</p> <ul> <li>Proprietary \"mebx\" metadata streams (no public documentation)</li> <li>Binary format structure unknown</li> <li>66KB of data extracted (stream 6 from test video)</li> <li>Appears to contain floating-point arrays (from hexdump analysis)</li> <li>No existing parser or documentation available</li> </ul> <p>What Would Be Required:</p> <ol> <li>Reverse-engineer Apple's mebx binary format</li> <li>Decode timestamp synchronization</li> <li>Handle coordinate frame transformations (device \u2192 world coordinates)</li> <li>Account for video rotation metadata (-90\u00b0, 90\u00b0, 180\u00b0)</li> <li>Graceful fallback when IMU not available</li> <li>Testing across iOS versions (format may change)</li> </ol> <p>Estimated Effort: Weeks to months of reverse engineering</p>"},{"location":"technical/imu-metadata/#cost-benefit-analysis","title":"\u2696\ufe0f Cost-Benefit Analysis","text":"<p>Costs:</p> <ul> <li>\ud83d\udd34 High implementation complexity (reverse engineering)</li> <li>\ud83d\udd34 Undocumented proprietary format (may break with iOS updates)</li> <li>\ud83d\udd34 Only benefits iPhone users (not universal)</li> <li>\ud83d\udd34 New Rust dependency (telemetry-parser) or custom parser</li> <li>\ud83d\udd34 Coordinate frame alignment complexity</li> <li>\ud83d\udd34 Maintenance burden</li> </ul> <p>Benefits:</p> <ul> <li>\ud83d\udfe2 Could improve timing precision from \u00b130-50ms to \u00b15-10ms</li> <li>\ud83d\udfe2 Physics-based validation (flight acceleration check)</li> <li>\ud83d\udfe2 Independent measurement method</li> </ul> <p>Reality Check:</p> <ul> <li>Current accuracy (\u00b130-50ms) is 10-20% error for typical measurements (200-400ms contact times)</li> <li>This is acceptable for coaching and performance tracking</li> <li>This is comparable to commercial force plate timing accuracy (\u00b120-30ms for optical systems)</li> <li>IMU improvement would reduce to 2-5% error</li> <li>Marginal improvement for very high implementation cost</li> </ul> <p>Tested Results (iPhone 16 Pro, 60fps):</p> <pre><code>Manual observation:  Contact frames 138-162, Flight frames 162-191\nAuto-detected:       Contact frames 139-159, Flight frames 160-172\nAccuracy:            \u00b11-4 frames (17-67ms) for contact detection\n</code></pre>"},{"location":"technical/imu-metadata/#recommendation-do-not-implement","title":"\u274c Recommendation: DO NOT IMPLEMENT","text":"<p>Reasons:</p> <ol> <li>No existing parser - would need custom reverse engineering</li> <li>High complexity - proprietary undocumented format</li> <li>Current accuracy is sufficient - \u00b130-50ms is acceptable for athletic performance</li> <li>Limited scope - only helps iPhone Camera app users</li> <li>Better alternatives exist:</li> <li>Use 120fps cameras for higher temporal resolution</li> <li>Force plates for ground truth validation (research setting)</li> <li>Third-party iOS apps if IMU absolutely needed</li> </ol>"},{"location":"technical/imu-metadata/#alternative-vision-based-improvements","title":"\u2705 Alternative: Vision-Based Improvements","text":"<p>Already implemented:</p> <ul> <li>\u2705 Auto-tuning for any FPS (30/60/120fps)</li> <li>\u2705 Drop start auto-detection (\u00b15 frames)</li> <li>\u2705 Sub-frame interpolation (fractional frame precision)</li> <li>\u2705 Trajectory curvature analysis (acceleration patterns)</li> <li>\u2705 Video rotation handling (iPhone portrait videos)</li> </ul> <p>Future enhancements (if needed):</p> <ul> <li>Support for 120fps+ videos (minimal code changes)</li> <li>Multi-camera triangulation (3D position)</li> <li>Machine learning for pose refinement</li> <li>All universal solutions that work on any video source</li> </ul>"},{"location":"technical/imu-metadata/#decision","title":"\ud83c\udfaf Decision","text":"<p>Status: DEFERRED - Mark as \"future enhancement if strong community demand exists\"</p> <p>Current vision-based approach is production-ready:</p> <ul> <li>\u2705 Sufficiently accurate for athletic performance tracking (\u00b130-50ms)</li> <li>\u2705 Universal (works on any video source, not just iPhone)</li> <li>\u2705 Low complexity, well-tested, maintainable</li> <li>\u2705 Intelligent auto-tuning eliminates manual parameter adjustment</li> <li>\u2705 Handles 30fps, 60fps, 120fps+ videos automatically</li> </ul> <p>IMU support would require:</p> <ul> <li>Reverse engineering proprietary Apple format (weeks of work)</li> <li>Only benefits iPhone Camera app users (limited scope)</li> <li>Marginal accuracy improvement (\u00b130-50ms \u2192 \u00b15-10ms)</li> <li>High maintenance burden (may break with iOS updates)</li> </ul> <p>Recommendation: Focus development effort on:</p> <ol> <li>Validating current accuracy against force plates</li> <li>Improving vision algorithms (multi-camera, ML-based pose refinement)</li> <li>Better user guidance (camera placement, fps recommendations)</li> <li>Supporting more jump types (CMJ, squat jumps)</li> </ol> <p>These provide better ROI than IMU support.</p>"},{"location":"technical/imu-metadata/#practical-implications-for-video-analysis","title":"Practical Implications for Video Analysis","text":""},{"location":"technical/imu-metadata/#multi-jump-videos","title":"Multi-Jump Videos","text":"<ul> <li>Don't split in Photos app: IMU data will be lost</li> <li>Keep original file: Process time ranges without splitting</li> <li>Use proper trimming: <code>-c copy</code> preserves all data</li> </ul> <p>Note: Since Kinemotion does not currently use IMU data, splitting videos in Photos app does not affect analysis results. However, keeping IMU data preserved may be valuable for future enhancements or other analysis tools.</p>"},{"location":"technical/imu-metadata/#workflow-recommendations","title":"Workflow Recommendations","text":"<ol> <li>Record video with iPhone</li> <li>Keep original file intact (preserves IMU even if not currently used)</li> <li>Use FFmpeg for any necessary trimming/splitting</li> <li>Process time ranges rather than creating separate files</li> <li>Convert formats only with stream copying</li> </ol>"},{"location":"technical/imu-metadata/#key-commands-reference","title":"Key Commands Reference","text":"<pre><code># Inspect metadata streams\nffprobe -v quiet -select_streams s -show_entries stream=index,codec_name input.mov\n\n# Safe format conversion\nffmpeg -i input.mov -c copy -map 0 output.mp4\n\n# Safe trimming\nffmpeg -i input.mov -ss 00:01:30 -t 00:00:15 -c copy trimmed.mov\n\n# Split into segments while preserving metadata\nffmpeg -i input.mov -c copy -map 0 -f segment -segment_time 10 output_%03d.mov\n</code></pre>"},{"location":"technical/real-time-analysis/","title":"Real-Time CMJ Analysis - Technical Assessment","text":""},{"location":"technical/real-time-analysis/#executive-summary","title":"Executive Summary","text":"<p>Can the current CMJ implementation work with streaming video for real-time analysis?</p> <p>Answer: No, but near real-time (1-2 second delay) is feasible.</p>"},{"location":"technical/real-time-analysis/#current-implementation-architecture","title":"Current Implementation Architecture","text":""},{"location":"technical/real-time-analysis/#design-constraints","title":"Design Constraints","text":"<p>The current CMJ analysis algorithm requires the complete video due to fundamental architectural decisions:</p>"},{"location":"technical/real-time-analysis/#1-backward-search-algorithm","title":"1. Backward Search Algorithm","text":"<p>Core principle: Find peak height first, work backward to find all events.</p> <pre><code># Current algorithm (requires complete video)\ndef detect_cmj_phases(positions, fps, ...):\n    # Step 1: Find peak height (global minimum y)\n    peak_frame = argmin(positions)  # Needs complete trajectory!\n\n    # Step 2: Work backward from peak\n    takeoff = find_peak_velocity_before(peak_frame)\n    lowest_point = find_max_position_before(takeoff)\n    landing = find_impact_after(peak_frame)\n\n    return (lowest_point, takeoff, landing)\n</code></pre> <p>Why this matters:</p> <ul> <li>Can't find peak height until jump is complete</li> <li>Can't work backward until we know where peak is</li> <li>Requires global view of entire trajectory</li> </ul>"},{"location":"technical/real-time-analysis/#2-savitzky-golay-smoothing","title":"2. Savitzky-Golay Smoothing","text":"<p>Symmetric window requirement:</p> <pre><code># Current smoothing (non-causal)\nvelocity = savgol_filter(\n    positions,\n    window_length=5,      # Uses \u00b12 frames around current\n    polyorder=2,\n    deriv=1,\n    mode='interp'\n)\n</code></pre> <p>Implications:</p> <ul> <li>Needs frames from FUTURE to smooth current frame</li> <li>Can't process frame-by-frame as they arrive</li> <li>Non-causal filter (not real-time compatible)</li> </ul>"},{"location":"technical/real-time-analysis/#3-global-trajectory-analysis","title":"3. Global Trajectory Analysis","text":"<p>Auto-tuning and phase detection:</p> <ul> <li>Analyzes entire video to determine tracking quality</li> <li>Uses global min/max for phase boundaries</li> <li>Optimizes parameters based on full sequence</li> </ul> <p>Validation results:</p> <ul> <li>\u2705 Jump height: 50.6cm (\u00b11 frame = 33ms accuracy)</li> <li>\u2705 Takeoff detection: Frame 154 (known: 153)</li> <li>\u2705 Landing detection: Frame 173 (known: 172)</li> </ul> <p>This accuracy comes FROM having the complete trajectory.</p>"},{"location":"technical/real-time-analysis/#real-time-feasibility-analysis","title":"Real-Time Feasibility Analysis","text":""},{"location":"technical/real-time-analysis/#option-1-near-real-time-with-buffering-recommended","title":"Option 1: Near Real-Time with Buffering \u2b50 RECOMMENDED","text":"<p>Approach: Buffer frames, detect jump completion, analyze buffer.</p> <pre><code>class NearRealTimeCMJAnalyzer:\n    def __init__(self):\n        self.buffer = CircularBuffer(max_frames=300)  # 10 seconds @ 30fps\n        self.state = \"waiting\"\n\n    def process_frame(self, frame):\n        # Add to buffer\n        self.buffer.add(frame)\n\n        # Detect jump completion\n        if self.state == \"waiting\" and self.detect_movement_start():\n            self.state = \"jumping\"\n\n        if self.state == \"jumping\" and self.detect_jump_complete():\n            # Run offline algorithm on buffered frames\n            metrics = analyze_cmj_offline(self.buffer.get_frames())\n            self.state = \"waiting\"\n            self.buffer.clear()\n            return metrics  # Results 1-2 seconds after landing\n\n        return None  # Jump not complete yet\n</code></pre> <p>Characteristics:</p> <ul> <li>Latency: 1-2 seconds after landing</li> <li>Accuracy: \u2b50\u2b50\u2b50\u2b50\u2b50 Same as offline (50.6cm validated)</li> <li>Complexity: Low (reuses existing algorithm)</li> <li>Implementation: 200-300 lines of new code</li> </ul> <p>Jump completion detection:</p> <pre><code>def detect_jump_complete(buffer):\n    recent_positions = buffer.get_recent(frames=30)  # Last 1 second\n    recent_velocity = compute_velocity(recent_positions)\n\n    # Landed if:\n    # 1. Low velocity for sustained period\n    # 2. Position has returned to near-starting level\n    # 3. At least 2 seconds have passed since movement start\n\n    if (abs(recent_velocity) &lt; 0.005 and\n        duration &gt; 2.0 and\n        position_stabilized):\n        return True\n    return False\n</code></pre> <p>Advantages:</p> <ul> <li>\u2705 Maintains accuracy (same algorithm)</li> <li>\u2705 Simple implementation</li> <li>\u2705 Keeps triple extension visualization</li> <li>\u2705 1-2 second delay acceptable for coaching</li> <li>\u2705 No new validation needed</li> </ul> <p>Disadvantages:</p> <ul> <li>\u26a0\ufe0f Not instant (1-2 second delay)</li> <li>\u26a0\ufe0f Memory usage (buffer of ~300 frames)</li> </ul>"},{"location":"technical/real-time-analysis/#option-2-true-real-time-with-forward-detection","title":"Option 2: True Real-Time with Forward Detection","text":"<p>Approach: Detect phases as they occur, no buffering.</p> <pre><code>class RealTimeCMJAnalyzer:\n    def __init__(self):\n        self.state = \"standing\"\n        self.events = {}\n        self.position_history = deque(maxlen=10)  # Causal filter\n\n    def process_frame(self, frame, frame_idx):\n        # Track pose\n        landmarks = tracker.process(frame)\n        position = extract_position(landmarks)\n\n        # Causal smoothing (only past frames)\n        self.position_history.append(position)\n        smoothed_pos = exponential_smooth(self.position_history)\n        velocity = self.position_history[-1] - self.position_history[-2]\n\n        # State machine for phase detection\n        if self.state == \"standing\" and velocity &gt; 0.015:\n            self.state = \"eccentric\"\n            self.events['eccentric_start'] = frame_idx\n\n        elif self.state == \"eccentric\" and velocity &lt; 0:\n            self.state = \"concentric\"\n            self.events['lowest_point'] = frame_idx\n\n        elif self.state == \"concentric\" and velocity &lt; -0.02:\n            self.state = \"flight\"\n            self.events['takeoff'] = frame_idx\n\n        elif self.state == \"flight\" and abs(velocity) &lt; 0.01:\n            self.state = \"landed\"\n            self.events['landing'] = frame_idx\n            return self.calculate_metrics()  # Return immediately!\n\n        return None  # Jump not complete\n</code></pre> <p>Characteristics:</p> <ul> <li>Latency: \\&lt;100ms (instant)</li> <li>Accuracy: \u2b50\u2b50\u2b50 Lower (causal filtering, no global optimization)</li> <li>Complexity: High (new algorithm, needs extensive validation)</li> <li>Implementation: 500-800 lines + validation</li> </ul> <p>Challenges:</p> <ol> <li> <p>Causal filtering artifacts:</p> </li> <li> <p>Exponential smoothing has lag</p> </li> <li>Can't use future frames to refine past estimates</li> <li> <p>Velocity estimates noisier</p> </li> <li> <p>No global optimization:</p> </li> <li> <p>Can't find \"true\" peak (don't know it yet)</p> </li> <li>Can't refine takeoff by working backward</li> <li> <p>Miss opportunities for sub-frame interpolation</p> </li> <li> <p>False positives:</p> </li> <li> <p>May detect non-jump movements</p> </li> <li>Need robust state machine</li> <li> <p>Sensitive to noise</p> </li> <li> <p>Accuracy degradation:</p> </li> <li> <p>Estimated: 55-60cm instead of 50.6cm (\u00b110% error)</p> </li> <li>Frame detection: \u00b12-3 frames instead of \u00b11</li> <li>Less reliable phase detection</li> </ol> <p>Advantages:</p> <ul> <li>\u2705 Instant feedback (\\&lt;100ms)</li> <li>\u2705 True real-time</li> <li>\u2705 Lower memory usage</li> </ul> <p>Disadvantages:</p> <ul> <li>\u26a0\ufe0f Reduced accuracy (~10% error)</li> <li>\u26a0\ufe0f Complex validation needed</li> <li>\u26a0\ufe0f High implementation effort</li> <li>\u26a0\ufe0f May not work reliably</li> </ul>"},{"location":"technical/real-time-analysis/#performance-analysis","title":"Performance Analysis","text":""},{"location":"technical/real-time-analysis/#current-processing-speed","title":"Current Processing Speed","text":"<p>Test video: 236 frames @ 29.58fps</p> <pre><code>Processing time breakdown:\n- MediaPipe tracking: ~5-6 seconds\n- Smoothing: ~0.1 seconds\n- Phase detection: ~0.01 seconds\n- Rendering debug video: ~2 seconds\nTotal: ~7-8 seconds for 8-second video\n</code></pre> <p>Per-frame cost: ~30-40ms (mostly MediaPipe)</p> <p>Bottleneck: MediaPipe pose tracking (not our algorithm)</p>"},{"location":"technical/real-time-analysis/#real-time-requirements","title":"Real-Time Requirements","text":"<p>For 30fps streaming:</p> <ul> <li>Must process each frame in \\&lt;33ms</li> <li>MediaPipe: ~25-30ms per frame</li> <li>Our algorithm: \\&lt;1ms per frame</li> <li>Conclusion: MediaPipe is the limiting factor, but feasible</li> </ul> <p>For 60fps streaming:</p> <ul> <li>Must process each frame in \\&lt;17ms</li> <li>MediaPipe: ~25-30ms per frame</li> <li>Conclusion: Would need GPU acceleration or lower MediaPipe model complexity</li> </ul>"},{"location":"technical/real-time-analysis/#implementation-roadmap","title":"Implementation Roadmap","text":""},{"location":"technical/real-time-analysis/#phase-1-near-real-time-buffered-recommended","title":"Phase 1: Near Real-Time (Buffered) - RECOMMENDED","text":"<p>Effort: 1-2 days</p> <p>Implementation:</p> <ol> <li>Create <code>RealtimeCMJAnalyzer</code> class:</li> </ol> <pre><code>class RealtimeCMJAnalyzer:\n    def __init__(self, buffer_seconds=10):\n        self.buffer = CircularFrameBuffer(buffer_seconds)\n        self.tracker = PoseTracker(...)\n\n    def process_frame(self, frame):\n        \"\"\"Process single frame, return metrics when jump complete.\"\"\"\n        landmarks = self.tracker.process_frame(frame)\n        self.buffer.add(frame, landmarks)\n\n        if self.is_jump_complete():\n            return self.analyze_buffered_jump()\n        return None\n</code></pre> <ol> <li> <p>Jump completion detector:</p> </li> <li> <p>Monitor position stability</p> </li> <li>Detect landing (velocity \u2192 near zero)</li> <li> <p>Wait 1 second after landing to ensure complete</p> </li> <li> <p>Analyze buffered frames:</p> </li> <li> <p>Call existing <code>detect_cmj_phases()</code></p> </li> <li>Use proven algorithm</li> <li>Return metrics with 1-2 second delay</li> </ol> <p>API Example:</p> <pre><code>analyzer = RealtimeCMJAnalyzer()\n\nwhile True:\n    frame = camera.read()\n    result = analyzer.process_frame(frame)\n\n    if result:\n        print(f\"Jump complete! Height: {result.jump_height*100:.1f}cm\")\n</code></pre> <p>Benefits:</p> <ul> <li>\u2705 Maintains 50.6cm accuracy</li> <li>\u2705 Simple implementation</li> <li>\u2705 All features work (triple extension, etc.)</li> <li>\u2705 1-2 second delay acceptable</li> </ul>"},{"location":"technical/real-time-analysis/#phase-2-true-real-time-forward-detection-future","title":"Phase 2: True Real-Time (Forward Detection) - FUTURE","text":"<p>Effort: 1-2 weeks + validation</p> <p>Implementation:</p> <ol> <li>Causal filter (exponential smoothing or one-sided Savitzky-Golay)</li> <li>Forward phase detection (state machine)</li> <li>Event triggers (real-time callbacks)</li> <li>Extensive validation (test against known jumps)</li> </ol> <p>Expected accuracy: 55-60cm (\u00b110% error)</p> <p>Use cases: Specialized applications requiring instant feedback</p>"},{"location":"technical/real-time-analysis/#recommendations","title":"Recommendations","text":""},{"location":"technical/real-time-analysis/#for-most-use-cases-near-real-time-buffered","title":"For Most Use Cases: Near Real-Time (Buffered)","text":"<p>Implement buffered mode if:</p> <ul> <li>Need quick results during training (1-2 sec acceptable)</li> <li>Want to maintain accuracy (50.6cm validated)</li> <li>Building coaching app or live testing system</li> <li>Value reliability over instant feedback</li> </ul>"},{"location":"technical/real-time-analysis/#for-researchanalysis-offline-current","title":"For Research/Analysis: Offline (Current)","text":"<p>Use current implementation for:</p> <ul> <li>Maximum accuracy needed</li> <li>Post-session analysis</li> <li>Research/publication data</li> <li>Detailed biomechanical analysis</li> </ul>"},{"location":"technical/real-time-analysis/#future-true-real-time","title":"Future: True Real-Time","text":"<p>Only implement if:</p> <ul> <li>Instant feedback is critical (\\&lt;100ms)</li> <li>Willing to accept ~10% accuracy reduction</li> <li>Have time for extensive validation</li> <li>Specialized application requires it</li> </ul>"},{"location":"technical/real-time-analysis/#technical-constraints","title":"Technical Constraints","text":""},{"location":"technical/real-time-analysis/#mediapipe-processing-speed","title":"MediaPipe Processing Speed","text":"<p>Current: ~25-30ms per frame (M1 Pro)</p> <p>Implications:</p> <ul> <li>30fps: \u2705 Feasible (33ms budget, 25-30ms used)</li> <li>60fps: \u26a0\ufe0f Challenging (17ms budget, 25-30ms used)</li> </ul> <p>Solutions for 60fps:</p> <ol> <li>Use GPU acceleration</li> <li>Use lighter MediaPipe model (reduced accuracy)</li> <li>Process every other frame</li> <li>Use dedicated hardware (Jetson Nano, etc.)</li> </ol>"},{"location":"technical/real-time-analysis/#memory-requirements","title":"Memory Requirements","text":"<p>Buffered approach:</p> <ul> <li>10 seconds @ 30fps = 300 frames</li> <li>720x1280x3 bytes per frame = 2.7MB per frame</li> <li>Total buffer: ~800MB (manageable)</li> </ul> <p>True real-time:</p> <ul> <li>Minimal buffer (10-20 frames for smoothing)</li> <li>~50-100MB (very low)</li> </ul>"},{"location":"technical/real-time-analysis/#example-use-cases","title":"Example Use Cases","text":""},{"location":"technical/real-time-analysis/#use-case-1-training-app-near-real-time","title":"Use Case 1: Training App (Near Real-Time)","text":"<p>Scenario: Mobile app for athletes to track CMJ during workouts</p> <pre><code># Training app implementation\napp = TrainingApp()\nanalyzer = RealtimeCMJAnalyzer(buffer_seconds=10)\n\nwhile training_session_active:\n    frame = phone_camera.capture()\n    result = analyzer.process_frame(frame)\n\n    if result:\n        # Show results 1-2 seconds after landing\n        app.display_result(f\"Jump: {result.jump_height*100:.0f}cm\")\n        app.save_to_session(result)\n        app.play_feedback_sound()\n</code></pre> <p>Latency: 1-2 seconds after landing \u2713 Acceptable</p>"},{"location":"technical/real-time-analysis/#use-case-2-research-lab-offline-current","title":"Use Case 2: Research Lab (Offline - Current)","text":"<p>Scenario: Biomechanics research, publication-quality data</p> <pre><code># Current offline analysis (already implemented)\nmetrics = process_cmj_video(\n    \"athlete_cmj.mp4\",\n    quality=\"accurate\",\n    output_video=\"debug.mp4\"\n)\n\n# Accuracy: 50.6cm (validated)\n# Use for research papers, detailed analysis\n</code></pre> <p>Latency: N/A (offline) \u2713 Maximum accuracy</p>"},{"location":"technical/real-time-analysis/#use-case-3-competition-testing-near-real-time","title":"Use Case 3: Competition Testing (Near Real-Time)","text":"<p>Scenario: Testing session with immediate results</p> <pre><code># Competition testing station\nanalyzer = RealtimeCMJAnalyzer()\n\nfor athlete in competition:\n    print(f\"Testing {athlete.name}...\")\n\n    while True:\n        frame = camera.capture()\n        result = analyzer.process_frame(frame)\n\n        if result:\n            print(f\"Result: {result.jump_height*100:.1f}cm\")\n            save_to_database(athlete, result)\n            break  # Move to next athlete\n</code></pre> <p>Latency: 1-2 seconds \u2713 Fast enough for testing</p>"},{"location":"technical/real-time-analysis/#implementation-specification","title":"Implementation Specification","text":""},{"location":"technical/real-time-analysis/#buffered-near-real-time-mode","title":"Buffered Near Real-Time Mode","text":""},{"location":"technical/real-time-analysis/#api-design","title":"API Design","text":"<pre><code>from kinemotion import RealtimeCMJAnalyzer\n\n# Initialize analyzer\nanalyzer = RealtimeCMJAnalyzer(\n    buffer_seconds=10,           # Buffer duration\n    quality=\"balanced\",           # Analysis quality\n    detection_confidence=0.5,     # MediaPipe settings\n    jump_timeout=5.0,            # Max jump duration\n)\n\n# Process streaming frames\nwhile camera.is_open():\n    frame = camera.read()\n\n    # Returns None until jump complete\n    result = analyzer.process_frame(frame)\n\n    if result:\n        print(f\"Jump height: {result.metrics.jump_height*100:.1f}cm\")\n        print(f\"Latency: {result.latency_ms:.0f}ms after landing\")\n\n    # Optional: Get live preview (incomplete analysis)\n    preview = analyzer.get_preview()\n    if preview:\n        display_text(f\"Current phase: {preview.phase}\")\n</code></pre>"},{"location":"technical/real-time-analysis/#jump-completion-detection","title":"Jump Completion Detection","text":"<pre><code>def detect_jump_complete(self):\n    \"\"\"Detect if a complete CMJ has occurred and analysis can begin.\"\"\"\n\n    # Require minimum activity\n    if self.buffer.duration &lt; 2.0:\n        return False\n\n    # Get recent trajectory (last 1 second)\n    recent_positions = self.buffer.get_recent(frames=30)\n    recent_velocity = compute_signed_velocity(recent_positions)\n\n    # Check for landing indicators:\n    # 1. Position stabilized (low velocity)\n    stable = np.abs(recent_velocity[-10:]).mean() &lt; 0.005\n\n    # 2. Clear upward motion detected earlier\n    had_upward_motion = np.any(recent_velocity &lt; -0.015)\n\n    # 3. Position returned to near-starting level\n    current_pos = recent_positions[-1]\n    starting_pos = self.buffer.get_starting_position()\n    returned = abs(current_pos - starting_pos) &lt; 0.05\n\n    return stable and had_upward_motion and returned\n</code></pre>"},{"location":"technical/real-time-analysis/#processing-pipeline","title":"Processing Pipeline","text":"<pre><code>def analyze_buffered_jump(self):\n    \"\"\"Analyze complete jump from buffer using offline algorithm.\"\"\"\n\n    # Extract frames and landmarks from buffer\n    frames = self.buffer.get_frames()\n    landmarks = self.buffer.get_landmarks()\n\n    # Use existing offline algorithm (proven accurate)\n    positions = extract_positions(landmarks)\n    phases = detect_cmj_phases(positions, fps, ...)  # Backward search\n    metrics = calculate_cmj_metrics(...)\n\n    # Calculate latency\n    landing_time = phases[3] / fps\n    current_time = self.buffer.duration\n    latency = current_time - landing_time\n\n    return RealtimeResult(\n        metrics=metrics,\n        latency_ms=latency * 1000,\n        frames_analyzed=len(frames)\n    )\n</code></pre> <p>Accuracy: Same as offline (50.6cm validated) \u2713</p>"},{"location":"technical/real-time-analysis/#true-real-time-mode-future","title":"True Real-Time Mode (Future)","text":""},{"location":"technical/real-time-analysis/#forward-only-detection","title":"Forward-Only Detection","text":"<p>State machine approach:</p> <pre><code>class TrueRealtimeCMJAnalyzer:\n    def __init__(self):\n        self.state = \"idle\"\n        self.events = {}\n        self.position_buffer = deque(maxlen=10)  # Minimal buffer\n\n    def process_frame(self, frame, frame_idx):\n        # Extract position (causal only)\n        position = self.extract_position_causal(frame)\n        self.position_buffer.append(position)\n\n        # Compute causal velocity (only past frames)\n        velocity = self.compute_causal_velocity()\n\n        # State transitions\n        if self.state == \"idle\" and abs(velocity) &lt; 0.005:\n            self.state = \"standing\"\n\n        elif self.state == \"standing\" and velocity &gt; 0.015:\n            self.state = \"eccentric\"\n            self.events['eccentric_start'] = frame_idx\n\n        elif self.state == \"eccentric\" and velocity &lt; 0:\n            self.state = \"concentric\"\n            self.events['lowest_point'] = frame_idx\n\n        elif self.state == \"concentric\" and velocity &lt; -0.025:\n            self.state = \"flight\"\n            self.events['takeoff'] = frame_idx\n\n        elif self.state == \"flight\" and abs(velocity) &lt; 0.01:\n            self.state = \"landing\"\n            self.events['landing'] = frame_idx\n\n            # Calculate metrics immediately\n            return self.calculate_instant_metrics()\n\n        return None\n</code></pre> <p>Causal smoothing:</p> <pre><code>def compute_causal_velocity(self):\n    \"\"\"Compute velocity using only past frames (causal filter).\"\"\"\n\n    # Option 1: Exponential moving average\n    alpha = 0.3\n    smoothed = self.position_buffer[0]\n    for pos in self.position_buffer[1:]:\n        smoothed = alpha * pos + (1 - alpha) * smoothed\n    velocity = self.position_buffer[-1] - smoothed\n\n    # Option 2: One-sided Savitzky-Golay\n    # Uses only past frames (asymmetric window)\n    velocity = savgol_filter(\n        list(self.position_buffer),\n        window_length=9,\n        polyorder=2,\n        deriv=1,\n        mode='mirror'  # Only use left side\n    )[-1]\n\n    return velocity\n</code></pre> <p>Expected accuracy:</p> <ul> <li>Jump height: \u00b110% error (55-60cm instead of 50.6cm)</li> <li>Frame detection: \u00b12-3 frames instead of \u00b11</li> <li>Phase timing: Less precise</li> </ul> <p>Why lower accuracy:</p> <ul> <li>Causal filtering has inherent lag</li> <li>Can't refine estimates with future data</li> <li>No global trajectory optimization</li> <li>More sensitive to noise</li> </ul>"},{"location":"technical/real-time-analysis/#performance-comparison","title":"Performance Comparison","text":"Metric Offline (Current) Near Real-Time True Real-Time Latency N/A 1-2 seconds \\&lt;100ms Jump Height Accuracy 50.6cm \u2713 50.6cm \u2713 ~55-60cm Frame Detection Error \u00b11 frame \u00b11 frame \u00b12-3 frames Smoothing Quality Excellent Excellent Good Triple Extension Yes Yes Limited Memory Usage Full video ~800MB buffer ~50MB Implementation Effort \u2713 Done 1-2 days 1-2 weeks Validation Effort \u2713 Done Minimal Extensive"},{"location":"technical/real-time-analysis/#recommendation-matrix","title":"Recommendation Matrix","text":""},{"location":"technical/real-time-analysis/#choose-offline-current-if","title":"Choose Offline (Current) If","text":"<ul> <li>\u2705 Maximum accuracy required (research, validation)</li> <li>\u2705 Processing pre-recorded videos</li> <li>\u2705 Time is not critical</li> <li>\u2705 Want triple extension with full coverage</li> <li>\u2705 Publication-quality data needed</li> </ul>"},{"location":"technical/real-time-analysis/#choose-near-real-time-if","title":"Choose Near Real-Time If","text":"<ul> <li>\u2705 Need quick results (1-2 sec acceptable)</li> <li>\u2705 Coaching/training applications</li> <li>\u2705 Live testing sessions</li> <li>\u2705 Want to maintain accuracy</li> <li>\u2705 Building mobile/web app</li> </ul>"},{"location":"technical/real-time-analysis/#choose-true-real-time-if","title":"Choose True Real-Time If","text":"<ul> <li>\u26a0\ufe0f Instant feedback critical (\\&lt;100ms)</li> <li>\u26a0\ufe0f Interactive applications (games, VR)</li> <li>\u26a0\ufe0f Can accept ~10% accuracy reduction</li> <li>\u26a0\ufe0f Have resources for extensive validation</li> <li>\u26a0\ufe0f Specialized use case</li> </ul>"},{"location":"technical/real-time-analysis/#technical-considerations","title":"Technical Considerations","text":""},{"location":"technical/real-time-analysis/#buffered-mode-implementation-checklist","title":"Buffered Mode Implementation Checklist","text":"<p>Core components needed:</p> <ol> <li> <p>CircularFrameBuffer class</p> </li> <li> <p>Store frames + landmarks</p> </li> <li>Efficient memory management</li> <li> <p>Thread-safe if needed</p> </li> <li> <p>JumpCompletionDetector</p> </li> <li> <p>Analyze recent frames</p> </li> <li>Detect landing + stability</li> <li> <p>Trigger analysis</p> </li> <li> <p>RealtimeCMJAnalyzer wrapper</p> </li> <li> <p>Manages buffer</p> </li> <li>Calls existing offline algorithm</li> <li> <p>Returns results with latency info</p> </li> <li> <p>Testing suite</p> </li> <li> <p>Validate against known jumps</p> </li> <li>Test various scenarios</li> <li>Measure actual latency</li> </ol> <p>Estimated implementation: 200-300 lines</p>"},{"location":"technical/real-time-analysis/#true-real-time-implementation-checklist","title":"True Real-Time Implementation Checklist","text":"<p>Major components needed:</p> <ol> <li> <p>Causal filtering system</p> </li> <li> <p>Exponential smoothing</p> </li> <li>One-sided Savitzky-Golay</li> <li> <p>Adaptive filtering</p> </li> <li> <p>Forward phase detector</p> </li> <li> <p>State machine</p> </li> <li>Event triggers</li> <li> <p>Robust to noise</p> </li> <li> <p>Instant metrics calculator</p> </li> <li> <p>No backward refinement</p> </li> <li>Immediate results</li> <li> <p>Error handling</p> </li> <li> <p>Extensive validation</p> </li> <li> <p>Compare with offline</p> </li> <li>Test accuracy degradation</li> <li>Validate across multiple videos</li> </ol> <p>Estimated implementation: 500-800 lines</p>"},{"location":"technical/real-time-analysis/#conclusion","title":"Conclusion","text":""},{"location":"technical/real-time-analysis/#summary","title":"Summary","text":"<p>Current implementation (offline):</p> <ul> <li>\u2705 Production-ready</li> <li>\u2705 Validated accuracy (50.6cm)</li> <li>\u2705 \u00b11 frame precision</li> <li>\u2705 Triple extension tracking</li> <li>\u274c Not real-time compatible</li> </ul> <p>Feasible near-term: Buffered near real-time</p> <ul> <li>1-2 second delay</li> <li>Same accuracy as offline</li> <li>Simple implementation</li> <li>Good for coaching/training apps</li> </ul> <p>Future possibility: True real-time</p> <ul> <li>\\&lt;100ms latency</li> <li>~10% accuracy reduction</li> <li>Complex implementation</li> <li>Specialized use cases</li> </ul>"},{"location":"technical/real-time-analysis/#recommendation","title":"Recommendation","text":"<p>Implement buffered near real-time mode as next feature:</p> <ul> <li>Maintains accuracy (critical for credibility)</li> <li>Simple enough to implement quickly (1-2 days)</li> <li>Meets 80% of real-time use cases</li> <li>Builds on proven algorithm</li> </ul> <p>Don't implement true real-time unless:</p> <ul> <li>Specific application requires it</li> <li>Can accept accuracy reduction</li> <li>Have time for extensive validation</li> </ul>"},{"location":"technical/real-time-analysis/#questions-answers","title":"Questions &amp; Answers","text":"<p>Q: Can I use this for a live training app? A: Yes! Use buffered near real-time mode (1-2 sec delay). Accuracy maintained.</p> <p>Q: Can I get instant feedback as I jump? A: Not with current algorithm. Would need true real-time mode (~10% accuracy loss).</p> <p>Q: What about webcam input? A: Buffered mode works great with webcam. True real-time possible but less accurate.</p> <p>Q: How much memory does buffering need? A: ~800MB for 10 second buffer @ 1080p. Manageable for most systems.</p> <p>Q: Will triple extension work in real-time? A: Yes in buffered mode (same as offline). Limited in true real-time (harder to track during motion).</p> <p>Kinemotion CMJ Module Real-Time Analysis Technical Assessment Version 0.1.0 - 2025-11-06</p>"},{"location":"technical/triple-extension/","title":"Triple Extension Analysis for CMJ","text":""},{"location":"technical/triple-extension/#overview","title":"Overview","text":"<p>The CMJ debug video now includes triple extension tracking - real-time visualization of ankle, knee, and hip joint angles during the jump movement.</p>"},{"location":"technical/triple-extension/#what-is-triple-extension","title":"What is Triple Extension?","text":"<p>Triple extension is the simultaneous extension (straightening) of three key joints during the propulsive phase of jumping:</p> <ol> <li>Ankle Extension (plantarflexion) - Pushing through the toes</li> <li>Knee Extension - Straightening the legs</li> <li>Hip Extension - Driving the hips upward/forward</li> </ol>"},{"location":"technical/triple-extension/#why-it-matters","title":"Why It Matters","text":"<ul> <li>Performance indicator: Proper triple extension maximizes power transfer</li> <li>Technique assessment: Identifies incomplete extension (power leaks)</li> <li>Coaching tool: Visual feedback for athletes</li> <li>Progress tracking: Monitor improvement over time</li> </ul>"},{"location":"technical/triple-extension/#visualization-in-debug-video","title":"Visualization in Debug Video","text":""},{"location":"technical/triple-extension/#skeleton-overlay","title":"Skeleton Overlay","text":"<p>Color-coded segments:</p> <ul> <li>\ud83d\udfe6 Cyan: Foot (heel \u2192 ankle/toe)</li> <li>\ud83d\udd34 Light Red: Shin/tibia (ankle \u2192 knee)</li> <li>\ud83d\udfe2 Light Green: Femur/thigh (knee \u2192 hip)</li> <li>\ud83d\udd35 Light Blue: Trunk (hip \u2192 shoulder)</li> </ul>"},{"location":"technical/triple-extension/#joint-angle-display-top-right-panel","title":"Joint Angle Display (Top Right Panel)","text":"<p>Shows real-time angles:</p> <ul> <li>Ankle: Dorsiflexion/plantarflexion angle</li> <li>Knee: Flexion/extension angle</li> <li>Hip: Flexion/extension angle</li> <li>Trunk: Forward/backward tilt</li> </ul> <p>Angle indicators at joints:</p> <ul> <li>\ud83d\udfe2 Green ring: Extended (&gt;160\u00b0) - Good!</li> <li>\ud83d\udfe0 Orange ring: Moderate (90-160\u00b0)</li> <li>\ud83d\udd34 Red ring: Flexed (\\&lt;90\u00b0) - Deep squat</li> </ul>"},{"location":"technical/triple-extension/#typical-angle-values","title":"Typical Angle Values","text":"<p>At Lowest Point (Countermovement Bottom):</p> <pre><code>Ankle:  70-90\u00b0  (neutral to slight dorsiflexion)\nKnee:   90-110\u00b0 (moderate squat)\nHip:    90-110\u00b0 (hip flexion)\nTrunk:  0-20\u00b0   (slight forward lean)\n</code></pre> <p>At Takeoff (Leaving Ground):</p> <pre><code>Ankle:  110-130\u00b0 (strong plantarflexion)\nKnee:   160-180\u00b0 (near full extension)\nHip:    170-180\u00b0 (full extension)\nTrunk:  0-10\u00b0    (nearly vertical)\n</code></pre> <p>During Flight:</p> <pre><code>All joints: ~180\u00b0 (full extension)\n</code></pre>"},{"location":"technical/triple-extension/#mediapipe-limitations","title":"MediaPipe Limitations","text":""},{"location":"technical/triple-extension/#ankle-and-knee-visibility-issues","title":"Ankle and Knee Visibility Issues","text":"<p>Important Note: In lateral (side) view videos, MediaPipe may struggle to detect ankle and knee landmarks:</p> <p>Test video results:</p> <ul> <li>Heel: 100% visible \u2713</li> <li>Hip: 100% visible \u2713</li> <li>Shoulder: 100% visible \u2713</li> <li>Ankle: 27% visible \u26a0\ufe0f</li> <li>Knee: 18% visible \u26a0\ufe0f</li> </ul>"},{"location":"technical/triple-extension/#why-this-happens","title":"Why This Happens","text":"<ol> <li>Occlusion: In side view, ankle/knee may be hidden by the body</li> <li>Angle: MediaPipe trained primarily on frontal/oblique views</li> <li>Contrast: Ankle/knee may blend with background</li> <li>Resolution: Lower resolution reduces detection accuracy</li> </ol>"},{"location":"technical/triple-extension/#what-we-do-about-it","title":"What We Do About It","text":"<p>Graceful Degradation:</p> <ul> <li>Shows \"N/A\" for angles that can't be calculated</li> <li>Draws available skeleton segments only</li> <li>Always shows hip-shoulder (trunk) which is reliably detected</li> <li>Falls back to left side if right side unavailable</li> <li>Lower visibility threshold (0.2) to capture more landmarks</li> </ul> <p>When Triple Extension Works Best:</p> <ul> <li>\u2705 Higher resolution videos (1080p+)</li> <li>\u2705 Good contrast/lighting</li> <li>\u2705 Athlete wearing contrasting clothing</li> <li>\u2705 Clean background</li> <li>\u2705 Slight oblique angle (not perfectly perpendicular)</li> </ul> <p>When It May Be Limited:</p> <ul> <li>\u26a0\ufe0f Perfect side view (perpendicular)</li> <li>\u26a0\ufe0f Low resolution (720p or less)</li> <li>\u26a0\ufe0f Poor lighting</li> <li>\u26a0\ufe0f Busy background</li> <li>\u26a0\ufe0f Loose/baggy clothing</li> </ul>"},{"location":"technical/triple-extension/#interpreting-results","title":"Interpreting Results","text":""},{"location":"technical/triple-extension/#good-triple-extension-pattern","title":"Good Triple Extension Pattern","text":"<p>Progressive extension from lowest point to takeoff:</p> Phase Ankle Knee Hip Notes Lowest Point 75\u00b0 95\u00b0 100\u00b0 Deep countermovement Mid-Concentric 95\u00b0 135\u00b0 145\u00b0 Rapid extension Takeoff 120\u00b0 175\u00b0 178\u00b0 Full extension \u2713 Flight 125\u00b0 180\u00b0 180\u00b0 Maintained <p>Indicators of good technique:</p> <ul> <li>All three joints extend simultaneously</li> <li>Near-full extension at takeoff (knee &gt;170\u00b0, hip &gt;170\u00b0)</li> <li>Smooth progression through concentric phase</li> </ul>"},{"location":"technical/triple-extension/#poor-extension-patterns","title":"Poor Extension Patterns","text":""},{"location":"technical/triple-extension/#problem-1-incomplete-knee-extension","title":"Problem 1: Incomplete knee extension","text":"<pre><code>Takeoff: Ankle 120\u00b0, Knee 150\u00b0, Hip 175\u00b0\n\u2192 Power leak: Not fully utilizing leg strength\n</code></pre>"},{"location":"technical/triple-extension/#problem-2-sequential-extension-not-simultaneous","title":"Problem 2: Sequential extension (not simultaneous)","text":"<pre><code>Early concentric: Hip 170\u00b0, Knee 120\u00b0, Ankle 80\u00b0\n\u2192 Poor coordination: Extending in sequence instead of together\n</code></pre>"},{"location":"technical/triple-extension/#problem-3-excessive-trunk-lean","title":"Problem 3: Excessive trunk lean","text":"<pre><code>Takeoff: Trunk 30\u00b0 forward\n\u2192 Sub-optimal: Reduces vertical force component\n</code></pre>"},{"location":"technical/triple-extension/#usage","title":"Usage","text":""},{"location":"technical/triple-extension/#cli","title":"CLI","text":"<pre><code># Generate debug video with triple extension\nkinemotion cmj-analyze video.mp4 --output debug.mp4\n\n# The debug video will automatically include:\n# - Skeleton overlay\n# - Joint angles (when visible)\n# - Phase-coded visualization\n</code></pre>"},{"location":"technical/triple-extension/#what-youll-see","title":"What You'll See","text":"<p>Throughout the video:</p> <ul> <li>Phase-colored overlay (standing/eccentric/concentric/flight/landing)</li> <li>Skeleton segments (whatever MediaPipe detects)</li> <li>Joint markers (white circles with black borders)</li> </ul> <p>When ankle/knee are visible (typically ~20-30% of frames):</p> <ul> <li>Complete skeleton from heel to shoulder</li> <li>All joint angles displayed</li> <li>Angle arcs at each joint</li> </ul> <p>When ankle/knee are NOT visible:</p> <ul> <li>Heel-hip-shoulder segments shown</li> <li>Trunk angle displayed</li> <li>\"N/A\" shown for missing angles</li> </ul>"},{"location":"technical/triple-extension/#tips-for-better-triple-extension-tracking","title":"Tips for Better Triple Extension Tracking","text":""},{"location":"technical/triple-extension/#camera-setup","title":"Camera Setup","text":"<ol> <li> <p>Slight oblique angle - Not perfectly perpendicular (try 80\u00b0 instead of 90\u00b0)</p> </li> <li> <p>Helps MediaPipe see ankle/knee better</p> </li> <li> <p>Still captures vertical motion accurately</p> </li> <li> <p>Higher resolution - 1080p minimum, 4K better</p> </li> <li> <p>Improves landmark detection</p> </li> <li> <p>Reduces tracking loss</p> </li> <li> <p>Contrasting clothing - Wear fitted, solid-color clothing</p> </li> <li> <p>Different color than background</p> </li> <li> <p>Helps landmark detection</p> </li> <li> <p>Good lighting - Even, bright lighting</p> </li> <li> <p>No harsh shadows</p> </li> <li> <p>Improves tracking accuracy</p> </li> <li> <p>Clean background - Minimal visual clutter</p> </li> <li> <p>Solid color wall ideal</p> </li> <li>Reduces false detections</li> </ol>"},{"location":"technical/triple-extension/#video-quality-checklist","title":"Video Quality Checklist","text":"<p>Before recording:</p> <ul> <li>\u2705 1080p or 4K resolution</li> <li>\u2705 60fps (better temporal resolution)</li> <li>\u2705 Bright, even lighting</li> <li>\u2705 Clean background</li> <li>\u2705 Contrasting clothing</li> <li>\u2705 Slightly oblique camera angle (~80\u00b0, not 90\u00b0)</li> <li>\u2705 Stable tripod</li> <li>\u2705 Full body in frame throughout jump</li> </ul>"},{"location":"technical/triple-extension/#validation","title":"Validation","text":"<p>The triple extension feature has been tested with:</p> <p>\u2705 Real CMJ video (samples/cmjs/cmj.mp4) \u2705 Handles missing landmarks gracefully \u2705 Shows trunk angle throughout (100% visibility) \u2705 Shows ankle/knee/hip when available (~20-30% of frames) \u2705 All 70 tests passing \u2705 No errors or crashes</p>"},{"location":"technical/triple-extension/#limitations","title":"Limitations","text":"<p>Current implementation:</p> <ul> <li>Joint angles shown only when landmarks detected by MediaPipe</li> <li>In pure lateral view, ankle/knee have low visibility (~20-30%)</li> <li>Trunk angle (hip-shoulder) always available (100% visibility)</li> </ul> <p>Workarounds:</p> <ul> <li>Use slightly oblique camera angle for better detection</li> <li>Focus on trunk angle for lateral videos</li> <li>Use frontal/oblique view if triple extension is primary goal</li> <li>Note: Frontal view reduces jump height accuracy!</li> </ul> <p>Future improvements:</p> <ul> <li>Could interpolate missing joint positions using IK</li> <li>Could use temporal smoothing to fill gaps</li> <li>Could estimate joint positions from hip-heel trajectory</li> </ul>"},{"location":"technical/triple-extension/#example-output","title":"Example Output","text":"<p>Debug video shows:</p> <pre><code>Frame 140-155 (Concentric phase):\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 TRIPLE EXTENSION    \u2502\n  \u2502 Ankle:  N/A         \u2502 \u2190 Not detected\n  \u2502 Knee:   N/A         \u2502 \u2190 Not detected\n  \u2502 Hip:    N/A         \u2502 \u2190 Not detected\n  \u2502 Trunk:  12\u00b0         \u2502 \u2190 Always shown! \u2713\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nFrame 160-165 (Flight):\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 TRIPLE EXTENSION    \u2502\n  \u2502 Ankle:  118\u00b0 \ud83d\udfe0     \u2502 \u2190 Detected!\n  \u2502 Knee:   172\u00b0 \ud83d\udfe2     \u2502 \u2190 Extended\n  \u2502 Hip:    175\u00b0 \ud83d\udfe2     \u2502 \u2190 Extended\n  \u2502 Trunk:  5\u00b0          \u2502 \u2190 Vertical\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"technical/triple-extension/#conclusion","title":"Conclusion","text":"<p>Triple extension tracking is a valuable coaching tool that:</p> <ul> <li>\u2705 Works well when MediaPipe detects ankle/knee</li> <li>\u2705 Shows trunk angle throughout entire video</li> <li>\u2705 Provides visual feedback on technique</li> <li>\u26a0\ufe0f Limited by MediaPipe detection in pure lateral view</li> <li>\ud83d\udca1 Works better with slightly oblique camera angle</li> </ul> <p>For this CMJ video: Trunk angle available 100% of the time, ankle/knee angles available ~20-30% (when visible during flight phase).</p> <p>Kinemotion CMJ Module - Triple Extension Feature Biomechanical analysis with joint angle tracking</p>"},{"location":"translations/es/camera-setup/","title":"Gu\u00eda de Configuraci\u00f3n de C\u00e1mara","text":"<p>English version available: camera-setup.md</p> <p>Esta gu\u00eda proporciona las mejores pr\u00e1cticas para grabar videos de drop jumps y CMJ para asegurar un an\u00e1lisis preciso con kinemotion.</p>"},{"location":"translations/es/camera-setup/#descripcion-general","title":"Descripci\u00f3n General","text":"<p>Kinemotion ahora utiliza posicionamiento de c\u00e1mara a 45\u00b0 como configuraci\u00f3n est\u00e1ndar, proporcionando mejor visibilidad de puntos de referencia y precisi\u00f3n de rastreo comparado con vistas laterales puras. Esta gu\u00eda cubre:</p> <ol> <li>Un iPhone a 45\u00b0 (configuraci\u00f3n est\u00e1ndar recomendada)</li> <li>Configuraci\u00f3n est\u00e9reo con dos iPhones (avanzado - para mayor precisi\u00f3n)</li> </ol> <p>\u00bfPor qu\u00e9 45\u00b0 en lugar de lateral (90\u00b0)?</p> <p>La investigaci\u00f3n muestra que el \u00e1ngulo de visi\u00f3n de la c\u00e1mara afecta significativamente la precisi\u00f3n de la estimaci\u00f3n de pose. El \u00e1ngulo de 45\u00b0 proporciona:</p> <ul> <li>Mejor visibilidad: 40-60% de visibilidad de tobillo/rodilla vs 18-27% en vista lateral</li> <li>Oclusi\u00f3n reducida: Ambas piernas m\u00e1s visibles (menos auto-oclusi\u00f3n)</li> <li>Buena captura del plano sagital: A\u00fan mide altura de salto y movimiento vertical con precisi\u00f3n</li> <li>Compromiso pr\u00e1ctico: Entre frontal (alta visibilidad, pobre profundidad) y lateral (sagital puro, alta oclusi\u00f3n)</li> </ul>"},{"location":"translations/es/camera-setup/#configuracion-1-un-iphone-a-45-estandar","title":"Configuraci\u00f3n 1: Un iPhone a 45\u00b0 (Est\u00e1ndar)","text":""},{"location":"translations/es/camera-setup/#posicionamiento-de-camara","title":"Posicionamiento de C\u00e1mara","text":"<p>Recomendado para: La mayor\u00eda de usuarios, entornos de entrenamiento, evaluaci\u00f3n de atletas individuales</p>"},{"location":"translations/es/camera-setup/#diagrama-vista-superior-una-camara","title":"Diagrama Vista Superior (Una C\u00e1mara)","text":"<pre><code>                    N (Norte - Atleta mira hacia adelante)\n                    \u2191\n\n        [Caj\u00f3n]     |\n            |       |\n            \u2193       |\n           \u2b24 Atleta (salta arriba/abajo)\n            \u2198\n             \u2198 \u00e1ngulo 45\u00b0\n              \u2198\n            [iPhone en Tr\u00edpode]\n\nVisualizaci\u00f3n vista lateral:\n\n    Atleta            iPhone\n       \u2b24  - - - - - - [\ud83d\udcf1]\n                      \u2191\n                   3-5m distancia\n                   Altura de cadera (130-150cm)\n</code></pre> <p>Posicionamiento clave:</p> <ul> <li>\u00c1ngulo: 45\u00b0 del plano sagital del atleta (entre lateral y frontal)</li> <li>Distancia: 3-5 metros (\u00f3ptimo: 4 metros)</li> <li>Altura: Nivel de cadera (130-150 cm del suelo)</li> <li>Orientaci\u00f3n: Modo horizontal (apaisado)</li> </ul>"},{"location":"translations/es/camera-setup/#instrucciones-detalladas-de-configuracion","title":"Instrucciones Detalladas de Configuraci\u00f3n","text":""},{"location":"translations/es/camera-setup/#1-colocacion-fisica","title":"1. Colocaci\u00f3n F\u00edsica","text":"<p>Paso a paso:</p> <ol> <li>Posicione al atleta en el caj\u00f3n - El atleta debe estar en su posici\u00f3n de salto</li> <li>Identifique el plano sagital - Imagine una l\u00ednea de adelante hacia atr\u00e1s a trav\u00e9s del centro del atleta</li> <li>Marque la posici\u00f3n de 45\u00b0 - Desde el lateral del atleta, mu\u00e9vase 45\u00b0 hacia el frente</li> <li>Si el atleta mira al Norte, la c\u00e1mara debe estar al Sureste o Suroeste</li> <li>La c\u00e1mara ve el frente-lateral del atleta (no perfil puro)</li> <li>Establezca la distancia - Mida 3-5m desde la posici\u00f3n de salto del atleta</li> <li>Establezca la altura - Lente de c\u00e1mara a altura de cadera del atleta (t\u00edpicamente 130-150 cm)</li> <li>Nivele el tr\u00edpode - Asegure que la c\u00e1mara est\u00e9 nivelada (no inclinada arriba/abajo)</li> </ol>"},{"location":"translations/es/camera-setup/#2-composicion-del-encuadre","title":"2. Composici\u00f3n del Encuadre","text":"<p>A 1080p (1920x1080), encuadre al atleta as\u00ed:</p> <pre><code>|--------------------------|\n|  [10-15% margen arriba]  |\n|                          |\n|         \ud83d\udc64 Atleta        | \u2190 Cuerpo completo visible\n|          \u2195               | \u2190 Altura completa del salto\n|         / \\              | \u2190 Ambas piernas visibles\n|        /   \\             |\n|    [\u00c1rea de aterrizaje]  | \u2190 Suelo visible\n| [10-15% margen abajo]    |\n|--------------------------|\n</code></pre> <p>Lista de verificaci\u00f3n:</p> <ul> <li>\u2705 Cuerpo entero visible (cabeza a pies)</li> <li>\u2705 10-15% margen sobre la cabeza (para altura de salto)</li> <li>\u2705 Superficie de aterrizaje visible en el encuadre</li> <li>\u2705 Atleta permanece centrado durante todo el movimiento</li> <li>\u2705 Ambas piernas visibles (ventaja clave del \u00e1ngulo de 45\u00b0)</li> <li>\u274c No corte partes del cuerpo</li> <li>\u274c No haga paneo o zoom durante la grabaci\u00f3n</li> </ul>"},{"location":"translations/es/camera-setup/#3-configuracion-de-camara","title":"3. Configuraci\u00f3n de C\u00e1mara","text":"Configuraci\u00f3n Especificaci\u00f3n Raz\u00f3n Resoluci\u00f3n 1080p (1920x1080) M\u00ednimo para detecci\u00f3n precisa de puntos de referencia Velocidad de Cuadros 60 fps (30 fps m\u00ednimo) 60 fps recomendado para tiempos de contacto cortos Orientaci\u00f3n Horizontal (apaisado) Campo de visi\u00f3n m\u00e1s amplio Enfoque Manual (bloqueado en atleta) Previene b\u00fasqueda de autoenfoque Exposici\u00f3n Bloqueada/manual Brillo consistente durante todo el video Velocidad de Obturaci\u00f3n 1/120s o m\u00e1s r\u00e1pido Reduce desenfoque de movimiento Estabilizaci\u00f3n Tr\u00edpode (requerido) Elimina vibraci\u00f3n de c\u00e1mara <p>Configuraciones espec\u00edficas de iPhone:</p> <pre><code>App C\u00e1mara \u2192 Ajustes:\n- Formato: M\u00e1s Compatible (H.264)\n- Grabar Video: 1080p a 60fps\n- Bloquear Enfoque: Toque y mantenga en el atleta\n- Bloquear Exposici\u00f3n: Deslice arriba/abajo para ajustar, luego bloquee\n</code></pre>"},{"location":"translations/es/camera-setup/#4-iluminacion","title":"4. Iluminaci\u00f3n","text":"<p>Mejores pr\u00e1cticas:</p> <ul> <li>Iluminaci\u00f3n uniforme sobre el cuerpo del atleta</li> <li>Evite contraluz (atleta como silueta)</li> <li>Interior: Luces de gimnasio generalmente suficientes</li> <li>Exterior: Condiciones nubladas ideales (luz suave y uniforme)</li> </ul> <p>Por qu\u00e9 importa: MediaPipe depende del contraste visual. La iluminaci\u00f3n deficiente reduce las puntuaciones de visibilidad de puntos de referencia y la precisi\u00f3n del an\u00e1lisis.</p>"},{"location":"translations/es/camera-setup/#5-fondo","title":"5. Fondo","text":"<p>\u00d3ptimo:</p> <ul> <li>Pared simple o fondo de color s\u00f3lido</li> <li>Alto contraste con la ropa del atleta</li> <li>Movimiento m\u00ednimo en el fondo</li> </ul> <p>Evite:</p> <ul> <li>Fondos ocupados (equipamiento, otras personas)</li> <li>Colores similares a la ropa del atleta</li> <li>Superficies reflectivas (espejos, ventanas)</li> </ul>"},{"location":"translations/es/camera-setup/#rendimiento-esperado","title":"Rendimiento Esperado","text":"<p>Mejoras sobre vista lateral (90\u00b0):</p> M\u00e9trica Vista Lateral (90\u00b0) \u00c1ngulo 45\u00b0 Mejora Visibilidad Tobillo/Rodilla 18-27% 40-60% +100-150% Precisi\u00f3n \u00c1ngulo Articular ~10-15\u00b0 error ~8-12\u00b0 error ~20-30% mejor Confiabilidad de Detecci\u00f3n Buena Excelente M\u00e1s consistente Detecci\u00f3n Contacto con Suelo Desafiante M\u00e1s f\u00e1cil M\u00e1s robusto <p>Limitaciones:</p> <ul> <li>A\u00fan monocular (estimaci\u00f3n de profundidad ruidosa)</li> <li>Sin restricciones biomec\u00e1nicas (vs Pose2Sim)</li> <li>No de grado de investigaci\u00f3n (para eso, use configuraci\u00f3n de doble c\u00e1mara)</li> </ul>"},{"location":"translations/es/camera-setup/#lista-de-verificacion-de-configuracion-de-camara","title":"Lista de Verificaci\u00f3n de Configuraci\u00f3n de C\u00e1mara","text":"<p>Antes de grabar, verifique:</p> <ul> <li>[ ] iPhone en tr\u00edpode estable (sin movimiento durante grabaci\u00f3n)</li> <li>[ ] C\u00e1mara a 45\u00b0 del plano sagital del atleta</li> <li>[ ] Distancia: 3-5 metros del \u00e1rea de aterrizaje</li> <li>[ ] Altura: Lente de c\u00e1mara a altura de cadera del atleta (130-150cm)</li> <li>[ ] Encuadre: Cuerpo completo visible (cabeza a pies + m\u00e1rgenes 10-15%)</li> <li>[ ] Configuraci\u00f3n: 1080p, 60 fps, orientaci\u00f3n horizontal</li> <li>[ ] Enfoque: Bloqueado en atleta (toque y mantenga)</li> <li>[ ] Exposici\u00f3n: Bloqueada (iluminaci\u00f3n consistente)</li> <li>[ ] Iluminaci\u00f3n: Uniforme, sin sombras marcadas ni contraluz</li> <li>[ ] Fondo: Simple, m\u00ednimas distracciones</li> <li>[ ] Grabaci\u00f3n de prueba: Atleta permanece en encuadre durante todo el salto</li> </ul>"},{"location":"translations/es/camera-setup/#configuracion-2-estereo-con-dos-iphones-avanzado","title":"Configuraci\u00f3n 2: Est\u00e9reo con Dos iPhones (Avanzado)","text":""},{"location":"translations/es/camera-setup/#cuando-usar-configuracion-de-doble-camara","title":"Cu\u00e1ndo Usar Configuraci\u00f3n de Doble C\u00e1mara","text":"<p>Recomendado para:</p> <ul> <li>Aplicaciones de investigaci\u00f3n que requieren mayor precisi\u00f3n</li> <li>Evaluaci\u00f3n de atletas de \u00e9lite</li> <li>Cuando la precisi\u00f3n de profundidad es cr\u00edtica</li> <li>An\u00e1lisis biomec\u00e1nico que requiere \u00e1ngulos articulares</li> </ul> <p>Beneficios sobre c\u00e1mara \u00fanica:</p> <ul> <li>~50% reducci\u00f3n de error (30.1mm RMSE vs 56.3mm monocular)</li> <li>Reconstrucci\u00f3n 3D precisa (elimina ambig\u00fcedad de profundidad)</li> <li>Mejor visibilidad de puntos de referencia (cada c\u00e1mara ve \u00e1ngulos diferentes)</li> <li>Precisi\u00f3n de grado de investigaci\u00f3n (con calibraci\u00f3n y procesamiento adecuados)</li> </ul> <p>Requisitos:</p> <ul> <li>2 iPhones (se recomienda mismo modelo para configuraciones coincidentes)</li> <li>2 tr\u00edpodes</li> <li>Patr\u00f3n de calibraci\u00f3n (tablero ChArUco o tablero de ajedrez)</li> <li>Flujo de trabajo de procesamiento m\u00e1s complejo</li> </ul>"},{"location":"translations/es/camera-setup/#posicionamiento-de-camaras","title":"Posicionamiento de C\u00e1maras","text":""},{"location":"translations/es/camera-setup/#configuracion-optima-45-del-plano-sagital-separacion-de-90","title":"Configuraci\u00f3n \u00f3ptima: \u00b145\u00b0 del plano sagital, separaci\u00f3n de 90\u00b0","text":""},{"location":"translations/es/camera-setup/#diagrama-vista-superior-doble-camara","title":"Diagrama Vista Superior (Doble C\u00e1mara)","text":"<pre><code>                    N (Atleta mira hacia adelante)\n                    \u2191\n\n    [iPhone 2]      |      [iPhone 1]\n    (Lado izq.)     |      (Lado der.)\n         \u2198          |          \u2199\n          \u2198 45\u00b0     |      45\u00b0 \u2199\n           \u2198        |        \u2199\n             \u2198   [Caj\u00f3n]   \u2199\n               \u2198    |   \u2199\n                 \u2198  \u2193 \u2199\n                   \u2b24 Atleta\n\n    Separaci\u00f3n total: 90\u00b0 (\u00f3ptimo para triangulaci\u00f3n)\n</code></pre> <p>\u00bfPor qu\u00e9 separaci\u00f3n de 90\u00b0?</p> <p>La investigaci\u00f3n de Pagnon et al. (2022) y Dill et al. (2024) encontr\u00f3 que un \u00e1ngulo de 90\u00b0 entre c\u00e1maras es \u00f3ptimo para reconstrucci\u00f3n 3D est\u00e9reo. Esto balancea:</p> <ul> <li>Precisi\u00f3n de triangulaci\u00f3n (\u00e1ngulos m\u00e1s amplios mejor)</li> <li>Campo de visi\u00f3n superpuesto (c\u00e1maras deben ver los mismos puntos de referencia)</li> <li>Restricciones pr\u00e1cticas de configuraci\u00f3n</li> </ul>"},{"location":"translations/es/camera-setup/#configuracion-detallada-de-doble-camara","title":"Configuraci\u00f3n Detallada de Doble C\u00e1mara","text":""},{"location":"translations/es/camera-setup/#paso-1-posicionar-ambas-camaras","title":"Paso 1: Posicionar Ambas C\u00e1maras","text":"<p>iPhone 1 (C\u00e1mara derecha):</p> <ul> <li>Posicionar a 45\u00b0 del lado derecho del atleta</li> <li>Si el atleta mira al Norte, la c\u00e1mara est\u00e1 al Sureste</li> <li>Distancia: 3-5m del atleta</li> <li>Altura: Nivel de cadera (130-150cm)</li> </ul> <p>iPhone 2 (C\u00e1mara izquierda):</p> <ul> <li>Posicionar a 45\u00b0 del lado izquierdo del atleta</li> <li>Si el atleta mira al Norte, la c\u00e1mara est\u00e1 al Suroeste</li> <li>Distancia: 3-5m del atleta (igual que iPhone 1)</li> <li>Altura: Nivel de cadera (igualar iPhone 1 exactamente)</li> </ul> <p>Alineaci\u00f3n cr\u00edtica:</p> <ul> <li>Ambas c\u00e1maras a la misma altura (tolerancia \u00b12cm)</li> <li>Ambas c\u00e1maras a la misma distancia del atleta (tolerancia \u00b110cm)</li> <li>Ambas c\u00e1maras niveladas (no inclinadas)</li> <li>Separaci\u00f3n de 90\u00b0 entre c\u00e1maras (tolerancia \u00b15\u00b0)</li> </ul>"},{"location":"translations/es/camera-setup/#paso-2-composicion-del-encuadre-ambas-camaras","title":"Paso 2: Composici\u00f3n del Encuadre (Ambas C\u00e1maras)","text":"<p>Ambos iPhones deben encuadrar al atleta id\u00e9nticamente:</p> <pre><code>Vista de cada c\u00e1mara:\n|------------------------|\n|   [margen]             |\n|      \ud83d\udc64 Cuerpo comp.   | \u2190 Mismo encuadre\n|       \u2195 Altura salto   | \u2190 Ambas c\u00e1maras\n|      / \\               |\n|  [\u00c1rea aterrizaje]     |\n|   [margen]             |\n|------------------------|\n</code></pre> <p>Sincronizar encuadre:</p> <ul> <li>Atleta centrado en ambos encuadres</li> <li>Mismos m\u00e1rgenes (10-15% arriba/abajo)</li> <li>Ambas ven secuencia completa de salto</li> <li>\u00c1rea de aterrizaje visible en ambas</li> </ul>"},{"location":"translations/es/camera-setup/#paso-3-configuracion-de-camara-ambos-iphones","title":"Paso 3: Configuraci\u00f3n de C\u00e1mara (Ambos iPhones)","text":""},{"location":"translations/es/camera-setup/#critico-ambas-camaras-deben-tener-configuraciones-identicas","title":"CR\u00cdTICO: Ambas c\u00e1maras deben tener configuraciones id\u00e9nticas","text":"Configuraci\u00f3n Ambas C\u00e1maras Resoluci\u00f3n 1080p (1920x1080) - exactamente igual Velocidad de Cuadros 60 fps - exactamente igual Orientaci\u00f3n Horizontal - exactamente igual Enfoque Manual, bloqueado Exposici\u00f3n Manual, bloqueada (mismo brillo) Formato H.264, M\u00e1s Compatible <p>Por qu\u00e9 importan configuraciones id\u00e9nticas:</p> <ul> <li>La sincronizaci\u00f3n requiere velocidades de cuadros coincidentes</li> <li>La triangulaci\u00f3n asume la misma resoluci\u00f3n</li> <li>Diferentes exposiciones afectan la detecci\u00f3n de puntos de referencia</li> </ul>"},{"location":"translations/es/camera-setup/#paso-4-sincronizacion","title":"Paso 4: Sincronizaci\u00f3n","text":""},{"location":"translations/es/camera-setup/#opcion-a-inicio-manual-simple","title":"Opci\u00f3n A: Inicio manual (simple)","text":"<ol> <li>Iniciar grabaci\u00f3n en iPhone 1</li> <li>Iniciar grabaci\u00f3n en iPhone 2 dentro de 1-2 segundos</li> <li>Se\u00f1al de sincronizaci\u00f3n: Que el atleta aplauda o salte una vez antes de la prueba real</li> <li>Usar este evento para sincronizar videos en post-procesamiento</li> </ol>"},{"location":"translations/es/camera-setup/#opcion-b-sincronizacion-de-audio-mejor","title":"Opci\u00f3n B: Sincronizaci\u00f3n de audio (mejor)","text":"<ol> <li>Usar se\u00f1al de audio externa (aplauso, pitido, comando de voz)</li> <li>Ambos iPhones graban audio</li> <li>Alinear videos usando forma de onda de audio en post-procesamiento</li> <li>Software como Pose2Sim tiene herramientas de sincronizaci\u00f3n incorporadas</li> </ol>"},{"location":"translations/es/camera-setup/#opcion-c-sincronizacion-por-hardware-mejor-requiere-equipo","title":"Opci\u00f3n C: Sincronizaci\u00f3n por hardware (mejor, requiere equipo)","text":"<ol> <li>Usar dispositivo de disparo externo</li> <li>Inicia ambas c\u00e1maras simult\u00e1neamente</li> <li>Sincronizaci\u00f3n m\u00e1s precisa</li> <li>Requiere hardware adicional</li> </ol> <p>Recomendaci\u00f3n: Comience con Opci\u00f3n A (manual + aplauso), actualice a Opci\u00f3n B si es necesario.</p>"},{"location":"translations/es/camera-setup/#paso-5-calibracion","title":"Paso 5: Calibraci\u00f3n","text":"<p>Requerido: Calibraci\u00f3n \u00fanica antes del primer uso o si cambian las posiciones de c\u00e1mara</p> <p>Opciones de patr\u00f3n de calibraci\u00f3n:</p> <ol> <li> <p>Tablero ChArUco (recomendado - m\u00e1s robusto)</p> </li> <li> <p>Imprimir patr\u00f3n ChArUco grande (A3 o mayor)</p> </li> <li>Montar en tablero r\u00edgido</li> <li> <p>Tama\u00f1o de cuadr\u00edcula: 7x5 o similar</p> </li> <li> <p>Tablero de ajedrez (alternativa)</p> </li> <li> <p>Imprimir tablero de ajedrez grande (A3 o mayor)</p> </li> <li>Cuadr\u00edcula 8x6 o 9x7</li> <li>Asegurar perfectamente plano</li> </ol> <p>Procedimiento de calibraci\u00f3n:</p> <pre><code># Si usa Pose2Sim\n1. Grabar patr\u00f3n de calibraci\u00f3n desde ambas c\u00e1maras\n2. Mover patr\u00f3n a trav\u00e9s del volumen de captura (10-15 posiciones diferentes)\n3. Asegurar que el patr\u00f3n sea visible en ambas c\u00e1maras simult\u00e1neamente\n4. Ejecutar calibraci\u00f3n:\n   Pose2Sim.calibration()\n</code></pre> <p>Salidas de calibraci\u00f3n:</p> <ul> <li>Intr\u00ednsecos de c\u00e1mara (distancia focal, distorsi\u00f3n)</li> <li>Extr\u00ednsecos de c\u00e1mara (posiciones relativas, rotaci\u00f3n)</li> <li>Se guarda en archivo de calibraci\u00f3n para reutilizaci\u00f3n</li> </ul> <p>Re-calibrar cuando:</p> <ul> <li>Las posiciones de c\u00e1mara cambien</li> <li>Se usen diferentes lentes</li> <li>Despu\u00e9s de varias semanas (verificaci\u00f3n de deriva)</li> </ul>"},{"location":"translations/es/camera-setup/#procesamiento-de-videos-de-doble-camara","title":"Procesamiento de Videos de Doble C\u00e1mara","text":"<p>Soporte actual de kinemotion: Solo c\u00e1mara \u00fanica</p> <p>Para procesar videos est\u00e9reo, necesitar\u00e1:</p>"},{"location":"translations/es/camera-setup/#opcion-a-usar-pose2sim-recomendado","title":"Opci\u00f3n A: Usar Pose2Sim (recomendado)","text":"<pre><code># Instalar Pose2Sim\npip install pose2sim\n\n# Procesar videos est\u00e9reo\nPose2Sim.calibration()      # Una vez\nPose2Sim.poseEstimation()   # Ejecutar MediaPipe en ambos videos\nPose2Sim.synchronization()  # Sincronizar videos\nPose2Sim.triangulation()    # Reconstrucci\u00f3n 3D\nPose2Sim.filtering()        # Suavizar trayectorias\nPose2Sim.kinematics()       # \u00c1ngulos articulares OpenSim\n</code></pre>"},{"location":"translations/es/camera-setup/#opcion-b-futuro-soporte-estereo-de-kinemotion","title":"Opci\u00f3n B: Futuro soporte est\u00e9reo de kinemotion","text":"<p>El soporte de doble c\u00e1mara puede ser agregado a kinemotion en versiones futuras. Hoja de ruta actual:</p> <ul> <li>M\u00f3dulo de triangulaci\u00f3n est\u00e9reo</li> <li>Sincronizaci\u00f3n autom\u00e1tica</li> <li>Flujo de trabajo de calibraci\u00f3n integrado</li> </ul>"},{"location":"translations/es/camera-setup/#opcion-c-triangulacion-manual","title":"Opci\u00f3n C: Triangulaci\u00f3n manual","text":"<p>Si tiene experiencia en programaci\u00f3n, implemente triangulaci\u00f3n est\u00e9reo usando OpenCV y la salida de MediaPipe de ambas c\u00e1maras.</p>"},{"location":"translations/es/camera-setup/#rendimiento-esperado-doble-camara","title":"Rendimiento Esperado (Doble C\u00e1mara)","text":"<p>Mejoras de precisi\u00f3n sobre c\u00e1mara \u00fanica:</p> M\u00e9trica C\u00e1mara \u00danica (45\u00b0) Doble C\u00e1mara (Est\u00e9reo) Mejora RMSE de Posici\u00f3n ~56mm ~30mm 47% mejor Error de \u00c1ngulo Articular ~8-12\u00b0 ~5-7\u00b0 ~30-40% mejor Precisi\u00f3n de Profundidad Pobre (ruidosa) Buena Elimina ambig\u00fcedad Visibilidad de Puntos 40-60% 70-90% Cobertura multi-\u00e1ngulo <p>Investigaci\u00f3n validada:</p> <ul> <li>Dill et al. (2024): MediaPipe est\u00e9reo logr\u00f3 30.1mm RMSE vs est\u00e1ndar de oro Qualisys</li> <li>Pagnon et al. (2022): Separaci\u00f3n de c\u00e1mara de 90\u00b0 \u00f3ptima para triangulaci\u00f3n</li> </ul>"},{"location":"translations/es/camera-setup/#lista-de-verificacion-de-doble-camara","title":"Lista de Verificaci\u00f3n de Doble C\u00e1mara","text":"<p>Antes de grabar, verifique:</p> <ul> <li>[ ] Ambos iPhones en tr\u00edpodes estables</li> <li>[ ] C\u00e1mara 1 a +45\u00b0 del lado derecho del atleta</li> <li>[ ] C\u00e1mara 2 a -45\u00b0 del lado izquierdo del atleta</li> <li>[ ] Separaci\u00f3n total de 90\u00b0 entre c\u00e1maras</li> <li>[ ] Misma distancia (3-5m) del atleta para ambas c\u00e1maras</li> <li>[ ] Misma altura (nivel de cadera, 130-150cm) para ambas c\u00e1maras</li> <li>[ ] Ambas niveladas (no inclinadas arriba/abajo)</li> <li>[ ] Configuraciones id\u00e9nticas (1080p, 60fps, horizontal)</li> <li>[ ] Enfoque y exposici\u00f3n id\u00e9nticos bloqueados</li> <li>[ ] M\u00e9todo de sincronizaci\u00f3n planeado (aplauso, se\u00f1al de audio, etc.)</li> <li>[ ] Calibraci\u00f3n completada (una vez)</li> <li>[ ] Grabaci\u00f3n de prueba desde ambas c\u00e1maras simult\u00e1neamente</li> </ul>"},{"location":"translations/es/camera-setup/#configuracion-de-grabacion-ambas-configuraciones","title":"Configuraci\u00f3n de Grabaci\u00f3n (Ambas Configuraciones)","text":""},{"location":"translations/es/camera-setup/#especificaciones-de-video","title":"Especificaciones de Video","text":"Configuraci\u00f3n Requisito Recomendaci\u00f3n Raz\u00f3n Resoluci\u00f3n 1080p m\u00ednimo 1080p (1920x1080) Mayor resoluci\u00f3n mejora precisi\u00f3n de MediaPipe Velocidad de Cuadros 30 fps m\u00ednimo 60 fps Mejor para tiempos de contacto cortos (150-250ms) Orientaci\u00f3n Solo horizontal Horizontal Campo de visi\u00f3n m\u00e1s amplio para movimiento de salto Formato MP4, MOV, AVI MP4 (H.264) Compatibilidad universal Bitrate M\u00e1s alto mejor Auto o 50+ Mbps Preserva detalle durante movimiento"},{"location":"translations/es/camera-setup/#por-que-60-fps-vs-30-fps","title":"\u00bfPor qu\u00e9 60 fps vs 30 fps?","text":"<p>Para drop jumps y CMJ:</p> M\u00e9trica 30 fps 60 fps Resoluci\u00f3n temporal 33.3ms por cuadro 16.7ms por cuadro Muestreo contacto con suelo 5-8 cuadros 10-15 cuadros Error de medici\u00f3n de tiempo \u00b133ms \u00b116ms Precisi\u00f3n de velocidad Buena Mejor <p>Tiempos de contacto con suelo en drop jumps: 150-250ms</p> <ul> <li>A 30 fps: Solo 5-8 muestras durante contacto</li> <li>A 60 fps: 10-15 muestras durante contacto (2x mejor)</li> </ul> <p>Recomendaci\u00f3n: Use 60 fps si su iPhone lo soporta. La mejora en precisi\u00f3n justifica el tama\u00f1o de archivo mayor.</p>"},{"location":"translations/es/camera-setup/#configuraciones-de-camara-de-iphone","title":"Configuraciones de C\u00e1mara de iPhone","text":"<p>C\u00f3mo configurar iPhone para grabaci\u00f3n \u00f3ptima:</p> <ol> <li>Abrir app C\u00e1mara</li> <li>Ajustes \u2192 C\u00e1mara \u2192 Grabar Video</li> <li>Seleccionar: 1080p a 60 fps (o 30 fps si 60 no disponible)</li> <li>Ajustes \u2192 C\u00e1mara \u2192 Formatos</li> <li>Seleccionar: M\u00e1s Compatible (H.264, no HEVC)</li> <li>Antes de grabar:</li> <li>Bloquear enfoque: Toque y mantenga en atleta hasta que aparezca \"Bloqueo AE/AF\"</li> <li>Bloquear exposici\u00f3n: Deslice arriba/abajo para ajustar brillo, luego mantenga bloqueado</li> <li>Composici\u00f3n de encuadre:</li> <li>Posicionar atleta en el centro</li> <li>Asegurar cuerpo completo visible con m\u00e1rgenes</li> <li>Iniciar grabaci\u00f3n antes de que el atleta comience la secuencia de salto</li> </ol> <p>Consejo Profesional: Grabe un video de prueba primero y verifique:</p> <ul> <li>Atleta permanece en encuadre</li> <li>Enfoque permanece n\u00edtido</li> <li>Iluminaci\u00f3n es adecuada</li> <li>Sin desenfoque de movimiento</li> </ul>"},{"location":"translations/es/camera-setup/#guias-de-iluminacion","title":"Gu\u00edas de Iluminaci\u00f3n","text":""},{"location":"translations/es/camera-setup/#grabacion-interior","title":"Grabaci\u00f3n Interior","text":"<p>Recomendado:</p> <ul> <li>Luces de gimnasio superiores (t\u00edpicamente 400-800 lux suficiente)</li> <li>Iluminaci\u00f3n uniforme a trav\u00e9s del \u00e1rea de salto</li> <li>Evite crear sombra del atleta en el fondo</li> </ul> <p>Verificar:</p> <ul> <li>Cara y articulaciones del atleta claramente visibles</li> <li>Sin sombras marcadas en el cuerpo</li> <li>Sin puntos brillantes (ventanas, superficies reflectivas)</li> </ul>"},{"location":"translations/es/camera-setup/#grabacion-exterior","title":"Grabaci\u00f3n Exterior","text":"<p>Mejores condiciones:</p> <ul> <li>D\u00eda nublado (iluminaci\u00f3n suave y uniforme)</li> <li>Evite sol del mediod\u00eda (sombras marcadas)</li> <li>Evite tarde (\u00e1ngulo bajo, sombras largas)</li> </ul> <p>Posicionamiento:</p> <ul> <li>Sol detr\u00e1s o al lado de las c\u00e1maras</li> <li>Atleta no a contraluz (silueta)</li> <li>Considere hora del d\u00eda para iluminaci\u00f3n consistente</li> </ul>"},{"location":"translations/es/camera-setup/#guias-de-fondo","title":"Gu\u00edas de Fondo","text":"<p>Fondo \u00f3ptimo:</p> <ul> <li>Pared simple (color neutro)</li> <li>Contraste con ropa del atleta</li> <li>Sin patrones o elementos ocupados</li> <li>Est\u00e1tico (sin movimiento)</li> </ul> <p>Ejemplos de contraste de color:</p> <ul> <li>Atleta con ropa oscura \u2192 fondo claro (pared blanca/gris)</li> <li>Atleta con ropa clara \u2192 fondo oscuro (pared azul/gris)</li> <li>Evite: Atleta en blanco \u2192 fondo blanco (bajo contraste)</li> </ul> <p>Por qu\u00e9 importa: MediaPipe separa figura del fondo. Alto contraste mejora precisi\u00f3n de detecci\u00f3n de puntos de referencia y reduce falsos positivos.</p>"},{"location":"translations/es/camera-setup/#errores-comunes-a-evitar","title":"Errores Comunes a Evitar","text":""},{"location":"translations/es/camera-setup/#camara-no-a-angulo-de-45","title":"\u274c C\u00e1mara No a \u00c1ngulo de 45\u00b0","text":"<pre><code>\u274c INCORRECTO: Lateral puro (90\u00b0)\n         [Atleta]\n             |\n             |\n    [C\u00e1mara]\u2190\u2518\n\n\u274c INCORRECTO: Frontal puro (0\u00b0)\n    [C\u00e1mara]\n       \u2193\n    [Atleta]\n\n\u2705 CORRECTO: \u00c1ngulo de 45\u00b0\n         [Atleta]\n             \u2198\n              \u2198 45\u00b0\n            [C\u00e1mara]\n</code></pre> <p>Problema con lateral: Alta oclusi\u00f3n, baja visibilidad de tobillo/rodilla Problema con frontal: Ambig\u00fcedad de profundidad, medici\u00f3n de altura de salto pobre Soluci\u00f3n: Use \u00e1ngulo de 45\u00b0 como se especifica</p>"},{"location":"translations/es/camera-setup/#camara-demasiado-cerca-3m","title":"\u274c C\u00e1mara Demasiado Cerca (\\&lt;3m)","text":"<p>Problemas:</p> <ul> <li>Distorsi\u00f3n de perspectiva (efecto gran angular)</li> <li>Riesgo de que atleta salga del encuadre</li> <li>Distorsi\u00f3n de lente en bordes (l\u00edneas curvas)</li> </ul> <p>Soluci\u00f3n: Mantener distancia de 3-5m</p>"},{"location":"translations/es/camera-setup/#camara-demasiado-alta-o-baja","title":"\u274c C\u00e1mara Demasiado Alta o Baja","text":"<pre><code>\u274c Muy alta (mirando hacia abajo):\n    [C\u00e1mara]\n       \u2193 \u2198\n         [Atleta]\n\n\u274c Muy baja (mirando hacia arriba):\n         [Atleta]\n       \u2197 \u2191\n    [C\u00e1mara]\n\n\u2705 Correcta (nivel de cadera):\n    [C\u00e1mara] \u2192 [Atleta]\n</code></pre> <p>Problema: Error de paralaje, proporciones distorsionadas Soluci\u00f3n: Lente de c\u00e1mara a altura de cadera (130-150cm)</p>"},{"location":"translations/es/camera-setup/#encuadre-pobre","title":"\u274c Encuadre Pobre","text":"<p>Errores comunes:</p> <ul> <li>Atleta muy peque\u00f1o en encuadre (c\u00e1mara muy lejos)</li> <li>Atleta cortado durante salto (c\u00e1mara muy cerca o baja)</li> <li>No centrado (atleta se sale del encuadre)</li> </ul> <p>Soluci\u00f3n:</p> <ul> <li>Grabar prueba primero</li> <li>Ajustar encuadre para incluir salto completo con m\u00e1rgenes</li> <li>Marcar posici\u00f3n de salto para asegurar consistencia</li> </ul>"},{"location":"translations/es/camera-setup/#configuraciones-inconsistentes-entre-camaras-duales","title":"\u274c Configuraciones Inconsistentes Entre C\u00e1maras Duales","text":"<p>Solo para configuraci\u00f3n est\u00e9reo:</p> <p>Problemas:</p> <ul> <li>Diferentes velocidades de cuadros \u2192 sincronizaci\u00f3n imposible</li> <li>Diferentes resoluciones \u2192 triangulaci\u00f3n falla</li> <li>Diferentes exposiciones \u2192 detecci\u00f3n de puntos de referencia inconsistente</li> </ul> <p>Soluci\u00f3n: Configurar ambos iPhones id\u00e9nticamente (ver Lista de Verificaci\u00f3n de Doble C\u00e1mara)</p>"},{"location":"translations/es/camera-setup/#resolucion-de-problemas","title":"Resoluci\u00f3n de Problemas","text":""},{"location":"translations/es/camera-setup/#advertencia-de-visibilidad-de-puntos-de-referencia-pobre","title":"Advertencia de \"Visibilidad de Puntos de Referencia Pobre\"","text":"<p>S\u00edntomas: Kinemotion reporta puntuaciones bajas de visibilidad</p> <p>Causas:</p> <ul> <li>Iluminaci\u00f3n insuficiente</li> <li>Bajo contraste con el fondo</li> <li>C\u00e1mara desenfocada</li> <li>Desenfoque de movimiento (velocidad de obturaci\u00f3n muy lenta)</li> </ul> <p>Soluciones:</p> <ol> <li>Agregar fuentes de iluminaci\u00f3n</li> <li>Cambiar fondo o ropa del atleta para contraste</li> <li>Bloquear enfoque en atleta (toque y mantenga)</li> <li>Aumentar velocidad de obturaci\u00f3n (reducir exposici\u00f3n si es necesario)</li> <li>Asegurar resoluci\u00f3n 1080p</li> </ol>"},{"location":"translations/es/camera-setup/#la-altura-del-salto-parece-incorrecta","title":"La Altura del Salto Parece Incorrecta","text":"<p>Posibles causas:</p> <ol> <li>\u00c1ngulo de c\u00e1mara no exactamente 45\u00b0 (error de medici\u00f3n)</li> <li>Falta par\u00e1metro de calibraci\u00f3n <code>--drop-height</code></li> <li>Atleta movi\u00e9ndose horizontalmente (deriva durante salto)</li> <li>C\u00e1mara no nivelada (inclinada)</li> </ol> <p>Soluciones:</p> <ol> <li>Verificar \u00e1ngulo de 45\u00b0 con app de medici\u00f3n o transportador</li> <li>Proporcionar altura del caj\u00f3n: <code>--drop-height 0.40</code></li> <li>Entrenar al atleta para saltar derecho hacia arriba (deriva m\u00ednima)</li> <li>Usar indicador de nivel de tr\u00edpode o app de nivel de tel\u00e9fono</li> </ol>"},{"location":"translations/es/camera-setup/#error-no-se-detecto-drop-jump","title":"Error \"No se Detect\u00f3 Drop Jump\"","text":"<p>Posibles causas:</p> <ol> <li>Video no incluye secuencia completa</li> <li>Atleta cortado en encuadre</li> <li>Calidad de rastreo muy pobre</li> </ol> <p>Soluciones:</p> <ol> <li>Iniciar grabaci\u00f3n antes de que atleta suba al caj\u00f3n</li> <li>Ajustar encuadre - probar con salto de pr\u00e1ctica</li> <li>Mejorar calidad de video (iluminaci\u00f3n, enfoque, resoluci\u00f3n)</li> <li>Usar bandera manual <code>--drop-start-frame</code> si auto-detecci\u00f3n falla</li> </ol>"},{"location":"translations/es/camera-setup/#doble-camara-videos-no-sincronizados","title":"Doble C\u00e1mara: Videos No Sincronizados","text":"<p>S\u00edntomas: Triangulaci\u00f3n falla o produce poses 3D irreales</p> <p>Soluciones:</p> <ol> <li>Verificar que ambos videos tengan velocidades de cuadros id\u00e9nticas</li> <li>Usar se\u00f1al audio/visual para sincronizar (aplauso, pitido)</li> <li>Usar m\u00f3dulo de sincronizaci\u00f3n de Pose2Sim</li> <li>Considerar gatillo de hardware para futuras grabaciones</li> </ol>"},{"location":"translations/es/camera-setup/#recomendaciones-de-equipo","title":"Recomendaciones de Equipo","text":""},{"location":"translations/es/camera-setup/#configuracion-de-camara-unica","title":"Configuraci\u00f3n de C\u00e1mara \u00danica","text":"<p>Opci\u00f3n Econ\u00f3mica ($100-300):</p> <ul> <li>iPhone SE (2020 o posterior) o Android insignia</li> <li>Tr\u00edpode b\u00e1sico con soporte para smartphone ($20-50)</li> <li>Total: ~$150-350</li> </ul> <p>Gama Media ($500-800):</p> <ul> <li>iPhone reciente (11 o posterior) con 4K/60fps</li> <li>Tr\u00edpode de calidad con cabeza fluida ($100-200)</li> <li>Total: ~$600-1000</li> </ul> <p>Lo que necesita:</p> <ul> <li>iPhone capaz de 1080p @ 60fps m\u00ednimo</li> <li>Tr\u00edpode estable (peso ligero OK para uso interior)</li> <li>Indicador de nivel (la mayor\u00eda de tr\u00edpodes tienen nivel de burbuja)</li> </ul>"},{"location":"translations/es/camera-setup/#configuracion-de-doble-camara","title":"Configuraci\u00f3n de Doble C\u00e1mara","text":"<p>Est\u00e9reo Econ\u00f3mico ($300-600):</p> <ul> <li>2x iPhone SE o similar</li> <li>2x tr\u00edpodes b\u00e1sicos</li> <li>Tablero de calibraci\u00f3n (imprimir y montar, \\&lt;$20)</li> <li>Total: ~$350-650</li> </ul> <p>Est\u00e9reo Gama Media ($1000-1600):</p> <ul> <li>2x iPhone reciente (mismo modelo)</li> <li>2x tr\u00edpodes de calidad</li> <li>Tablero de calibraci\u00f3n profesional</li> <li>Opcional: Gatillo de sincronizaci\u00f3n por hardware</li> <li>Total: ~$1200-1800</li> </ul> <p>Lo que necesita:</p> <ul> <li>2 iPhones (mismo modelo muy recomendado)</li> <li>2 tr\u00edpodes estables (ajuste de altura id\u00e9ntico)</li> <li>Patr\u00f3n de calibraci\u00f3n (ChArUco o tablero de ajedrez)</li> <li>Capacidad de procesamiento (laptop/desktop para Pose2Sim)</li> </ul> <p>Comparaci\u00f3n de costo con sistemas de grado de investigaci\u00f3n:</p> <ul> <li>MoCap basado en marcadores (Vicon, Qualisys): $50,000-$500,000</li> <li>Markerless comercial (Theia3D): $5,000-$20,000</li> <li>Doble iPhone + Pose2Sim: $300-$1,800 (\u00a1100x m\u00e1s barato!)</li> </ul>"},{"location":"translations/es/camera-setup/#validacion-y-verificaciones-de-calidad","title":"Validaci\u00f3n y Verificaciones de Calidad","text":""},{"location":"translations/es/camera-setup/#despues-de-grabar","title":"Despu\u00e9s de Grabar","text":"<p>Para cada video, verifique:</p> <ol> <li> <p>Verificaci\u00f3n de reproducci\u00f3n:</p> </li> <li> <p>Secuencia de salto completa capturada</p> </li> <li>Atleta permanece en encuadre</li> <li>Enfoque n\u00edtido durante todo</li> <li> <p>Sin desenfoque de movimiento</p> </li> <li> <p>M\u00e9tricas de calidad:</p> </li> <li> <p>Tama\u00f1o de archivo apropiado (60fps 1080p \u2248 200MB/min)</p> </li> <li>Sin cuadros perdidos (reproducci\u00f3n suave)</li> <li> <p>Audio claro (si se usa para sincronizaci\u00f3n)</p> </li> <li> <p>Prueba de an\u00e1lisis:</p> </li> <li> <p>Ejecutar kinemotion en video</p> </li> <li>Verificar salida de superposici\u00f3n de depuraci\u00f3n</li> <li>Verificar calidad de detecci\u00f3n de puntos de referencia</li> </ol>"},{"location":"translations/es/camera-setup/#indicadores-de-calidad","title":"Indicadores de Calidad","text":"<p>Video de buena calidad (listo para an\u00e1lisis):</p> <ul> <li>\u2705 Puntuaciones de visibilidad de MediaPipe &gt;0.5 promedio</li> <li>\u2705 Rastreo suave de puntos de referencia (jitter m\u00ednimo)</li> <li>\u2705 Todas las fases de salto detectadas autom\u00e1ticamente</li> <li>\u2705 Superposici\u00f3n de depuraci\u00f3n muestra rastreo consistente</li> </ul> <p>Video de calidad pobre (se recomienda re-grabar):</p> <ul> <li>\u274c Puntuaciones de visibilidad \\&lt;0.3 promedio</li> <li>\u274c Posiciones de puntos de referencia err\u00e1tica (p\u00e9rdida de rastreo)</li> <li>\u274c Detecci\u00f3n de fase fallida</li> <li>\u274c Superposici\u00f3n de depuraci\u00f3n muestra huecos o poses irreales</li> </ul>"},{"location":"translations/es/camera-setup/#consejos-avanzados","title":"Consejos Avanzados","text":""},{"location":"translations/es/camera-setup/#para-grabacion-consistente-multi-sesion","title":"Para Grabaci\u00f3n Consistente Multi-Sesi\u00f3n","text":"<p>Crear una configuraci\u00f3n estandarizada:</p> <ol> <li> <p>Marcar posiciones de c\u00e1mara en el suelo con cinta</p> </li> <li> <p>Medir \u00e1ngulo de 45\u00b0 con precisi\u00f3n</p> </li> <li>Marcar c\u00edrculo de distancia de 4m</li> <li> <p>Etiquetar posiciones \"C\u00e1mara 1\" y \"C\u00e1mara 2\"</p> </li> <li> <p>Documentar su configuraci\u00f3n:</p> </li> <li> <p>Tomar fotos de posiciones de c\u00e1mara</p> </li> <li>Anotar configuraciones de altura de tr\u00edpode</li> <li> <p>Guardar captura de pantalla de configuraciones de c\u00e1mara</p> </li> <li> <p>Usar mismo equipo a trav\u00e9s de sesiones</p> </li> <li> <p>Mismo(s) iPhone(s)</p> </li> <li>Misma altura de tr\u00edpode</li> <li>Misma habitaci\u00f3n/ubicaci\u00f3n si es posible</li> </ol> <p>Beneficios:</p> <ul> <li>Mediciones consistentes a trav\u00e9s del tiempo</li> <li>M\u00e1s f\u00e1cil comparar progreso del atleta</li> <li>Configuraci\u00f3n simplificada para cada sesi\u00f3n</li> </ul>"},{"location":"translations/es/camera-setup/#optimizacion-para-diferentes-tipos-de-salto","title":"Optimizaci\u00f3n para Diferentes Tipos de Salto","text":"<p>Espec\u00edfico para Drop Jump:</p> <ul> <li>Asegurar que caj\u00f3n de salto sea visible en encuadre (importante para contexto)</li> <li>Capturar fase de estar parado antes de caer</li> <li>Necesita ver contacto con suelo claramente</li> </ul> <p>Espec\u00edfico para CMJ:</p> <ul> <li>Iniciar con atleta ya en encuadre (sin caj\u00f3n)</li> <li>Capturar fase de contramovimiento (movimiento hacia abajo)</li> <li>Necesita rango completo de movimiento (punto m\u00e1s bajo al pico)</li> </ul> <p>Ambos:</p> <ul> <li>60 fps beneficioso para movimientos r\u00e1pidos</li> <li>Altura de c\u00e1mara a nivel de cadera \u00f3ptima</li> <li>\u00c1ngulo de 45\u00b0 funciona para ambos tipos de salto</li> </ul>"},{"location":"translations/es/camera-setup/#antecedentes-de-investigacion","title":"Antecedentes de Investigaci\u00f3n","text":""},{"location":"translations/es/camera-setup/#por-que-estas-recomendaciones","title":"\u00bfPor Qu\u00e9 Estas Recomendaciones?","text":"<p>\u00c1ngulo de c\u00e1mara (45\u00b0):</p> <ul> <li>Baldinger et al. (2025) mostr\u00f3 que el \u00e1ngulo de visi\u00f3n de c\u00e1mara afecta significativamente la validez del \u00e1ngulo articular</li> <li>45\u00b0 reduce oclusi\u00f3n mientras mantiene visibilidad del plano sagital</li> <li>Compromiso entre frontal (alta visibilidad) y lateral (sagital puro)</li> </ul> <p>Separaci\u00f3n de doble c\u00e1mara de 90\u00b0:</p> <ul> <li>Pagnon et al. (2022): Prob\u00f3 m\u00faltiples \u00e1ngulos, encontr\u00f3 90\u00b0 \u00f3ptimo para triangulaci\u00f3n 3D</li> <li>Dill et al. (2024): Valid\u00f3 MediaPipe est\u00e9reo a 30.1mm RMSE con configuraci\u00f3n de 90\u00b0</li> <li>Balance entre l\u00ednea base amplia (precisi\u00f3n) y vistas superpuestas (coincidencia)</li> </ul> <p>1080p @ 60fps:</p> <ul> <li>Mayor resoluci\u00f3n mejora detecci\u00f3n de puntos de referencia de MediaPipe</li> <li>60 fps necesario para eventos temporales precisos (contacto con suelo)</li> <li>Validado en m\u00faltiples estudios como suficiente para biomec\u00e1nica</li> </ul>"},{"location":"translations/es/camera-setup/#limitaciones-de-camara-unica","title":"Limitaciones de C\u00e1mara \u00danica","text":"<p>Lo que c\u00e1mara \u00fanica (45\u00b0) NO PUEDE proporcionar:</p> <ul> <li>Precisi\u00f3n de grado de investigaci\u00f3n (limitado a ~8-12\u00b0 errores de \u00e1ngulo articular)</li> <li>Coordenadas 3D/profundidad precisas (eje-z ruidoso)</li> <li>Restricciones biomec\u00e1nicas (sin modelo esquel\u00e9tico)</li> <li>Validaci\u00f3n contra est\u00e1ndar de oro (necesita multi-c\u00e1mara)</li> </ul> <p>Lo que c\u00e1mara \u00fanica (45\u00b0) PUEDE proporcionar:</p> <ul> <li>Mediciones de calidad para entrenamiento y evaluaci\u00f3n</li> <li>Comparaciones relativas (mismo atleta a trav\u00e9s del tiempo)</li> <li>M\u00e9tricas clave de drop jump (tiempo de contacto, tiempo de vuelo, RSI)</li> <li>M\u00e9tricas de CMJ (altura de salto, profundidad de contramovimiento)</li> </ul> <p>Para precisi\u00f3n de grado de investigaci\u00f3n: Use configuraci\u00f3n est\u00e9reo de doble c\u00e1mara con Pose2Sim o OpenCap.</p>"},{"location":"translations/es/camera-setup/#resumen","title":"Resumen","text":""},{"location":"translations/es/camera-setup/#un-iphone-a-45-configuracion-estandar","title":"Un iPhone a 45\u00b0 (Configuraci\u00f3n Est\u00e1ndar)","text":"<p>Configuraci\u00f3n r\u00e1pida:</p> <ol> <li>Posicionar c\u00e1mara a 45\u00b0 del plano sagital del atleta</li> <li>4 metros de distancia, altura de cadera (130-150cm)</li> <li>1080p @ 60 fps, horizontal, enfoque/exposici\u00f3n bloqueados</li> <li>Encuadrar cuerpo completo con m\u00e1rgenes de 10-15%</li> <li>Iluminaci\u00f3n uniforme, fondo simple</li> <li>Grabar secuencia completa de salto</li> </ol> <p>Precisi\u00f3n esperada: Buena para entrenamiento/evaluaci\u00f3n (~8-12\u00b0 \u00e1ngulos articulares)</p>"},{"location":"translations/es/camera-setup/#estereo-con-dos-iphones-configuracion-avanzada","title":"Est\u00e9reo con Dos iPhones (Configuraci\u00f3n Avanzada)","text":"<p>Configuraci\u00f3n r\u00e1pida:</p> <ol> <li>Posicionar C\u00e1mara 1 a +45\u00b0 (derecha), C\u00e1mara 2 a -45\u00b0 (izquierda)</li> <li>Ambas a 4m distancia, ambas a altura de cadera, separaci\u00f3n de 90\u00b0</li> <li>Configuraciones id\u00e9nticas: 1080p @ 60fps</li> <li>Calibrar con patr\u00f3n ChArUco/tablero de ajedrez</li> <li>Sincronizar con aplauso o se\u00f1al de audio</li> <li>Procesar con Pose2Sim para reconstrucci\u00f3n 3D</li> </ol> <p>Precisi\u00f3n esperada: Grado de investigaci\u00f3n (~5-7\u00b0 \u00e1ngulos articulares, 30mm RMSE)</p>"},{"location":"translations/es/camera-setup/#guia-de-decision","title":"Gu\u00eda de Decisi\u00f3n","text":"<p>Use c\u00e1mara \u00fanica si:</p> <ul> <li>Aplicaciones de entrenamiento/coaching</li> <li>Evaluar mejoras relativas</li> <li>Restricciones de presupuesto/equipo</li> <li>Se prioriza simplicidad</li> </ul> <p>Use doble c\u00e1mara si:</p> <ul> <li>Aplicaciones de investigaci\u00f3n</li> <li>Evaluaci\u00f3n de atletas de \u00e9lite</li> <li>Se necesita cinem\u00e1tica 3D precisa</li> <li>Publicaci\u00f3n o validaci\u00f3n requerida</li> </ul>"},{"location":"translations/es/camera-setup/#documentacion-relacionada","title":"Documentaci\u00f3n Relacionada","text":"<ul> <li>English Version - Versi\u00f3n en ingl\u00e9s de esta gu\u00eda</li> <li>Estimaci\u00f3n de Pose para Biomec\u00e1nica Deportiva - Investigaci\u00f3n completa sobre sistemas de pose</li> <li>Referencia R\u00e1pida de Sistemas de Pose - Gu\u00eda de comparaci\u00f3n de sistemas</li> <li>Gu\u00eda de Par\u00e1metros CLI - Par\u00e1metros de an\u00e1lisis</li> <li>Gu\u00eda CMJ - Especificaciones de salto con contramovimiento</li> <li>CLAUDE.md principal - Documentaci\u00f3n completa del proyecto (GitHub)</li> </ul>"},{"location":"translations/es/camera-setup/#referencias","title":"Referencias","text":"<p>Investigaci\u00f3n de \u00e1ngulo de c\u00e1mara:</p> <ul> <li>Baldinger, M., Reimer, L. M., &amp; Senner, V. (2025). Influence of the Camera Viewing Angle on OpenPose Validity in Motion Analysis. Sensors, 25(3), 799. https://doi.org/10.3390/s25030799</li> </ul> <p>Validaci\u00f3n de c\u00e1mara est\u00e9reo:</p> <ul> <li>Dill, S., et al. (2024). Accuracy Evaluation of 3D Pose Reconstruction Algorithms Through Stereo Camera Information Fusion for Physical Exercises with MediaPipe Pose. Sensors, 24(23), 7772. https://doi.org/10.3390/s24237772</li> </ul> <p>Separaci\u00f3n \u00f3ptima de c\u00e1mara:</p> <ul> <li>Pagnon, D., Domalain, M., &amp; Reveret, L. (2022). Pose2Sim: An End-to-End Workflow for 3D Markerless Sports Kinematics\u2014Part 2: Accuracy. Sensors, 22(7), 2712. https://doi.org/10.3390/s22072712</li> </ul> <p>Para bibliograf\u00eda completa, ver sports-biomechanics-pose-estimation.md.</p> <p>\u00daltima Actualizaci\u00f3n: 6 de noviembre, 2025</p>"}]}